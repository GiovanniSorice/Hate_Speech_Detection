{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN59DdiajxPGSzEpvX9N7r5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Hate_Speech_Detection/blob/main/notebooks/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKT-LzHeV8ox"
      },
      "source": [
        " ### Removing Tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv46YwYufRS0",
        "outputId": "b3d3169d-9e41-46b5-ec75-61818585dd70"
      },
      "source": [
        "!pip install tokenizer\n",
        "!pip install ekphrasis\n",
        "!pip install wordninja\n",
        "!pip install emoji\n",
        "!pip install spacy_udpipe\n",
        "!pip install language_tool_python\n",
        "!pip install compound-word-splitter\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install enchant\n",
        "!sudo apt-get install hunspell-it"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/88/76356c78aef6b527db91362ed92a0da2d18a0271921d79559fc0bc8c49c3/tokenizer-2.4.0-py2.py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 13.0MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 61kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 5.7MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizer\n",
            "Successfully installed tokenizer-2.4.0\n",
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp37-none-any.whl size=82844 sha256=c942b063b345a5e15d3d52ac36e3c144bfec8daf61e96f3a19471f9c3ca75938\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=7b543c8aa2cf35a2e9fec34bfb64de0cb445be64c0eba3d2ed77a8e406229107\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-5.9 ujson-4.0.2\n",
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 5.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp37-none-any.whl size=541554 sha256=3a34d4877d7db9a81659ffaa4e902f878dd6804732bfd300d1f090a528780453\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.8MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n",
            "Collecting spacy_udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy_udpipe) (2.2.4)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (53.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.0)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626744 sha256=5897999ec44f1789bc809bec49641552c753697715977c7eba37d3bfc6e48250\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe\n",
            "Successfully installed spacy-udpipe-0.3.2 ufal.udpipe-1.2.0.3\n",
            "Collecting language_tool_python\n",
            "  Downloading https://files.pythonhosted.org/packages/13/bd/ff1e1a9a78128d787a959f0a46c39601f21e354712fd4b330278e2af831a/language_tool_python-2.5.2-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2020.12.5)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.5.2\n",
            "Collecting compound-word-splitter\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/0c/62c1ff670016291bd7c24ebed9476ad3cc165eda571ba8442628c636cac1/compound-word-splitter-0.4.tar.gz\n",
            "Building wheels for collected packages: compound-word-splitter\n",
            "  Building wheel for compound-word-splitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compound-word-splitter: filename=compound_word_splitter-0.4-cp37-none-any.whl size=2565 sha256=3e528612bed7b7d4188f512a741b8657c6e80c6580c098563f97e8d8d41e8dd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/de/38/8898744bbad2093c1d909ad5b95f221e85877c3e96131f09d1\n",
            "Successfully built compound-word-splitter\n",
            "Installing collected packages: compound-word-splitter\n",
            "Successfully installed compound-word-splitter-0.4\n",
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8c/bd224a5db562ac008edbfaf015f5d5c98ea13e745247cd4ab5fc5b683085/pyenchant-3.2.0-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.1 [309 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.1 [87.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,310 kB in 1s (1,670 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 149406 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  hunspell libreoffice-writer\n",
            "The following NEW packages will be installed:\n",
            "  hunspell-it\n",
            "0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 340 kB of archives.\n",
            "After this operation, 1,752 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-it all 1:6.0.3-3 [340 kB]\n",
            "Fetched 340 kB in 1s (498 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package hunspell-it.\n",
            "(Reading database ... 149820 files and directories currently installed.)\n",
            "Preparing to unpack .../hunspell-it_1%3a6.0.3-3_all.deb ...\n",
            "Unpacking hunspell-it (1:6.0.3-3) ...\n",
            "Setting up hunspell-it (1:6.0.3-3) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRjY391AGd-f"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from tokenizer import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "import wordninja\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import ast\n",
        "\n",
        "import emoji\n",
        "import unicodedata\n",
        "\n",
        "import gzip\n",
        "\n",
        "import spacy_udpipe\n",
        "import language_tool_python\n",
        "\n",
        "import splitter\n",
        "import enchant\n",
        "from itertools import groupby\n",
        "import string\n",
        "\n",
        "import sys\n",
        "import os\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQjATsWGdpJW"
      },
      "source": [
        "# directory name:\n",
        "input_dir = '/content/drive/My Drive/HLT/dataset_test_evalita/'\n",
        "output_dir = '/content/drive/My Drive/HLT/dataset_test_evalita_preprocessed/'\n",
        "input_dir_preprocessing = '/content/drive/My Drive/HLT/preprocessing/'\n",
        "\n",
        "# Spec\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhfNkQOddzQ9",
        "outputId": "6fc08a35-8313-4f68-ab4a-a2cfb19a4068"
      },
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeSkmao9eW02"
      },
      "source": [
        "tsv_tweets_file = open(input_dir+\"haspeede2_reference_taskAB-tweets.tsv\")\n",
        "#tsv_news_file = open(\"/content/drive/My Drive/HLT/dataset/haspeede2_reference_taskAB-news.tsv\")\n",
        "#tsv_file = open(input_dir+\"haspeede2_dev_taskAB.tsv\")\n",
        "\n",
        "#dataset_tweets_raw = pd.read_csv(tsv_tweets_file,sep='\\t')\n",
        "#dataset_tweets_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "#dataset_news_raw = pd.read_csv(tsv_news_file,sep='\\t')\n",
        "#dataset_news_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "dataset_raw = pd.read_csv(tsv_tweets_file,sep='\\t', header=None)\n",
        "#dataset_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "dataset_raw.rename(columns={0: \"id\"}, inplace=True)\n",
        "dataset_raw.rename(columns={1: \"text\"}, inplace=True)\n",
        "dataset_raw.rename(columns={2: \"hs\"}, inplace=True)\n",
        "dataset_raw.rename(columns={3: \"stereotype\"}, inplace=True)\n",
        "\n",
        "#Bad Words\n",
        "f2 = open(input_dir_preprocessing+\"bad_words.csv\", 'r', encoding='utf8')\n",
        "\n",
        "bad_words_set = [] #list of lowercase words\n",
        "\n",
        "for x in f2:\n",
        "    y = x.rstrip()\n",
        "    y = y.lower()\n",
        "    if y != '':\n",
        "        bad_words_set.append(y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63ozW-a2-kNG",
        "outputId": "74318af8-9b92-4552-f38f-99112d4bef1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        }
      },
      "source": [
        "dataset_raw"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>hs</th>\n",
              "      <th>stereotype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11834</td>\n",
              "      <td>@user A me pare una scelta politica suicida puntare tutto su una battaglia sicuramente perdente in favore dell'immigrazione incontrollata...Meglio così, spariranno più velocemente!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12113</td>\n",
              "      <td>@user e' un perfetto musulmano!!! chi dice il contrario mente, applica alla lettera la sura \"la donna\" del Corano. Il pericolo non e' lui in quanto tale, che lo dice, ma gli infami bugiardi! dell' islam moderato che NON ESISTE,</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11770</td>\n",
              "      <td>Mai Tg e i giornaloni hanno parlato di questa ennesima porcata fatta da immigrati ?? ⤵️⤵️⤵️⤵️  Sanremo, Segregata e stuprata per mesi. La giovane sequestrata da sei immigrati. Legata al letto e stuprata a turno per due mesi. Poi ceduta ad altri \"clienti\"  URL</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11937</td>\n",
              "      <td>@user Ipocriti farabutti. Fanno morire i terremotati per I Bastardi clandestini immigrati schifosi.</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11870</td>\n",
              "      <td>@user @user @user L'IMMIGRAZIONE C'E' STATO UN CALO FISIOLOGICO QUANTI RINPATRII HA FATTO ? CI SONO MIGLIAIA DI IMMIGRATI IRREGOLARI CHE DELINQUONO TUTTI I GIORNI SUL TERRITORIO ITALIANI NON POSSONO ESSERE PUNITI DALLA GIUSTIZIA  IN QUANTO PROFUGHI QUINDI ?</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1258</th>\n",
              "      <td>10091</td>\n",
              "      <td>Il Razzismo gli viene inculcato sin da bambini dal corano e dall'islam Cani, Maiali. Cristiani ed Ebrei sono impuri, infedeli e da sterminare. Il nemico e gran Pezzo di Merda è colui che li vuole qui a tutti i costi.</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1259</th>\n",
              "      <td>10046</td>\n",
              "      <td>PERCHÉ DI QUESTO ORRENDO STUPRO NON DI PARLA? INFORMAZIONE CORROTTA,LA SINISTRA CONTINUA A COPRIRE I DELINQUENTI STRANIERI !! #viterbo #casapound #migranti URL</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1260</th>\n",
              "      <td>10377</td>\n",
              "      <td>FERMIAMO L'IMPOSTORE. Forse saranno necesarie altre CROCIATE, visto che il fiume di fango non si ferma.Anzi Vaticano, SX e POTERI MASSONICI ci stanno massacrando. Quando mai in Italia qualcuno veniva SGOZZATO, PRESO A SPRANGATE, FATTO A PEZZI? Fermiamo l'invasione dell'ISLAM!!!</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1261</th>\n",
              "      <td>10199</td>\n",
              "      <td>@user @user @user @user @user @user @user @user @user @user Matteo non si tocca...fuori i delinquenti stranieri da casa nostra !</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262</th>\n",
              "      <td>10574</td>\n",
              "      <td>Ci vuole \"coraggio\" ad abbinare la parola Eguaglianza all'Islam! Credo che, tra i tanti, proprio gli omosessuali dovrebbero indignarsi dinanzi a queste dimostrazioni di ignoranza. Per omosessualità, in alcuni Paesi Islamici, si muore ancora oggi. Altro che bandiere della Pace...! URL</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1263 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... stereotype\n",
              "0     11834  ...          0\n",
              "1     12113  ...          1\n",
              "2     11770  ...          1\n",
              "3     11937  ...          1\n",
              "4     11870  ...          1\n",
              "...     ...  ...        ...\n",
              "1258  10091  ...          1\n",
              "1259  10046  ...          1\n",
              "1260  10377  ...          1\n",
              "1261  10199  ...          1\n",
              "1262  10574  ...          1\n",
              "\n",
              "[1263 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPBGTK1PiGEx"
      },
      "source": [
        "# Preprocessing phase\n",
        "Following the \"Preprocessing\" section of https://books.openedition.org/aaccademia/4832?lang=it#tocfrom1n4 we will perform:\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTSJjfRgjZ8s"
      },
      "source": [
        "### Extraction of the first feature: length of the comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hDGiCnLhlHB"
      },
      "source": [
        "def comment_length(text):\n",
        "    return len(text)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF0sQvOrjqDN"
      },
      "source": [
        "dataset_raw['text_length'] = dataset_raw['text'].apply(comment_length)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTIhhE0Jjzqq"
      },
      "source": [
        "Extraction of the second feature: percentage of words written in CAPS-LOCK inside the comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8G1vCSnj4ML"
      },
      "source": [
        "def caps_lock_words(text):\n",
        "    words = text.split()\n",
        "    count_caps_lock = 0\n",
        "    number_of_words = len(words)\n",
        "    \n",
        "    for word in words:\n",
        "        if word.isupper() == True:\n",
        "            count_caps_lock = count_caps_lock + 1\n",
        "            \n",
        "    return ((count_caps_lock*100)//number_of_words)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeH2idQEj66P"
      },
      "source": [
        "dataset_raw['#C-L words'] = dataset_raw['text'].apply(caps_lock_words)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26m0-vh5JEOB"
      },
      "source": [
        " ### Removing Tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZTSDBK4JNIl"
      },
      "source": [
        "def clean_tag(text):\n",
        "    return re.sub(\n",
        "        r'@user', ' ', text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22IGkgdXJPRT"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_tag)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpIlGt9OI5je"
      },
      "source": [
        "### Replace the characters ‘&’, ‘@’ respectively in the letters ‘e’, ‘a’"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xZ02VkZJUX5"
      },
      "source": [
        "def replace_e_a(text):\n",
        "    text = re.sub(r'&', 'e', text)\n",
        "    return re.sub(r'@', 'a', text)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vURkcOrHJgiQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(replace_e_a)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9T6so5NJ21c"
      },
      "source": [
        "### Conversion of disguised bad words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joAwdJKBJ9Vw"
      },
      "source": [
        "def clean_disguised_bad_words(text):\n",
        "    text = \" \" + text + \" \"\n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' coglioni ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+e ', ' coglione ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+o ', ' cazzo ', text) \n",
        "    text = re.sub(r' ca[.x*@%#$^]+ro ', ' cazzaro ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' cazzi ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+a ', ' merda ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+e ', ' merde ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+ulo ', ' culo ', text) \n",
        "    text = re.sub(r' p[.x*@%#$^]+a ', ' puttana ', text)\n",
        "    text = re.sub(r' p[.x*@%#$^]+e ', ' puttane ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+a ', ' troia ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+e ', ' troie ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+o ', ' stronzo ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+i ', ' stronzi ', text)\n",
        "    return text"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjzGx0uFJ9y5"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_disguised_bad_words)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzeCUAIEKX5x"
      },
      "source": [
        "### Hashtag splitting and saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy7M8Rw3VltO"
      },
      "source": [
        "def find_hashtags(text):\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    if result:\n",
        "        return result\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mtFua9FVlCk"
      },
      "source": [
        "dataset_raw['hashtags'] = dataset_raw['text'].apply(find_hashtags)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHHr66WzQ7mJ"
      },
      "source": [
        "def split_hashtags(text):\n",
        "    \n",
        "    text = ' ' + text + ' '\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    \n",
        "    for word in result:\n",
        "        new_word = \" \".join(splitter.split(word[1:].lower(), 'it_IT'))\n",
        "        if len(new_word)==0:\n",
        "            new_word =  word[1:]\n",
        "                  \n",
        "        text = text.replace(word, new_word)\n",
        "        \n",
        "    return text"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNR6YmYRFlQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(split_hashtags)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZdFjV6zWCJ3"
      },
      "source": [
        " ### Removing URL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMMZC6EaWB92"
      },
      "source": [
        "def clean_URL(text):\n",
        "    return re.sub(r'URL', ' ', text)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GInlm-4NWByp"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_URL)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vw02ux1d8xr"
      },
      "source": [
        "### Extraction of the fourth feature: number of ‘?’ or ‘!’ inside the comment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAilG6e9d_Kv"
      },
      "source": [
        "def esclamations_and_questions(text):\n",
        "    return text.count('!') + text.count('?')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8aPhymFd_W4"
      },
      "source": [
        "dataset_raw['#?!'] = dataset_raw['text'].apply(esclamations_and_questions)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ODCBvIweoBT"
      },
      "source": [
        "### Extraction of the fifth feature: number of ‘.’ or ‘,’ inside the comment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4cXR155eoBV"
      },
      "source": [
        "def esclamations_and_questions(text):\n",
        "    return text.count(',') + text.count('.')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o2NoyA-eoBW"
      },
      "source": [
        "dataset_raw['#.,'] = dataset_raw['text'].apply(esclamations_and_questions)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2R9v14KfNvS"
      },
      "source": [
        "### Punctuation removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOfbQApfQq4"
      },
      "source": [
        "def strip_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJruDmPfQ4M"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(strip_punctuation)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jld5EAWV11"
      },
      "source": [
        "### Removal of nearby equal vowels, removal of nearby equal consonants if they are more than 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At0CUlq9akCI"
      },
      "source": [
        "correct_words_vowels = ['sciiti,',\n",
        " 'welcomerefugees',\n",
        " 'livetweet',\n",
        " 'desiree',\n",
        " 'canaan',\n",
        " 'tweet.',\n",
        " 'weekend',\n",
        " 'romafeyenoord',\n",
        " 'ebree,',\n",
        " 'greencard',\n",
        " 'creerà',\n",
        " 'cooperative.',\n",
        " 'moschee,',\n",
        " 'cooperanti',\n",
        " 'streetart',\n",
        " 'khalidmasood',\n",
        " 'tweet',\n",
        " 'woolfe',\n",
        " 'cooperazione',\n",
        " 'coop',\n",
        " 'seehofer',\n",
        " 'speech',\n",
        " 'coffee',\n",
        " 'scooter',\n",
        " 'street',\n",
        " 'veemenza',\n",
        " 'moschee.',\n",
        " 'maalox.',\n",
        " 'book',\n",
        " 'tweet',\n",
        " 'facebook:',\n",
        " 'sociial,',\n",
        " 'coop,',\n",
        " 'canaaniti.',\n",
        " 'europee,',\n",
        " 'cooperative',\n",
        " 'google',\n",
        " 'creeranno',\n",
        " 'mediterranee',\n",
        " 'cooperazione',\n",
        " 'cooperativa',\n",
        " '“boom”',\n",
        " 'refugees',\n",
        " 'moonlight',\n",
        " 'imaam',\n",
        " 'shooting',\n",
        " 'sciiti',\n",
        " 'sunniti',\n",
        " 'book',\n",
        " 'atee.',\n",
        " 'looking',\n",
        " 'week',\n",
        " 'ayaan',\n",
        " 'temporanee.',\n",
        " 'idee.',\n",
        " 'sibiliini',\n",
        " 'food',\n",
        " 'refugees',\n",
        " 'retweeted',\n",
        " 'boom',\n",
        " 'keep',\n",
        " 'vodoo',\n",
        " 'hooligans',\n",
        " 'ebree',\n",
        " 'refugees',\n",
        " 'speed',\n",
        " 'bloomberg',\n",
        " 'riina',\n",
        " 'hatespeech',\n",
        " 'google',\n",
        " 'masood',\n",
        " 'linee.',\n",
        " 'boom']"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFnrr_UqWWQ-"
      },
      "source": [
        "def delete_duplicate_vowels_and_redundant_consonant (text):\n",
        "    parole = text.split()\n",
        "    stringa = \"\"\n",
        "    for a in parole:\n",
        "        parola = a\n",
        "        a = [list(g) for k, g in groupby(a)]    \n",
        "        vocali = ['a','e','i','o','u','y']\n",
        "        \n",
        "        for idx,val in enumerate(a):\n",
        "            if idx == 0:\n",
        "                stringa += a[idx][0] \n",
        "            elif idx == len(a)-1:\n",
        "                stringa += a[idx][0]\n",
        "            elif a[idx][0] in vocali and (parola.lower() not in correct_words_vowels):\n",
        "                stringa += a[idx][0]\n",
        "            elif len(a[idx]) == 1:\n",
        "                stringa += a[idx][0]\n",
        "            elif len (a[idx]) >= 2:\n",
        "                stringa += a[idx][0]\n",
        "                stringa += a[idx][1]\n",
        "        stringa =  stringa + \" \"\n",
        "        \n",
        "    return(stringa)  "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol8qT-rhWWM6"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(delete_duplicate_vowels_and_redundant_consonant)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s06b9uii4eN-"
      },
      "source": [
        "### Translation of emoticons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3ctOpMF79JW"
      },
      "source": [
        "def translate_emoticon(text):\n",
        "    text_result = emoji.demojize(text, language='it')\n",
        "    text_result=re.sub(r':', ' ', text_result)\n",
        "    return text_result"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbR6rg3V7-Wt"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(translate_emoticon)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi5I3Y606DVQ"
      },
      "source": [
        "emoticons_text = {\n",
        "    '<kiss>': 'bacio',\n",
        "    '<happy>': 'felice',\n",
        "    '<laugh>': 'risata',\n",
        "    '<sad>': 'triste',\n",
        "    '<surprise>': 'sorpreso',\n",
        "    '<wink>': 'occhiolino',\n",
        "    '<tong>': 'faccia con lingua',\n",
        "    '<annoyed>': 'annoiato',\n",
        "    '<seallips>': 'labbra sigillate',\n",
        "    '<angel>': 'angelo',\n",
        "    '<devil>': 'diavolo',\n",
        "    '<highfive>' : 'batti il cinque',\n",
        "    '<heart>': 'cuore',\n",
        "    '<user>' : 'persona',\n",
        "}"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWgDe3ca6EC6"
      },
      "source": [
        "def clean_emoticon_text(text):\n",
        "    text_words = text.split()\n",
        "    new_words  = [emoticons_text.get(ele, ele) for ele in text_words]\n",
        "    return ' '.join(new_words)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRaxAUC86DJQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_emoticon_text)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM8zUkgjA_Xf"
      },
      "source": [
        "### Replacement of the abbreviations with the respective words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJEKyYL-BMhk"
      },
      "source": [
        "abbr_word = {'cmq':'comunque', 'gov':'governatori', 'fb':'facebook', 'tw':'twitter', 'juve':'juventus', 'ing':'ingegnere', \n",
        "             'sx':'sinistra', 'qdo':'quando', 'rep':'repubblica', 'grz':'grazie', 'ita':'italia', 'mln':'milioni', \n",
        "             'mld':'miliardi', 'pke':'perche', 'anke':'anche', 'cm':'come', 'dlla':'della', 'dlle':'delle', 'qst':'questa',\n",
        "             'ke':'che', 'nn':'non', 'sn':'sono', 'cn':'con', 'xk':'perche', 'xke':'perche', 'art':'articolo',\n",
        "             'tv':'televisore', '€':'euro', 'xché':'perché', 'xké':'perché', 'pkè':'perché'}"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNUJsA9xBMsL"
      },
      "source": [
        "def replace_abbreviation(text):\n",
        "    text_words = text.split()\n",
        "    new_text = \"\"\n",
        "    for token in text_words:\n",
        "      new_text  += abbr_word.get(token, token) +\" \"\n",
        "    \n",
        "    return new_text.strip()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRRSxGnEBRSe"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(replace_abbreviation)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcoE_FIQBZDO"
      },
      "source": [
        "### Removal of the laughs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTfCPOK3B9pL"
      },
      "source": [
        "laughs = ['ah', 'eh', 'he' 'ih', 'hi'] #non elimina ahahahah, ma solo ah\n",
        "vowels = ['a', 'e', 'i', 'o', 'u']\n",
        "\n",
        "def clean_laughs(text):\n",
        "    #s = \"ahahahah ho fame io, eh eh\" -> \" ho fame io,\"\n",
        "    text_words = text.split()\n",
        "    new_words  = [word for word in text_words if word not in laughs]\n",
        "    \n",
        "    new_text = ' '.join(new_words)\n",
        "    \n",
        "    for i in new_words:\n",
        "        for k in vowels:\n",
        "            if ('h' in i) and (len(i) >= 4):\n",
        "                if (len(i) - 2) <= (i.count(k) + i.count('h')):\n",
        "                    new_text = new_text.replace(i, '')\n",
        "    \n",
        "    return new_text"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEMhN6UCB90s"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_laughs)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPBk_1WFFbVv"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7TesaZ_Fcyx"
      },
      "source": [
        "def tokenization(text):\n",
        "    tknzr=SocialTokenizer(lowercase=False)\n",
        "    return tknzr.tokenize(text)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BcgiN-cFf25",
        "outputId": "de8983ec-bf8a-4bb8-c598-49cbca152100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset_raw['tokens'] = dataset_raw['text'].apply(tokenization)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADwlSkQyWH96"
      },
      "source": [
        "### Extraction of the sixth feature: number of bad words in the comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ3Ty8mKWqyG"
      },
      "source": [
        "def number_bad_words(tokens):\n",
        "    n_bad_words = 0\n",
        "\n",
        "    for word in tokens:\n",
        "        if word.lower() in bad_words_set:\n",
        "            n_bad_words = n_bad_words + 1\n",
        "        \n",
        "    return n_bad_words"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOJBDkIxWqk-"
      },
      "source": [
        "dataset_raw['#bad_words'] = dataset_raw['tokens'].apply(number_bad_words)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Vtd6HGWgDJ"
      },
      "source": [
        "### Extraction of the seventh feature: percentage of bad words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_6xdkb0WYds"
      },
      "source": [
        "def percentage_bad_words(tokens):\n",
        "    n_words = 0\n",
        "    n_bad_words = 0\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word != '<' and word != '>':\n",
        "            n_words = n_words + 1\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word.lower() in bad_words_set:\n",
        "            n_bad_words = n_bad_words + 1\n",
        "        \n",
        "    return ((n_bad_words*100)//n_words)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvQRET5VWaxZ"
      },
      "source": [
        "dataset_raw['%bad_words'] = dataset_raw['tokens'].apply(percentage_bad_words)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g_W0gmIpkFKB",
        "outputId": "47010c69-76d7-4f84-b13c-48197cff6646"
      },
      "source": [
        "dataset_raw.head(30)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>hs</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>text_length</th>\n",
              "      <th>#C-L words</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>#?!</th>\n",
              "      <th>#.,</th>\n",
              "      <th>tokens</th>\n",
              "      <th>#bad_words</th>\n",
              "      <th>%bad_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11834</td>\n",
              "      <td>A me pare una scelta politica suicida puntare tutto su una battaglia sicuramente perdente in favore dellimmigrazione incontrollataMeglio così spariranno più velocemente</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>180</td>\n",
              "      <td>4</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>[A, me, pare, una, scelta, politica, suicida, puntare, tutto, su, una, battaglia, sicuramente, perdente, in, favore, dellimmigrazione, incontrollataMeglio, così, spariranno, più, velocemente]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12113</td>\n",
              "      <td>e un perfetto musulmano chi dice il contrario mente applica alla lettera la sura la donna del Corano Il pericolo non e lui in quanto tale che lo dice ma gli infami bugiardi del islam moderato che NON ESISTE</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>227</td>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>[e, un, perfetto, musulmano, chi, dice, il, contrario, mente, applica, alla, lettera, la, sura, la, donna, del, Corano, Il, pericolo, non, e, lui, in, quanto, tale, che, lo, dice, ma, gli, infami, bugiardi, del, islam, moderato, che, NON, ESISTE]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11770</td>\n",
              "      <td>Mai Tg e i giornaloni hanno parlato di questa ennesima porcata fatta da immigrati freccia_curva_in_basso freccia_curva_in_basso freccia_curva_in_basso freccia_curva_in_basso Sanremo Segregata e stuprata per mesi La giovane sequestrata da sei immigrati Legata al letto e stuprata a turno per due mesi Poi ceduta ad altri clienti</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>259</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>[Mai, Tg, e, i, giornaloni, hanno, parlato, di, questa, ennesima, porcata, fatta, da, immigrati, freccia_curva_in_basso, freccia_curva_in_basso, freccia_curva_in_basso, freccia_curva_in_basso, Sanremo, Segregata, e, stuprata, per, mesi, La, giovane, sequestrata, da, sei, immigrati, Legata, al, letto, e, stuprata, a, turno, per, due, mesi, Poi, ceduta, ad, altri, clienti]</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11937</td>\n",
              "      <td>Ipocriti farabutti Fanno morire i terremotati per I Bastardi clandestini immigrati schifosi</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>99</td>\n",
              "      <td>7</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[Ipocriti, farabutti, Fanno, morire, i, terremotati, per, I, Bastardi, clandestini, immigrati, schifosi]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11870</td>\n",
              "      <td>LIMMIGRAZIONE CE STATO UN CALO FISIOLOGICO QUANTI RINPATRI HA FATTO CI SONO MIGLIAIA DI IMMIGRATI IRREGOLARI CHE DELINQUONO TUTTI I GIORNI SUL TERRITORIO ITALIANI NON POSSONO ESSERE PUNITI DALLA GIUSTIZIA IN QUANTO PROFUGHI QUINDI</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>257</td>\n",
              "      <td>87</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>[LIMMIGRAZIONE, CE, STATO, UN, CALO, FISIOLOGICO, QUANTI, RINPATRI, HA, FATTO, CI, SONO, MIGLIAIA, DI, IMMIGRATI, IRREGOLARI, CHE, DELINQUONO, TUTTI, I, GIORNI, SUL, TERRITORIO, ITALIANI, NON, POSSONO, ESSERE, PUNITI, DALLA, GIUSTIZIA, IN, QUANTO, PROFUGHI, QUINDI]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>12122</td>\n",
              "      <td>SI CERTO E TUTTIQUELLI CHE GIRANO A VUOTO SENZA FARE UNCAZZO CE LI DOBBIAMO TENERE ESPATRIOOLE GALERE SONO PIENE 80 SONO STRANIERI LI DOBBIAMO MANTENERE ANCHE LÌ BASTA</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>210</td>\n",
              "      <td>90</td>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>[SI, CERTO, E, TUTTIQUELLI, CHE, GIRANO, A, VUOTO, SENZA, FARE, UNCAZZO, CE, LI, DOBBIAMO, TENERE, ESPATRIOOLE, GALERE, SONO, PIENE, 80, SONO, STRANIERI, LI, DOBBIAMO, MANTENERE, ANCHE, LÌ, BASTA]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>12095</td>\n",
              "      <td>Ha ragione É il corano che prescrive la lotta agli infedeli e loro sottomissione nessuna convivenza pacifica e men che mai paritetica</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>139</td>\n",
              "      <td>8</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Ha, ragione, É, il, corano, che, prescrive, la, lotta, agli, infedeli, e, loro, sottomissione, nessuna, convivenza, pacifica, e, men, che, mai, paritetica]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11609</td>\n",
              "      <td>Per quanto mi riguarda laccogliere indiscriminatamente ci ha portato solo parassiti stupratori spacciatori e mafia nigeriana con tanto di traffico di organi umani e molta meno sicurezza Migranti Fico “L’accoglienza significa crescita e sicurezza”</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>258</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[Per, quanto, mi, riguarda, laccogliere, indiscriminatamente, ci, ha, portato, solo, parassiti, stupratori, spacciatori, e, mafia, nigeriana, con, tanto, di, traffico, di, organi, umani, e, molta, meno, sicurezza, Migranti, Fico, “, L, ’, accoglienza, significa, crescita, e, sicurezza, ”]</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12141</td>\n",
              "      <td>IO DOMANDO A QUESTE MERDE che vogliono e accettano altre merde mussulmane se gli viene in mente cosa si preparerebbe per noi oppure sono solo merde che n pensano se no ai soldi non parliamo poi q questo auspicio è proclamato da donne italiane mi auguro siano le prime a subirlo</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>282</td>\n",
              "      <td>11</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[IO, DOMANDO, A, QUESTE, MERDE, che, vogliono, e, accettano, altre, merde, mussulmane, se, gli, viene, in, mente, cosa, si, preparerebbe, per, noi, oppure, sono, solo, merde, che, n, pensano, se, no, ai, soldi, non, parliamo, poi, q, questo, auspicio, è, proclamato, da, donne, italiane, mi, auguro, siano, le, prime, a, subirlo]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>11868</td>\n",
              "      <td>Eccerto poverini poi i migranti si incattiviscono Ma porca puttana che cazzo dite Infatti sono dolcissimi quelli che pestano i vecchi e tagliano a pezzi ragazzine ago rara i</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>[#agorarai]</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>[Eccerto, poverini, poi, i, migranti, si, incattiviscono, Ma, porca, puttana, che, cazzo, dite, Infatti, sono, dolcissimi, quelli, che, pestano, i, vecchi, e, tagliano, a, pezzi, ragazzine, ago, rara, i]</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12176</td>\n",
              "      <td>Tutti i politici rubanohanno rubato e ruberannoLa Fornero ha rubato 10mila euro a mio padreMa basta con lecchini di immigrati che si ciondolano tutto il giorno con cellulari stracostosi e sigarette e per noia violentano o rubanoVuoi che tuo figlio conosca il Natale o no</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>281</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>[Tutti, i, politici, rubanohanno, rubato, e, ruberannoLa, Fornero, ha, rubato, 1, 0, mila, euro, a, mio, padreMa, basta, con, lecchini, di, immigrati, che, si, ciondolano, tutto, il, giorno, con, cellulari, stracostosi, e, sigarette, e, per, noia, violentano, o, rubanoVuoi, che, tuo, figlio, conosca, il, Natale, o, no]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11846</td>\n",
              "      <td>Rosarno le case popolari Solo agli immigrati Hanno avuto bisogno di governi non eletti di gente imposta ad un popolo disarmato Una volta messi li i VIGLIACCHI hanno dato inizio alla ns fine Se e quando si scatenerà la rabbia vera ne farò parte</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>256</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>[Rosarno, le, case, popolari, Solo, agli, immigrati, Hanno, avuto, bisogno, di, governi, non, eletti, di, gente, imposta, ad, un, popolo, disarmato, Una, volta, messi, li, i, VIGLIACCHI, hanno, dato, inizio, alla, ns, fine, Se, e, quando, si, scatenerà, la, rabbia, vera, ne, farò, parte]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>11984</td>\n",
              "      <td>i musulmano ammazzano tutti quelli che per il loro fumoso cervello sono infedeli i nostri terroristi istituzionali ci obbligano ad accogliere e mantenere i nostri assassini</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[i, musulmano, ammazzano, tutti, quelli, che, per, il, loro, fumoso, cervello, sono, infedeli, i, nostri, terroristi, istituzionali, ci, obbligano, ad, accogliere, e, mantenere, i, nostri, assassini]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>11691</td>\n",
              "      <td>laccusa della sinistraglia cialtrona immigrazionista a sa lv in i è che ha impedito che altri parassiti afroislamici infestassero lItalia e ha danneggiato il busines dellimmigrazione clandestina Solita feccia rosicona sa lv in iNonMollare</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>242</td>\n",
              "      <td>0</td>\n",
              "      <td>[#Salvini, #SalviniNonMollare]</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[laccusa, della, sinistraglia, cialtrona, immigrazionista, a, sa, lv, in, i, è, che, ha, impedito, che, altri, parassiti, afroislamici, infestassero, lItalia, e, ha, danneggiato, il, busines, dellimmigrazione, clandestina, Solita, feccia, rosicona, sa, lv, in, iNonMollare]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>11781</td>\n",
              "      <td>Alle idiote pdinne gli serve questo trattamento forse solo dopo si ricrederanno e dico forse tanto sono coglione idiote dementi Io dico unica salvezza NUOVE CROCIATE CONTRO LISLAM VIVA RICCARDO CUOR DI LEONE NUOVE CROCIATE ALTRO CHE ACCOGLIENZA</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>35</td>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>[Alle, idiote, pdinne, gli, serve, questo, trattamento, forse, solo, dopo, si, ricrederanno, e, dico, forse, tanto, sono, coglione, idiote, dementi, Io, dico, unica, salvezza, NUOVE, CROCIATE, CONTRO, LISLAM, VIVA, RICCARDO, CUOR, DI, LEONE, NUOVE, CROCIATE, ALTRO, CHE, ACCOGLIENZA]</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>12153</td>\n",
              "      <td>IL FATTO E CHE NESSUNO VUOLE FINTI PROFUGHI DA MANTENERE NEGLI HOTEL SOLO L ITALIA FA ENTRARE CANI E PORCI</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>118</td>\n",
              "      <td>90</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[IL, FATTO, E, CHE, NESSUNO, VUOLE, FINTI, PROFUGHI, DA, MANTENERE, NEGLI, HOTEL, SOLO, L, ITALIA, FA, ENTRARE, CANI, E, PORCI]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>11841</td>\n",
              "      <td>è semplicemente un paragone che usano per comodo E gente dalla memoria corta dimentica quanto malequanti morti lue subisce dallislam e se avessero subito unaggressione starebbero ben zitti Fanno i buoni con chi ci odia questo è lo schifo</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>261</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[è, semplicemente, un, paragone, che, usano, per, comodo, E, gente, dalla, memoria, corta, dimentica, quanto, malequanti, morti, lue, subisce, dallislam, e, se, avessero, subito, unaggressione, starebbero, ben, zitti, Fanno, i, buoni, con, chi, ci, odia, questo, è, lo, schifo]</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>12040</td>\n",
              "      <td>Della costituzione i muslims si puliscono il didietro come di tutte le leggi non muslim Riconoscono solo la legge coranicasharia e il Corano tutto il resto NON esiste Chi dice il contrario è un imbecille o in malafede O entrambi</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>238</td>\n",
              "      <td>7</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>[Della, costituzione, i, muslims, si, puliscono, il, didietro, come, di, tutte, le, leggi, non, muslim, Riconoscono, solo, la, legge, coranicasharia, e, il, Corano, tutto, il, resto, NON, esiste, Chi, dice, il, contrario, è, un, imbecille, o, in, malafede, O, entrambi]</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>12065</td>\n",
              "      <td>Mentre qui li si vuole sbarcare in Germania i medesimi chiedono mogli alla Merkel Di Battista ingenuamente vorrebbe accoglierli ma poi Altri 47 474747 No Dibba non può essere questa la soluzione Restiamo uniti No migranti no islamizzazione</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>257</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>[Mentre, qui, li, si, vuole, sbarcare, in, Germania, i, medesimi, chiedono, mogli, alla, Merkel, Di, Battista, ingenuamente, vorrebbe, accoglierli, ma, poi, Altri, 47, 474747, No, Dibba, non, può, essere, questa, la, soluzione, Restiamo, uniti, No, migranti, no, islamizzazione]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>12233</td>\n",
              "      <td>CENTRI ASOCIALI E ISLAMICI BRUCIANO LE CROCI PER SOLIDARIETÀ AI MIGRANTI LA SOLIDARIETÀ ANDATE A FARLA DOVE CE NE È BISOGNO NON POTETE ROMPERE I COGLIONI AD UNA NAZIONE INTERA AVETE SPOSATO LA STESSA CAUSA PREPOTENZA ARROGANZA ISLAMRADICAL CHIC IDEM</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>255</td>\n",
              "      <td>100</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[CENTRI, ASOCIALI, E, ISLAMICI, BRUCIANO, LE, CROCI, PER, SOLIDARIETÀ, AI, MIGRANTI, LA, SOLIDARIETÀ, ANDATE, A, FARLA, DOVE, CE, NE, È, BISOGNO, NON, POTETE, ROMPERE, I, COGLIONI, AD, UNA, NAZIONE, INTERA, AVETE, SPOSATO, LA, STESSA, CAUSA, PREPOTENZA, ARROGANZA, ISLAMRADICAL, CHIC, IDEM]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>11895</td>\n",
              "      <td>Politicamente corretti fate schifo come gli immigrati che vi stanno tanto a cuore e nel portafoglio prebende di “accoglienza”Andate a morire ammazzati magari tagliuzzati e mangiaticome le bestie assassine di Pamela avevano in mente di fareSiamo umaniVoi cacca umane</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>276</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[Politicamente, corretti, fate, schifo, come, gli, immigrati, che, vi, stanno, tanto, a, cuore, e, nel, portafoglio, prebende, di, “, accoglienza, ”, Andate, a, morire, ammazzati, magari, tagliuzzati, e, mangiaticome, le, bestie, assassine, di, Pamela, avevano, in, mente, di, fareSiamo, umaniVoi, cacca, umane]</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>12224</td>\n",
              "      <td>Quelli che dicono io non sto con sa lv in i perché non sono razzista lo sanno usare il mono neurone o siete condizionati dal politicamente corretto L’immigrazione esiste da sempre in Italia questa non è immigrazione questo è un colossale piano di invasione e sostituzione san remo</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>280</td>\n",
              "      <td>0</td>\n",
              "      <td>[#Salvini, #Sanremo]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Quelli, che, dicono, io, non, sto, con, sa, lv, in, i, perché, non, sono, razzista, lo, sanno, usare, il, mono, neurone, o, siete, condizionati, dal, politicamente, corretto, L, ’, immigrazione, esiste, da, sempre, in, Italia, questa, non, è, immigrazione, questo, è, un, colossale, piano, di, invasione, e, sostituzione, san, remo]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>12026</td>\n",
              "      <td>Dopo la sfida a UE malta itali a la CRIMINALE SeaWatch3 torna a fare guadagni dai trafficanti mafie malati ideologici Dem clero col busines immigrazione Sparge sporco parassitismo NERO e impoverimento razziale in Stati superiori porti chiusi</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>265</td>\n",
              "      <td>8</td>\n",
              "      <td>[#UE, #Malta, #Italia, #SeaWatch3, #immigrazione., #portichiusi]</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>[Dopo, la, sfida, a, UE, malta, itali, a, la, CRIMINALE, SeaWatch3, torna, a, fare, guadagni, dai, trafficanti, mafie, malati, ideologici, Dem, clero, col, busines, immigrazione, Sparge, sporco, parassitismo, NERO, e, impoverimento, razziale, in, Stati, superiori, porti, chiusi]</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>11955</td>\n",
              "      <td>Bisogna capire che il problema è di chi fa partire i barconi Bisogna capire l’incoscienza di queste persone per non dire che li mandano a morire L’Italia ormai per anni ha accolto milioni di INVASORI perché non si tratta di immigrazione ma l’Italia è piccola</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>277</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>[Bisogna, capire, che, il, problema, è, di, chi, fa, partire, i, barconi, Bisogna, capire, l, ’, incoscienza, di, queste, persone, per, non, dire, che, li, mandano, a, morire, L, ’, Italia, ormai, per, anni, ha, accolto, milioni, di, INVASORI, perché, non, si, tratta, di, immigrazione, ma, l, ’, Italia, è, piccola]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>11696</td>\n",
              "      <td>Per questo ci avete imposto l’obbligo vaccinale a prescindere Già noi il gregge dobbiamo garantire immunità “I migranti non ci portano malattie” TagadaLa7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>161</td>\n",
              "      <td>4</td>\n",
              "      <td>[#TagadaLa7]</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>[Per, questo, ci, avete, imposto, l, ’, obbligo, vaccinale, a, prescindere, Già, noi, il, gregge, dobbiamo, garantire, immunità, “, I, migranti, non, ci, portano, malattie, ”, TagadaLa7]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>12164</td>\n",
              "      <td>fare accordo con un paese africano qualsiasi e spedire là tutti gli afroislamici che i loro paesi non vogliono È follia tenere in casa criminali stranieri a zonzo</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>207</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>[fare, accordo, con, un, paese, africano, qualsiasi, e, spedire, là, tutti, gli, afroislamici, che, i, loro, paesi, non, vogliono, È, follia, tenere, in, casa, criminali, stranieri, a, zonzo]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>11998</td>\n",
              "      <td>Reggio emi li a Ed ecco a voi la triste realtà che auguro ai babbuini degli accoglioni CITTA ASSEDIATE da immigrati ma sindaci PD fanno finta di nulla e per fare becera propaganda si mettono anche contro il decreto sicurezza Patetici pagliacci verranno spazzati via alle urne</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>284</td>\n",
              "      <td>13</td>\n",
              "      <td>[#ReggioEmilia, #IMMIGRATI, #SINDACI, #PD, #DecretoSicurezza.]</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Reggio, emi, li, a, Ed, ecco, a, voi, la, triste, realtà, che, auguro, ai, babbuini, degli, accoglioni, CITTA, ASSEDIATE, da, immigrati, ma, sindaci, PD, fanno, finta, di, nulla, e, per, fare, becera, propaganda, si, mettono, anche, contro, il, decreto, sicurezza, Patetici, pagliacci, verranno, spazzati, via, alle, urne]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>11969</td>\n",
              "      <td>rivoluzione nazionale In Italia è di moda impiegare il termine fascista contro chiunque risulti avverso allinvasione Basta migranti in Italia ne abbiamo accolti fin troppi prima gli italiani</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>204</td>\n",
              "      <td>3</td>\n",
              "      <td>[#rivoluzionenazionale, #primagliitaliani]</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[rivoluzione, nazionale, In, Italia, è, di, moda, impiegare, il, termine, fascista, contro, chiunque, risulti, avverso, allinvasione, Basta, migranti, in, Italia, ne, abbiamo, accolti, fin, troppi, prima, gli, italiani]</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>11950</td>\n",
              "      <td>porta porta La Chiesa Valdese quanto tempo tratterrà i migranti salvati dalle guerre Non faranno mica la fine del pesce che dopo tre giorni puzza Unica soluzione respingere persone chiudere i porti Basta farse non zerbinarti ai 5S</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>248</td>\n",
              "      <td>2</td>\n",
              "      <td>[#PortaPorta, #migranti]</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>[porta, porta, La, Chiesa, Valdese, quanto, tempo, tratterrà, i, migranti, salvati, dalle, guerre, Non, faranno, mica, la, fine, del, pesce, che, dopo, tre, giorni, puzza, Unica, soluzione, respingere, persone, chiudere, i, porti, Basta, farse, non, zerbinarti, ai, 5, S]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>12132</td>\n",
              "      <td>Fanega nigeriani altro che profughi vengono qui per delinquere e farci fuori tutti A CASA vostra esseri INUTILI VIOLENTIIGNORANTI I nostri figli vanno al estero laureati a lavare piatti per mantenersi mentre cercano un lavoro</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>243</td>\n",
              "      <td>13</td>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>[Fanega, nigeriani, altro, che, profughi, vengono, qui, per, delinquere, e, farci, fuori, tutti, A, CASA, vostra, esseri, INUTILI, VIOLENTIIGNORANTI, I, nostri, figli, vanno, al, estero, laureati, a, lavare, piatti, per, mantenersi, mentre, cercano, un, lavoro]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... %bad_words\n",
              "0   11834  ...          0\n",
              "1   12113  ...          0\n",
              "2   11770  ...          2\n",
              "3   11937  ...          0\n",
              "4   11870  ...          0\n",
              "5   12122  ...          0\n",
              "6   12095  ...          0\n",
              "7   11609  ...          2\n",
              "8   12141  ...          0\n",
              "9   11868  ...          6\n",
              "10  12176  ...          0\n",
              "11  11846  ...          0\n",
              "12  11984  ...          0\n",
              "13  11691  ...          0\n",
              "14  11781  ...          5\n",
              "15  12153  ...          0\n",
              "16  11841  ...          2\n",
              "17  12040  ...          2\n",
              "18  12065  ...          0\n",
              "19  12233  ...          0\n",
              "20  11895  ...          2\n",
              "21  12224  ...          0\n",
              "22  12026  ...          5\n",
              "23  11955  ...          0\n",
              "24  11696  ...          0\n",
              "25  12164  ...          0\n",
              "26  11998  ...          0\n",
              "27  11969  ...          3\n",
              "28  11950  ...          0\n",
              "29  12132  ...          0\n",
              "\n",
              "[30 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzmHMXMI5gYT"
      },
      "source": [
        "dataset_raw.to_csv('test_dataset_tweet.csv', index=False)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNzXv4vCHNr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}