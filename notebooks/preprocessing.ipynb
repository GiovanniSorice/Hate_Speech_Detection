{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBsdd0onaxG4SOC1ogKjaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Hate_Speech_Detection/blob/main/notebooks/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKT-LzHeV8ox"
      },
      "source": [
        " ### Removing Tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv46YwYufRS0",
        "outputId": "ce1f0c1d-fb5f-4173-a718-ce1c543882b8"
      },
      "source": [
        "!pip install tokenizer\n",
        "!pip install ekphrasis\n",
        "!pip install wordninja\n",
        "!pip install emoji\n",
        "!pip install spacy_udpipe\n",
        "!pip install language_tool_python\n",
        "!pip install compound-word-splitter\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install enchant\n",
        "!sudo apt-get install hunspell-it"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizer in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.41.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (5.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (0.4.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: wordninja in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Collecting spacy_udpipe\n",
            "  Using cached https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy_udpipe) (2.2.4)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (53.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.0)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625296 sha256=da0452a34f987840b700f9ddbedad87013c209b5957c3af9b0427461d86a8146\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe\n",
            "Successfully installed spacy-udpipe-0.3.2 ufal.udpipe-1.2.0.3\n",
            "Requirement already satisfied: language_tool_python in /usr/local/lib/python3.6/dist-packages (2.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from language_tool_python) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->language_tool_python) (2020.12.5)\n",
            "Collecting compound-word-splitter\n",
            "  Using cached https://files.pythonhosted.org/packages/7b/0c/62c1ff670016291bd7c24ebed9476ad3cc165eda571ba8442628c636cac1/compound-word-splitter-0.4.tar.gz\n",
            "Building wheels for collected packages: compound-word-splitter\n",
            "  Building wheel for compound-word-splitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compound-word-splitter: filename=compound_word_splitter-0.4-cp36-none-any.whl size=2565 sha256=174a5a3e8f722e1ffcc7c9cd4883314a2118b76d6867877e0218f5e0995468da\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/de/38/8898744bbad2093c1d909ad5b95f221e85877c3e96131f09d1\n",
            "Successfully built compound-word-splitter\n",
            "Installing collected packages: compound-word-splitter\n",
            "Successfully installed compound-word-splitter-0.4\n",
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8c/bd224a5db562ac008edbfaf015f5d5c98ea13e745247cd4ab5fc5b683085/pyenchant-3.2.0-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "hunspell-it is already the newest version (1:6.0.3-3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRjY391AGd-f"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from tokenizer import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "import wordninja\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import ast\n",
        "\n",
        "import emoji\n",
        "import unicodedata\n",
        "\n",
        "import gzip\n",
        "\n",
        "import spacy_udpipe\n",
        "import language_tool_python\n",
        "\n",
        "import splitter\n",
        "import enchant\n",
        "from itertools import groupby\n",
        "import string\n",
        "\n",
        "import sys\n",
        "import os\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQjATsWGdpJW"
      },
      "source": [
        "# directory name:\n",
        "input_dir = '/content/drive/My Drive/HLT/dataset_training/'\n",
        "output_dir = '/content/drive/My Drive/HLT/clean_dataset_training/'\n",
        "input_dir_preprocessing = '/content/drive/My Drive/HLT/preprocessing/'\n",
        "\n",
        "# Spec\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhfNkQOddzQ9",
        "outputId": "9155d220-5edf-4153-8560-7581293b9a14"
      },
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeSkmao9eW02"
      },
      "source": [
        "#tsv_tweets_file = open(\"/content/drive/My Drive/HLT/dataset/haspeede2_reference_taskAB-tweets.tsv\")\n",
        "#tsv_news_file = open(\"/content/drive/My Drive/HLT/dataset/haspeede2_reference_taskAB-news.tsv\")\n",
        "tsv_file = open(input_dir+\"haspeede2_dev_taskAB.tsv\")\n",
        "\n",
        "#dataset_tweets_raw = pd.read_csv(tsv_tweets_file,sep='\\t')\n",
        "#dataset_tweets_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "#dataset_news_raw = pd.read_csv(tsv_news_file,sep='\\t')\n",
        "#dataset_news_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "dataset_raw = pd.read_csv(tsv_file,sep='\\t')\n",
        "dataset_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "\n",
        "#Bad Words\n",
        "f2 = open(input_dir_preprocessing+\"bad_words.csv\", 'r', encoding='utf8')\n",
        "\n",
        "bad_words_set = [] #list of lowercase words\n",
        "\n",
        "for x in f2:\n",
        "    y = x.rstrip()\n",
        "    y = y.lower()\n",
        "    if y != '':\n",
        "        bad_words_set.append(y)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPBGTK1PiGEx"
      },
      "source": [
        "# Preprocessing phase\n",
        "Following the \"Preprocessing\" section of https://books.openedition.org/aaccademia/4832?lang=it#tocfrom1n4 we will perform:\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTSJjfRgjZ8s"
      },
      "source": [
        "### Extraction of the first feature: length of the comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hDGiCnLhlHB"
      },
      "source": [
        "def comment_length(text):\n",
        "    return len(text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF0sQvOrjqDN"
      },
      "source": [
        "dataset_raw['text_length'] = dataset_raw['text'].apply(comment_length)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTIhhE0Jjzqq"
      },
      "source": [
        "Extraction of the second feature: percentage of words written in CAPS-LOCK inside the comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8G1vCSnj4ML"
      },
      "source": [
        "def caps_lock_words(text):\n",
        "    words = text.split()\n",
        "    count_caps_lock = 0\n",
        "    number_of_words = len(words)\n",
        "    \n",
        "    for word in words:\n",
        "        if word.isupper() == True:\n",
        "            count_caps_lock = count_caps_lock + 1\n",
        "            \n",
        "    return ((count_caps_lock*100)//number_of_words)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeH2idQEj66P"
      },
      "source": [
        "dataset_raw['#C-L words'] = dataset_raw['text'].apply(caps_lock_words)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26m0-vh5JEOB"
      },
      "source": [
        " ### Removing Tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZTSDBK4JNIl"
      },
      "source": [
        "def clean_tag(text):\n",
        "    return re.sub(\n",
        "        r'@user', ' ', text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22IGkgdXJPRT"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_tag)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpIlGt9OI5je"
      },
      "source": [
        "### Replace the characters ‘&’, ‘@’ respectively in the letters ‘e’, ‘a’"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xZ02VkZJUX5"
      },
      "source": [
        "def replace_e_a(text):\n",
        "    text = re.sub(r'&', 'e', text)\n",
        "    return re.sub(r'@', 'a', text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vURkcOrHJgiQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(replace_e_a)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9T6so5NJ21c"
      },
      "source": [
        "### Conversion of disguised bad words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joAwdJKBJ9Vw"
      },
      "source": [
        "def clean_disguised_bad_words(text):\n",
        "    text = \" \" + text + \" \"\n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' coglioni ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+e ', ' coglione ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+o ', ' cazzo ', text) \n",
        "    text = re.sub(r' ca[.x*@%#$^]+ro ', ' cazzaro ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' cazzi ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+a ', ' merda ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+e ', ' merde ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+ulo ', ' culo ', text) \n",
        "    text = re.sub(r' p[.x*@%#$^]+a ', ' puttana ', text)\n",
        "    text = re.sub(r' p[.x*@%#$^]+e ', ' puttane ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+a ', ' troia ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+e ', ' troie ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+o ', ' stronzo ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+i ', ' stronzi ', text)\n",
        "    return text"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjzGx0uFJ9y5"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_disguised_bad_words)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzeCUAIEKX5x"
      },
      "source": [
        "### Hashtag splitting and saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy7M8Rw3VltO"
      },
      "source": [
        "def find_hashtags(text):\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    if result:\n",
        "        return result\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mtFua9FVlCk"
      },
      "source": [
        "dataset_raw['hashtags'] = dataset_raw['text'].apply(find_hashtags)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHHr66WzQ7mJ"
      },
      "source": [
        "def split_hashtags(text):\n",
        "    \n",
        "    text = ' ' + text + ' '\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    \n",
        "    for word in result:\n",
        "        new_word = \" \".join(splitter.split(word[1:].lower(), 'it_IT'))\n",
        "        if len(new_word)==0:\n",
        "            new_word =  word[1:]\n",
        "                  \n",
        "        text = text.replace(word, new_word)\n",
        "        \n",
        "    return text"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNR6YmYRFlQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(split_hashtags)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZdFjV6zWCJ3"
      },
      "source": [
        " ### Removing URL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMMZC6EaWB92"
      },
      "source": [
        "def clean_URL(text):\n",
        "    return re.sub(r'URL', ' ', text)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GInlm-4NWByp"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_URL)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vw02ux1d8xr"
      },
      "source": [
        "### Extraction of the fourth feature: number of ‘?’ or ‘!’ inside the comment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAilG6e9d_Kv"
      },
      "source": [
        "def esclamations_and_questions(text):\n",
        "    return text.count('!') + text.count('?')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8aPhymFd_W4"
      },
      "source": [
        "dataset_raw['#?!'] = dataset_raw['text'].apply(esclamations_and_questions)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ODCBvIweoBT"
      },
      "source": [
        "### Extraction of the fifth feature: number of ‘.’ or ‘,’ inside the comment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4cXR155eoBV"
      },
      "source": [
        "def esclamations_and_questions(text):\n",
        "    return text.count(',') + text.count('.')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o2NoyA-eoBW"
      },
      "source": [
        "dataset_raw['#.,'] = dataset_raw['text'].apply(esclamations_and_questions)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2R9v14KfNvS"
      },
      "source": [
        "### Punctuation removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOfbQApfQq4"
      },
      "source": [
        "def strip_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJruDmPfQ4M"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(strip_punctuation)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jld5EAWV11"
      },
      "source": [
        "### Removal of nearby equal vowels, removal of nearby equal consonants if they are more than 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At0CUlq9akCI"
      },
      "source": [
        "correct_words_vowels = ['sciiti,',\n",
        " 'welcomerefugees',\n",
        " 'livetweet',\n",
        " 'desiree',\n",
        " 'canaan',\n",
        " 'tweet.',\n",
        " 'weekend',\n",
        " 'romafeyenoord',\n",
        " 'ebree,',\n",
        " 'greencard',\n",
        " 'creerà',\n",
        " 'cooperative.',\n",
        " 'moschee,',\n",
        " 'cooperanti',\n",
        " 'streetart',\n",
        " 'khalidmasood',\n",
        " 'tweet',\n",
        " 'woolfe',\n",
        " 'cooperazione',\n",
        " 'coop',\n",
        " 'seehofer',\n",
        " 'speech',\n",
        " 'coffee',\n",
        " 'scooter',\n",
        " 'street',\n",
        " 'veemenza',\n",
        " 'moschee.',\n",
        " 'maalox.',\n",
        " 'book',\n",
        " 'tweet',\n",
        " 'facebook:',\n",
        " 'sociial,',\n",
        " 'coop,',\n",
        " 'canaaniti.',\n",
        " 'europee,',\n",
        " 'cooperative',\n",
        " 'google',\n",
        " 'creeranno',\n",
        " 'mediterranee',\n",
        " 'cooperazione',\n",
        " 'cooperativa',\n",
        " '“boom”',\n",
        " 'refugees',\n",
        " 'moonlight',\n",
        " 'imaam',\n",
        " 'shooting',\n",
        " 'sciiti',\n",
        " 'sunniti',\n",
        " 'book',\n",
        " 'atee.',\n",
        " 'looking',\n",
        " 'week',\n",
        " 'ayaan',\n",
        " 'temporanee.',\n",
        " 'idee.',\n",
        " 'sibiliini',\n",
        " 'food',\n",
        " 'refugees',\n",
        " 'retweeted',\n",
        " 'boom',\n",
        " 'keep',\n",
        " 'vodoo',\n",
        " 'hooligans',\n",
        " 'ebree',\n",
        " 'refugees',\n",
        " 'speed',\n",
        " 'bloomberg',\n",
        " 'riina',\n",
        " 'hatespeech',\n",
        " 'google',\n",
        " 'masood',\n",
        " 'linee.',\n",
        " 'boom']"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFnrr_UqWWQ-"
      },
      "source": [
        "def delete_duplicate_vowels_and_redundant_consonant (text):\n",
        "    parole = text.split()\n",
        "    stringa = \"\"\n",
        "    for a in parole:\n",
        "        parola = a\n",
        "        a = [list(g) for k, g in groupby(a)]    \n",
        "        vocali = ['a','e','i','o','u','y']\n",
        "        \n",
        "        for idx,val in enumerate(a):\n",
        "            if idx == 0:\n",
        "                stringa += a[idx][0] \n",
        "            elif idx == len(a)-1:\n",
        "                stringa += a[idx][0]\n",
        "            elif a[idx][0] in vocali and (parola.lower() not in correct_words_vowels):\n",
        "                stringa += a[idx][0]\n",
        "            elif len(a[idx]) == 1:\n",
        "                stringa += a[idx][0]\n",
        "            elif len (a[idx]) >= 2:\n",
        "                stringa += a[idx][0]\n",
        "                stringa += a[idx][1]\n",
        "        stringa =  stringa + \" \"\n",
        "        \n",
        "    return(stringa)  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol8qT-rhWWM6"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(delete_duplicate_vowels_and_redundant_consonant)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s06b9uii4eN-"
      },
      "source": [
        "### Translation of emoticons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3ctOpMF79JW"
      },
      "source": [
        "def translate_emoticon(text):\n",
        "    text_result = emoji.demojize(text, language='it')\n",
        "    text_result=re.sub(r':', ' ', text_result)\n",
        "    return text_result"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbR6rg3V7-Wt"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(translate_emoticon)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi5I3Y606DVQ"
      },
      "source": [
        "emoticons_text = {\n",
        "    '<kiss>': 'bacio',\n",
        "    '<happy>': 'felice',\n",
        "    '<laugh>': 'risata',\n",
        "    '<sad>': 'triste',\n",
        "    '<surprise>': 'sorpreso',\n",
        "    '<wink>': 'occhiolino',\n",
        "    '<tong>': 'faccia con lingua',\n",
        "    '<annoyed>': 'annoiato',\n",
        "    '<seallips>': 'labbra sigillate',\n",
        "    '<angel>': 'angelo',\n",
        "    '<devil>': 'diavolo',\n",
        "    '<highfive>' : 'batti il cinque',\n",
        "    '<heart>': 'cuore',\n",
        "    '<user>' : 'persona',\n",
        "}"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWgDe3ca6EC6"
      },
      "source": [
        "def clean_emoticon_text(text):\n",
        "    text_words = text.split()\n",
        "    new_words  = [emoticons_text.get(ele, ele) for ele in text_words]\n",
        "    return ' '.join(new_words)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRaxAUC86DJQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_emoticon_text)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM8zUkgjA_Xf"
      },
      "source": [
        "### Replacement of the abbreviations with the respective words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJEKyYL-BMhk"
      },
      "source": [
        "abbr_word = {'cmq':'comunque', 'gov':'governatori', 'fb':'facebook', 'tw':'twitter', 'juve':'juventus', 'ing':'ingegnere', \n",
        "             'sx':'sinistra', 'qdo':'quando', 'rep':'repubblica', 'grz':'grazie', 'ita':'italia', 'mln':'milioni', \n",
        "             'mld':'miliardi', 'pke':'perche', 'anke':'anche', 'cm':'come', 'dlla':'della', 'dlle':'delle', 'qst':'questa',\n",
        "             'ke':'che', 'nn':'non', 'sn':'sono', 'cn':'con', 'xk':'perche', 'xke':'perche', 'art':'articolo',\n",
        "             'tv':'televisore', '€':'euro', 'xché':'perché', 'xké':'perché', 'pkè':'perché'}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNUJsA9xBMsL"
      },
      "source": [
        "def replace_abbreviation(text):\n",
        "    text_words = text.split()\n",
        "    new_text = \"\"\n",
        "    for token in text_words:\n",
        "      new_text  += abbr_word.get(token, token) +\" \"\n",
        "    \n",
        "    return new_text.strip()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRRSxGnEBRSe"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(replace_abbreviation)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcoE_FIQBZDO"
      },
      "source": [
        "### Removal of the laughs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTfCPOK3B9pL"
      },
      "source": [
        "laughs = ['ah', 'eh', 'he' 'ih', 'hi'] #non elimina ahahahah, ma solo ah\n",
        "vowels = ['a', 'e', 'i', 'o', 'u']\n",
        "\n",
        "def clean_laughs(text):\n",
        "    #s = \"ahahahah ho fame io, eh eh\" -> \" ho fame io,\"\n",
        "    text_words = text.split()\n",
        "    new_words  = [word for word in text_words if word not in laughs]\n",
        "    \n",
        "    new_text = ' '.join(new_words)\n",
        "    \n",
        "    for i in new_words:\n",
        "        for k in vowels:\n",
        "            if ('h' in i) and (len(i) >= 4):\n",
        "                if (len(i) - 2) <= (i.count(k) + i.count('h')):\n",
        "                    new_text = new_text.replace(i, '')\n",
        "    \n",
        "    return new_text"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEMhN6UCB90s"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_laughs)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPBk_1WFFbVv"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7TesaZ_Fcyx"
      },
      "source": [
        "def tokenization(text):\n",
        "    tknzr=SocialTokenizer(lowercase=False)\n",
        "    return tknzr.tokenize(text)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BcgiN-cFf25"
      },
      "source": [
        "dataset_raw['tokens'] = dataset_raw['text'].apply(tokenization)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADwlSkQyWH96"
      },
      "source": [
        "### Extraction of the sixth feature: number of bad words in the comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ3Ty8mKWqyG"
      },
      "source": [
        "def number_bad_words(tokens):\n",
        "    n_bad_words = 0\n",
        "\n",
        "    for word in tokens:\n",
        "        if word.lower() in bad_words_set:\n",
        "            n_bad_words = n_bad_words + 1\n",
        "        \n",
        "    return n_bad_words"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOJBDkIxWqk-"
      },
      "source": [
        "dataset_raw['#bad_words'] = dataset_raw['tokens'].apply(number_bad_words)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Vtd6HGWgDJ"
      },
      "source": [
        "### Extraction of the seventh feature: percentage of bad words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_6xdkb0WYds"
      },
      "source": [
        "def percentage_bad_words(tokens):\n",
        "    n_words = 0\n",
        "    n_bad_words = 0\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word != '<' and word != '>':\n",
        "            n_words = n_words + 1\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word.lower() in bad_words_set:\n",
        "            n_bad_words = n_bad_words + 1\n",
        "        \n",
        "    return ((n_bad_words*100)//n_words)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvQRET5VWaxZ"
      },
      "source": [
        "dataset_raw['%bad_words'] = dataset_raw['tokens'].apply(percentage_bad_words)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g_W0gmIpkFKB",
        "outputId": "8b3826db-251d-48c0-955c-bbf29ddc7779"
      },
      "source": [
        "dataset_raw.head(30)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>hs</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>text_length</th>\n",
              "      <th>#C-L words</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>#?!</th>\n",
              "      <th>#.,</th>\n",
              "      <th>tokens</th>\n",
              "      <th>#bad_words</th>\n",
              "      <th>%bad_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2066</td>\n",
              "      <td>È terrorismo anche questo per mettere in uno stato di soggezione le persone e renderle innocue mentre qualcuno</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>10</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>[È, terrorismo, anche, questo, per, mettere, in, uno, stato, di, soggezione, le, persone, e, renderle, innocue, mentre, qualcuno]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2045</td>\n",
              "      <td>infatti finché ci hanno guadagnato con i campi rom tutto era ok con alemanno ipocriti</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>[#rom, #Alemanno, #Ipocriti]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[infatti, finché, ci, hanno, guadagnato, con, i, campi, rom, tutto, era, ok, con, alemanno, ipocriti]</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>61</td>\n",
              "      <td>Corriere Tangenti Mafia Capitale dimenticataMazzette su buche e campi rom rom a</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "      <td>8</td>\n",
              "      <td>[#roma]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Corriere, Tangenti, Mafia, Capitale, dimenticataMazzette, su, buche, e, campi, rom, rom, a]</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1259</td>\n",
              "      <td>ad uno ad uno perché quando i migranti israeliti arrivarono in terra di Canaan fecero fuori tutti i Cananiti</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[ad, uno, ad, uno, perché, quando, i, migranti, israeliti, arrivarono, in, terra, di, Canaan, fecero, fuori, tutti, i, Cananiti]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>949</td>\n",
              "      <td>Il divertimento del giorno Trovare i patrioti italiani che inneggiano contro i rom facendo la spesa alla li dl multinazionale tedesca</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>0</td>\n",
              "      <td>[#Lidl]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Il, divertimento, del, giorno, Trovare, i, patrioti, italiani, che, inneggiano, contro, i, rom, facendo, la, spesa, alla, li, dl, multinazionale, tedesca]</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>256</td>\n",
              "      <td>Modena Comune paga la benzina ai nomadi che portano figli a scuola MODENA – La giunta PD…</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>96</td>\n",
              "      <td>16</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Modena, Comune, paga, la, benzina, ai, nomadi, che, portano, figli, a, scuola, MODENA, –, La, giunta, PD, …]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3001</td>\n",
              "      <td>altro che islam o cristianesimo a c dobbiamo sorbire anche dellignorante islam è uno solo</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>[altro, che, islam, o, cristianesimo, a, c, dobbiamo, sorbire, anche, dellignorante, islam, è, uno, solo]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>765</td>\n",
              "      <td>grazie stef stavo giusto caricando ho anche messo Che Salvini avallava il finanziare campi rom</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>113</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[grazie, stef, stavo, giusto, caricando, ho, anche, messo, Che, Salvini, avallava, il, finanziare, campi, rom]</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1157</td>\n",
              "      <td>e smettetela di dire che anche gli italiani sono stati migranti erano trattati male ma non per questo uccidevano innocenti per strada</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>[e, smettetela, di, dire, che, anche, gli, italiani, sono, stati, migranti, erano, trattati, male, ma, non, per, questo, uccidevano, innocenti, per, strada]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>268</td>\n",
              "      <td>Minorenne rom arrestato dopo furto in appartamento</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>55</td>\n",
              "      <td>12</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Minorenne, rom, arrestato, dopo, furto, in, appartamento]</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>644</td>\n",
              "      <td>Studentessa cinese morta fermato un nomade di 20anni e denunciato nomade sedicenne tutti del vicino campo nomade SkyTG24</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>121</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Studentessa, cinese, morta, fermato, un, nomade, di, 2, 0, anni, e, denunciato, nomade, sedicenne, tutti, del, vicino, campo, nomade, SkyTG24]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2518</td>\n",
              "      <td>Ddl di Calderoli “Il velo islamico e l’apologia della Sharia diventino reato” Siete d’accordo</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>6</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Ddl, di, Calderoli, “, Il, velo, islamico, e, l, ’, apologia, della, Sharia, diventino, reato, ”, Siete, d, ’, accordo]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>46</td>\n",
              "      <td>Roma rapina al campo rom La Barbuta tre arresti Lombra del racket</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>78</td>\n",
              "      <td>14</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[Roma, rapina, al, campo, rom, La, Barbuta, tre, arresti, Lombra, del, racket]</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2297</td>\n",
              "      <td>Rischiando di essere presa per pazza che non sa di cosa parlaTrump dopo i musulmani e i gayquelli di colorebenvenutoHitlerpreoccupante</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>[#Trump, #benvenutoHitler?#preoccupante]</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[Rischiando, di, essere, presa, per, pazza, che, non, sa, di, cosa, parlaTrump, dopo, i, musulmani, e, i, gayquelli, di, colorebenvenutoHitlerpreoccupante]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>181</td>\n",
              "      <td>Povertà esclusione e scarsa istruzione 410 minori rom hanno una vita segnata alla nascita</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>99</td>\n",
              "      <td>6</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Povertà, esclusione, e, scarsa, istruzione, 410, minori, rom, hanno, una, vita, segnata, alla, nascita]</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>779</td>\n",
              "      <td>rom a Torino grazie al collegamento con i campi rom di QuintaColonna su media set televisore stasera calo esponenziale dei furti del debbio</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>[#Roma, #Torino,, #rom, #QuintaColonna,, #mediaset, #tv,, #furti., #DelDebbio]</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>[rom, a, Torino, grazie, al, collegamento, con, i, campi, rom, di, QuintaColonna, su, media, set, televisore, stasera, calo, esponenziale, dei, furti, del, debbio]</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>285</td>\n",
              "      <td>lo zucchero te lo da il tuo amico negro oppure rom visto che vi fate tutti senza problemi</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[lo, zucchero, te, lo, da, il, tuo, amico, negro, oppure, rom, visto, che, vi, fate, tutti, senza, problemi]</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1633</td>\n",
              "      <td>profughi e ci sono ancora dei malvedenti malpensanti e maldicenti che parlano di donne e bambini</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>110</td>\n",
              "      <td>5</td>\n",
              "      <td>[#profughi]</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>[profughi, e, ci, sono, ancora, dei, malvedenti, malpensanti, e, maldicenti, che, parlano, di, donne, e, bambini]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2900</td>\n",
              "      <td>Sarà un caso che è stata colpita la città occidentale della Russia san Pietroburgo è civiltà che da fastidio allinciviltá terroristica</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>[#sanpietroburgo]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[Sarà, un, caso, che, è, stata, colpita, la, città, occidentale, della, Russia, san, Pietroburgo, è, civiltà, che, da, fastidio, allinciviltá, terroristica]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2855</td>\n",
              "      <td>Toscana regalati 30mila euro ai rom per fare la spesa in una Coop</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>71</td>\n",
              "      <td>7</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Toscana, regalati, 3, 0, mila, euro, ai, rom, per, fare, la, spesa, in, una, Coop]</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1770</td>\n",
              "      <td>Commissione Ue per visti turchi a mancano ancora 7 criteri stallo su legge antiterrorismo Accordo migranti 187 i rimpatri da aprile</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>[#Ue,, #Turchia]</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>[Commissione, Ue, per, visti, turchi, a, mancano, ancora, 7, criteri, stallo, su, legge, antiterrorismo, Accordo, migranti, 187, i, rimpatri, da, aprile]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1200</td>\n",
              "      <td>Quante inutili parole E ora di eliminare tutti questi politici immigrazionisti il mondo senza questi delinquenti…</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>121</td>\n",
              "      <td>11</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Quante, inutili, parole, E, ora, di, eliminare, tutti, questi, politici, immigrazionisti, il, mondo, senza, questi, delinquenti, …]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1181</td>\n",
              "      <td>Milano Lodi raggirava e truffava i clienti stranieri arrestato un avvocato</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>83</td>\n",
              "      <td>8</td>\n",
              "      <td>[#Milano:]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Milano, Lodi, raggirava, e, truffava, i, clienti, stranieri, arrestato, un, avvocato]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1062</td>\n",
              "      <td>INTERVISTA «Immorale il memorandum migranti siglato da Roma e Tripoli»</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "      <td>18</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[INTERVISTA, «, Immorale, il, memorandum, migranti, siglato, da, Roma, e, Tripoli, »]</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1922</td>\n",
              "      <td>Aleppo libera dai terroristi filo ci dentali torna alla vita Riaperto aeroporto faccina_con_sorriso_accennato</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>87</td>\n",
              "      <td>10</td>\n",
              "      <td>[#AleppoLibera, #terroristifiloccidentali, #aeroporto🙂]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Aleppo, libera, dai, terroristi, filo, ci, dentali, torna, alla, vita, Riaperto, aeroporto, faccina_con_sorriso_accennato]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>880</td>\n",
              "      <td>Quello che n si sopporta è il risultato della libera circolazionenomadi degli altri paesi sul groppone</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>117</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Quello, che, n, si, sopporta, è, il, risultato, della, libera, circolazionenomadi, degli, altri, paesi, sul, groppone]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1342</td>\n",
              "      <td>Inchiesta a torino Trovato in Croazia il tesoro della “regina” rom</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>7</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Inchiesta, a, torino, Trovato, in, Croazia, il, tesoro, della, “, regina, ”, rom]</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1137</td>\n",
              "      <td>a favore degli immigrati Ma hanno visto in altri Paesi europei dove han fatto quella marcia poi cosè successo</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>121</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>[a, favore, degli, immigrati, Ma, hanno, visto, in, altri, Paesi, europei, dove, han, fatto, quella, marcia, poi, cosè, successo]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>625</td>\n",
              "      <td>Ma non sono nomadisono stanzialin Francia ogni 15 giorni devono andare via per amore o per forza</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>[Ma, non, sono, nomadisono, stanzialin, Francia, ogni, 15, giorni, devono, andare, via, per, amore, o, per, forza]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2537</td>\n",
              "      <td>ITALIANI via dallItalia questo territorio ormai è dei profughi</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>79</td>\n",
              "      <td>20</td>\n",
              "      <td>None</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>[ITALIANI, via, dallItalia, questo, territorio, ormai, è, dei, profughi]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... %bad_words\n",
              "0   2066  ...          0\n",
              "1   2045  ...          6\n",
              "2     61  ...         25\n",
              "3   1259  ...          0\n",
              "4    949  ...          4\n",
              "5    256  ...          0\n",
              "6   3001  ...          0\n",
              "7    765  ...          6\n",
              "8   1157  ...          0\n",
              "9    268  ...         14\n",
              "10   644  ...          0\n",
              "11  2518  ...          0\n",
              "12    46  ...          8\n",
              "13  2297  ...          0\n",
              "14   181  ...          7\n",
              "15   779  ...          8\n",
              "16   285  ...         11\n",
              "17  1633  ...          0\n",
              "18  2900  ...          0\n",
              "19  2855  ...          6\n",
              "20  1770  ...          0\n",
              "21  1200  ...          0\n",
              "22  1181  ...          0\n",
              "23  1062  ...          8\n",
              "24  1922  ...          0\n",
              "25   880  ...          0\n",
              "26  1342  ...          7\n",
              "27  1137  ...          0\n",
              "28   625  ...          0\n",
              "29  2537  ...          0\n",
              "\n",
              "[30 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzmHMXMI5gYT"
      },
      "source": [
        "dataset_raw.to_csv('training_dataset.csv', index=False)"
      ],
      "execution_count": 56,
      "outputs": []
    }
  ]
}