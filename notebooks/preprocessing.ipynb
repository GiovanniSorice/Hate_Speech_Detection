{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOrxATrul9v1UMt10Kerffh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Hate_Speech_Detection/blob/main/notebooks/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKT-LzHeV8ox"
      },
      "source": [
        " ### Removing Tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv46YwYufRS0",
        "outputId": "da499aa6-7e66-4751-e589-6e4ff2ffda8e"
      },
      "source": [
        "!pip install tokenizer\n",
        "!pip install ekphrasis\n",
        "!pip install wordninja\n",
        "!pip install emoji\n",
        "!pip install spacy_udpipe\n",
        "!pip install language_tool_python\n",
        "!pip install compound-word-splitter\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install enchant\n",
        "!sudo apt-get install hunspell-it"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/88/76356c78aef6b527db91362ed92a0da2d18a0271921d79559fc0bc8c49c3/tokenizer-2.4.0-py2.py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 21.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 71kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 5.4MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizer\n",
            "Successfully installed tokenizer-2.4.0\n",
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/54/2a618356e54282e3f4fa0745cbddeed104ed5ff2930c98f2d9dfa9677af8/ujson-4.0.2-cp36-cp36m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp36-none-any.whl size=82844 sha256=949735b5859b4db13243afc5ea9df9c96636e172078794ef42205a79fd251759\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp36-none-any.whl size=46451 sha256=503f8fd001c7969561517b0e9a0ddbb12750dbd14b618507afd13b024af58f50\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-5.9 ujson-4.0.2\n",
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 4.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp36-none-any.whl size=541553 sha256=1d302dd58cdf1f0b63a33ff5ed2ad69b556064dca23ba2fabfd8a27c452eeec5\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 6.0MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n",
            "Collecting spacy_udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy_udpipe) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (53.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.0)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625205 sha256=cd0001da15c308ae3c5b99a08195d1fb7aa0b5d1cc198d3aca7230f43d5a187a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe\n",
            "Successfully installed spacy-udpipe-0.3.2 ufal.udpipe-1.2.0.3\n",
            "Collecting language_tool_python\n",
            "  Downloading https://files.pythonhosted.org/packages/13/bd/ff1e1a9a78128d787a959f0a46c39601f21e354712fd4b330278e2af831a/language_tool_python-2.5.2-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from language_tool_python) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->language_tool_python) (2020.12.5)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.5.2\n",
            "Requirement already satisfied: compound-word-splitter in /usr/local/lib/python3.6/dist-packages (0.4)\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.6/dist-packages (3.2.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "hunspell-it is already the newest version (1:6.0.3-3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRjY391AGd-f"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from tokenizer import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "import wordninja\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import ast\n",
        "\n",
        "import emoji\n",
        "import unicodedata\n",
        "\n",
        "import gzip\n",
        "\n",
        "import spacy_udpipe\n",
        "import language_tool_python\n",
        "\n",
        "import splitter\n",
        "import enchant\n",
        "from itertools import groupby\n",
        "\n",
        "import sys\n",
        "import os\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQjATsWGdpJW"
      },
      "source": [
        "# directory name:\n",
        "input_dir = '/content/drive/My Drive/HLT/dataset_training/'\n",
        "output_dir = '/content/drive/My Drive/HLT/clean_dataset_training/'\n",
        "\n",
        "# Spec\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhfNkQOddzQ9",
        "outputId": "ba8ef801-054b-40c6-89b0-74cf8f51a187"
      },
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeSkmao9eW02"
      },
      "source": [
        "#tsv_tweets_file = open(\"/content/drive/My Drive/HLT/dataset/haspeede2_reference_taskAB-tweets.tsv\")\n",
        "#tsv_news_file = open(\"/content/drive/My Drive/HLT/dataset/haspeede2_reference_taskAB-news.tsv\")\n",
        "tsv_file = open(input_dir+\"haspeede2_dev_taskAB.tsv\")\n",
        "\n",
        "#dataset_tweets_raw = pd.read_csv(tsv_tweets_file,sep='\\t')\n",
        "#dataset_tweets_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "#dataset_news_raw = pd.read_csv(tsv_news_file,sep='\\t')\n",
        "#dataset_news_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "dataset_raw = pd.read_csv(tsv_file,sep='\\t')\n",
        "dataset_raw.rename(columns={\"text \": \"text\"}, inplace=True)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPBGTK1PiGEx"
      },
      "source": [
        "# Preprocessing phase\n",
        "Following the \"Preprocessing\" section of https://books.openedition.org/aaccademia/4832?lang=it#tocfrom1n4 we will perform:\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTSJjfRgjZ8s"
      },
      "source": [
        "### Extraction of the first feature: length of the comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hDGiCnLhlHB"
      },
      "source": [
        "def comment_length(text):\n",
        "    return len(text)"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF0sQvOrjqDN"
      },
      "source": [
        "dataset_raw['text_length'] = dataset_raw['text'].apply(comment_length)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTIhhE0Jjzqq"
      },
      "source": [
        "Extraction of the second feature: percentage of words written in CAPS-LOCK inside the comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8G1vCSnj4ML"
      },
      "source": [
        "def caps_lock_words(text):\n",
        "    words = text.split()\n",
        "    count_caps_lock = 0\n",
        "    number_of_words = len(words)\n",
        "    \n",
        "    for word in words:\n",
        "        if word.isupper() == True:\n",
        "            count_caps_lock = count_caps_lock + 1\n",
        "            \n",
        "    return ((count_caps_lock*100)//number_of_words)"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeH2idQEj66P"
      },
      "source": [
        "dataset_raw['#C-L words'] = dataset_raw['text'].apply(caps_lock_words)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26m0-vh5JEOB"
      },
      "source": [
        " ###Removing Tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZTSDBK4JNIl"
      },
      "source": [
        "def clean_tag(text):\n",
        "    return re.sub(\n",
        "        r'@user', ' ', text)"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22IGkgdXJPRT"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_tag)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpIlGt9OI5je"
      },
      "source": [
        "### Replace the characters ‘&’, ‘@’ respectively in the letters ‘e’, ‘a’"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xZ02VkZJUX5"
      },
      "source": [
        "def replace_e_a(text):\n",
        "    text = re.sub(r'&', 'e', text)\n",
        "    return re.sub(r'@', 'a', text)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vURkcOrHJgiQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(replace_e_a)"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9T6so5NJ21c"
      },
      "source": [
        "### Conversion of disguised bad words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joAwdJKBJ9Vw"
      },
      "source": [
        "def clean_disguised_bad_words(text):\n",
        "    text = \" \" + text + \" \"\n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' coglioni ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+e ', ' coglione ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+o ', ' cazzo ', text) \n",
        "    text = re.sub(r' ca[.x*@%#$^]+ro ', ' cazzaro ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' cazzi ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+a ', ' merda ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+e ', ' merde ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+ulo ', ' culo ', text) \n",
        "    text = re.sub(r' p[.x*@%#$^]+a ', ' puttana ', text)\n",
        "    text = re.sub(r' p[.x*@%#$^]+e ', ' puttane ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+a ', ' troia ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+e ', ' troie ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+o ', ' stronzo ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+i ', ' stronzi ', text)\n",
        "    return text"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjzGx0uFJ9y5"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_disguised_bad_words)"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzeCUAIEKX5x"
      },
      "source": [
        "### Hashtag splitting and saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy7M8Rw3VltO"
      },
      "source": [
        "def find_hashtags(text):\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    if result:\n",
        "        return result\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mtFua9FVlCk"
      },
      "source": [
        "dataset_raw['hashtags'] = dataset_raw['text'].apply(find_hashtags)"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHHr66WzQ7mJ"
      },
      "source": [
        "def split_hashtags(text):\n",
        "    \n",
        "    text = ' ' + text + ' '\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    \n",
        "    for word in result:\n",
        "        new_word = \" \".join(splitter.split(word[1:].lower(), 'it_IT'))\n",
        "        if len(new_word)==0:\n",
        "            new_word =  word[1:]\n",
        "                  \n",
        "        text = text.replace(word, new_word)\n",
        "        \n",
        "    return text"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNR6YmYRFlQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(split_hashtags)"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZdFjV6zWCJ3"
      },
      "source": [
        " ### Removing URL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMMZC6EaWB92"
      },
      "source": [
        "def clean_URL(text):\n",
        "    return re.sub(r'URL', ' ', text)"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GInlm-4NWByp"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_URL)"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vw02ux1d8xr"
      },
      "source": [
        "### Extraction of the fourth feature: number of ‘?’ or ‘!’ inside the comment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAilG6e9d_Kv"
      },
      "source": [
        "def esclamations_and_questions(text):\n",
        "    return text.count('!') + text.count('?')"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8aPhymFd_W4"
      },
      "source": [
        "dataset_raw['#?!'] = dataset_raw['text'].apply(esclamations_and_questions)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ODCBvIweoBT"
      },
      "source": [
        "### Extraction of the fifth feature: number of ‘.’ or ‘,’ inside the comment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4cXR155eoBV"
      },
      "source": [
        "def esclamations_and_questions(text):\n",
        "    return text.count(',') + text.count('.')"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o2NoyA-eoBW"
      },
      "source": [
        "dataset_raw['#.,'] = dataset_raw['text'].apply(esclamations_and_questions)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jld5EAWV11"
      },
      "source": [
        "### Removal of nearby equal vowels, removal of nearby equal consonants if they are more than 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At0CUlq9akCI"
      },
      "source": [
        "correct_words_vowels = ['sciiti,',\n",
        " 'welcomerefugees',\n",
        " 'livetweet',\n",
        " 'desiree',\n",
        " 'canaan',\n",
        " 'tweet.',\n",
        " 'weekend',\n",
        " 'romafeyenoord',\n",
        " 'ebree,',\n",
        " 'greencard',\n",
        " 'creerà',\n",
        " 'cooperative.',\n",
        " 'moschee,',\n",
        " 'cooperanti',\n",
        " 'streetart',\n",
        " 'khalidmasood',\n",
        " 'tweet',\n",
        " 'woolfe',\n",
        " 'cooperazione',\n",
        " 'coop',\n",
        " 'seehofer',\n",
        " 'speech',\n",
        " 'coffee',\n",
        " 'scooter',\n",
        " 'street',\n",
        " 'veemenza',\n",
        " 'moschee.',\n",
        " 'maalox.',\n",
        " 'book',\n",
        " 'tweet',\n",
        " 'facebook:',\n",
        " 'sociial,',\n",
        " 'coop,',\n",
        " 'canaaniti.',\n",
        " 'europee,',\n",
        " 'cooperative',\n",
        " 'google',\n",
        " 'creeranno',\n",
        " 'mediterranee',\n",
        " 'cooperazione',\n",
        " 'cooperativa',\n",
        " '“boom”',\n",
        " 'refugees',\n",
        " 'moonlight',\n",
        " 'imaam',\n",
        " 'shooting',\n",
        " 'sciiti',\n",
        " 'sunniti',\n",
        " 'book',\n",
        " 'atee.',\n",
        " 'looking',\n",
        " 'week',\n",
        " 'ayaan',\n",
        " 'temporanee.',\n",
        " 'idee.',\n",
        " 'sibiliini',\n",
        " 'food',\n",
        " 'refugees',\n",
        " 'retweeted',\n",
        " 'boom',\n",
        " 'keep',\n",
        " 'vodoo',\n",
        " 'hooligans',\n",
        " 'ebree',\n",
        " 'refugees',\n",
        " 'speed',\n",
        " 'bloomberg',\n",
        " 'riina',\n",
        " 'hatespeech',\n",
        " 'google',\n",
        " 'masood',\n",
        " 'linee.',\n",
        " 'boom']"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFnrr_UqWWQ-"
      },
      "source": [
        "def delete_duplicate_vowels_and_redundant_consonant (text):\n",
        "    parole = text.split()\n",
        "    stringa = \"\"\n",
        "    for a in parole:\n",
        "        parola = a\n",
        "        a = [list(g) for k, g in groupby(a)]    \n",
        "        vocali = ['a','e','i','o','u','y']\n",
        "        \n",
        "        for idx,val in enumerate(a):\n",
        "            if idx == 0:\n",
        "                stringa += a[idx][0] \n",
        "            elif idx == len(a)-1:\n",
        "                stringa += a[idx][0]\n",
        "            elif a[idx][0] in vocali and (parola.lower() not in correct_words_vowels):\n",
        "                if len(a[idx])>1: \n",
        "                  print(parola)\n",
        "                stringa += a[idx][0]\n",
        "            elif len(a[idx]) == 1:\n",
        "                stringa += a[idx][0]\n",
        "            elif len (a[idx]) >= 2:\n",
        "                stringa += a[idx][0]\n",
        "                stringa += a[idx][1]\n",
        "        stringa =  stringa + \" \"\n",
        "        \n",
        "    return(stringa)  "
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol8qT-rhWWM6"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(delete_duplicate_vowels_and_redundant_consonant)"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2R9v14KfNvS"
      },
      "source": [
        "### Punctuation removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOfbQApfQq4"
      },
      "source": [
        "def clean_punctuation(text):\n",
        "    text = ' ' + text + ' '\n",
        "    text = re.sub(r'\\\\n', '. ', text)\n",
        "    text = re.sub(r'\\\\', ' ', text)\n",
        "    text = re.sub(r'/', ' ', text)\n",
        "    return re.sub(r'_', ' ', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJruDmPfQ4M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "g_W0gmIpkFKB",
        "outputId": "8f31a4d0-51f1-4bb9-da84-eb8f0e67607f"
      },
      "source": [
        "dataset_raw"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>hs</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>text_length</th>\n",
              "      <th>#C-L words</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>#?!</th>\n",
              "      <th>#.,</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2066</td>\n",
              "      <td>È terrorismo anche questo, per mettere in uno stato di soggezione le persone e renderle innocue, mentre qualcuno.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>10</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2045</td>\n",
              "      <td>infatti finché ci hanno guadagnato con i campi rom tutto era ok con alemanno ipocriti</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>[#rom, #Alemanno, #Ipocriti]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>61</td>\n",
              "      <td>Corriere: Tangenti, Mafia Capitale dimenticataMazzette su buche e campi rom rom a</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>86</td>\n",
              "      <td>8</td>\n",
              "      <td>[#roma]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1259</td>\n",
              "      <td>ad uno ad uno, perché quando i migranti israeliti arrivarono in terra di Canan fecero fuori tutti i Cananiti.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>949</td>\n",
              "      <td>Il divertimento del giorno? Trovare i patrioti italiani che inneggiano contro i rom facendo la spesa alla li dl (multinazionale tedesca).</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>0</td>\n",
              "      <td>[#Lidl]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6832</th>\n",
              "      <td>9340</td>\n",
              "      <td>Gli stati nazionali devono essere pronti a rinunciare alla propria sovranità. Lo ha detto la Merkel , che ha aggiunto che gli stati nazionali non devono ascoltare la volontà dei loro cittadini quando si tratta di questioni che riguardano immigrazione, confini, o persino sovranità</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>285</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6833</th>\n",
              "      <td>9121</td>\n",
              "      <td>Il ministro dell'interno della Germania HorstSehofer,sta facendo la proposta di dare soldi agli immigrati che vogliono tornare a casa e aiutarli a creare un'attività a casa loro e fare busines con la Germania.Chi paga?Una parte i crucchi e il resto l'Europa, cioè io e voi!</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>277</td>\n",
              "      <td>0</td>\n",
              "      <td>[#HorstSeehofer,sta]</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6834</th>\n",
              "      <td>8549</td>\n",
              "      <td>Salvini: In Italia troppi si sono montati la testa, io ringrazio Dio e voi per questi mesi straordinari. Vi raccontavano che su immigrazione non si poteva fare nulla, è bastato usare buonsenso e coraggio. io ci sono piazza del popolo</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>233</td>\n",
              "      <td>0</td>\n",
              "      <td>[#Salvini:, #iocisono, #piazzadelpopolo]</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6835</th>\n",
              "      <td>9240</td>\n",
              "      <td>Chi giubila in buona fede non ha capito niente. Purtroppo credo che i più non siano in buona fede. I migranti sono un grosso busines e chi finora li ha voluti non vuole perdere questo guadagno</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>206</td>\n",
              "      <td>2</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6836</th>\n",
              "      <td>8000</td>\n",
              "      <td>I giovani cristiani in etiopia sono indotti dagli islamisti a convertirsi all'Islam con promesse di lavoro, istruzione e aiuti abitativi. Alcune miniere impiegano solo musulmani. Lo riferisce ad ACS un leader cristiano locale, anonimo per motivi di sicurezza. Preghiamo per loro!</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>285</td>\n",
              "      <td>7</td>\n",
              "      <td>[#Etiopia]</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6837 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... #.,\n",
              "0     2066  ...   3\n",
              "1     2045  ...   0\n",
              "2       61  ...   1\n",
              "3     1259  ...   2\n",
              "4      949  ...   1\n",
              "...    ...  ...  ..\n",
              "6832  9340  ...   4\n",
              "6833  9121  ...   3\n",
              "6834  8549  ...   4\n",
              "6835  9240  ...   2\n",
              "6836  8000  ...   5\n",
              "\n",
              "[6837 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07CpYQggVwUu"
      },
      "source": [
        ""
      ],
      "execution_count": 198,
      "outputs": []
    }
  ]
}