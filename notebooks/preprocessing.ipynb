{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyONH8hH/IoTYehtqAzSZ3sx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Hate_Speech_Detection/blob/main/notebooks/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKT-LzHeV8ox"
      },
      "source": [
        " ### Removing Tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv46YwYufRS0",
        "outputId": "a2370d54-f2bb-45f2-b0d0-a307ceec38b0"
      },
      "source": [
        "!pip install tokenizer\n",
        "!pip install ekphrasis\n",
        "!pip install wordninja\n",
        "!pip install emoji\n",
        "!pip install spacy_udpipe\n",
        "!pip install language_tool_python\n",
        "!pip install compound-word-splitter\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install enchant\n",
        "!sudo apt-get install hunspell-it"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/88/76356c78aef6b527db91362ed92a0da2d18a0271921d79559fc0bc8c49c3/tokenizer-2.4.0-py2.py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 14.3MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 81kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 5.5MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizer\n",
            "Successfully installed tokenizer-2.4.0\n",
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp37-none-any.whl size=82844 sha256=13a871005ecf4347dfb85be5beaffe88ad1028bd784b217c1461c851f8fbdcde\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=d7e7f9fb207dab5c628a52902d9f4f0064c52ad90681270b319d0b7a8d2f895e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-5.9 ujson-4.0.2\n",
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 5.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp37-none-any.whl size=541554 sha256=b12ecccb303efe6719feed22186755ea6b6e7280b7f6dc3d847f67893e7c8a5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 5.5MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n",
            "Collecting spacy_udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy_udpipe) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (53.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.0)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626622 sha256=15d8fddf4ab8000c5313f3868504dbee3dfa5d04dd4ad2ba7346da30b8be8b0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe\n",
            "Successfully installed spacy-udpipe-0.3.2 ufal.udpipe-1.2.0.3\n",
            "Collecting language_tool_python\n",
            "  Downloading https://files.pythonhosted.org/packages/13/bd/ff1e1a9a78128d787a959f0a46c39601f21e354712fd4b330278e2af831a/language_tool_python-2.5.2-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.5.2\n",
            "Collecting compound-word-splitter\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/0c/62c1ff670016291bd7c24ebed9476ad3cc165eda571ba8442628c636cac1/compound-word-splitter-0.4.tar.gz\n",
            "Building wheels for collected packages: compound-word-splitter\n",
            "  Building wheel for compound-word-splitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compound-word-splitter: filename=compound_word_splitter-0.4-cp37-none-any.whl size=2565 sha256=057af126f08384d95854e33ab59ebe36c2395f00e8a0fc56206d4e7832efc19b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/de/38/8898744bbad2093c1d909ad5b95f221e85877c3e96131f09d1\n",
            "Successfully built compound-word-splitter\n",
            "Installing collected packages: compound-word-splitter\n",
            "Successfully installed compound-word-splitter-0.4\n",
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8c/bd224a5db562ac008edbfaf015f5d5c98ea13e745247cd4ab5fc5b683085/pyenchant-3.2.0-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.1 [309 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.1 [87.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,310 kB in 1s (1,515 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 149406 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  hunspell libreoffice-writer\n",
            "The following NEW packages will be installed:\n",
            "  hunspell-it\n",
            "0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 340 kB of archives.\n",
            "After this operation, 1,752 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-it all 1:6.0.3-3 [340 kB]\n",
            "Fetched 340 kB in 1s (575 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package hunspell-it.\n",
            "(Reading database ... 149820 files and directories currently installed.)\n",
            "Preparing to unpack .../hunspell-it_1%3a6.0.3-3_all.deb ...\n",
            "Unpacking hunspell-it (1:6.0.3-3) ...\n",
            "Setting up hunspell-it (1:6.0.3-3) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRjY391AGd-f"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from tokenizer import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "import wordninja\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import ast\n",
        "\n",
        "import emoji\n",
        "import unicodedata\n",
        "\n",
        "import gzip\n",
        "\n",
        "import spacy_udpipe\n",
        "import language_tool_python\n",
        "\n",
        "import splitter\n",
        "import enchant\n",
        "from itertools import groupby\n",
        "import string\n",
        "\n",
        "import sys\n",
        "import os\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQjATsWGdpJW"
      },
      "source": [
        "# directory name:\n",
        "input_dir = '/content/drive/My Drive/HLT/dataset_test_evalita/'\n",
        "output_dir = '/content/drive/My Drive/HLT/dataset_test_evalita_preprocessed/'\n",
        "input_dir_preprocessing = '/content/drive/My Drive/HLT/preprocessing/'\n",
        "\n",
        "# Spec\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhfNkQOddzQ9",
        "outputId": "0aa3bf69-a79b-4a2b-ffae-7a4d84a7ef89"
      },
      "source": [
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeSkmao9eW02"
      },
      "source": [
        "#tsv_tweets_file = open(input_dir+\"haspeede2_reference_taskAB-tweets.tsv\")\n",
        "tsv_news_file = open(input_dir + \"haspeede2_reference_taskAB-news.tsv\")\n",
        "#tsv_file = open(input_dir+\"haspeede2_dev_taskAB.tsv\")\n",
        "\n",
        "dataset_raw = pd.read_csv(tsv_news_file,sep='\\t', header=None)\n",
        "#dataset_raw.rename(columns={\"text \": \"text\"}, inplace=True)\n",
        "dataset_raw.rename(columns={0: \"id\"}, inplace=True)\n",
        "dataset_raw.rename(columns={1: \"text\"}, inplace=True)\n",
        "dataset_raw.rename(columns={2: \"hs\"}, inplace=True)\n",
        "dataset_raw.rename(columns={3: \"stereotype\"}, inplace=True)\n",
        "\n",
        "#Bad Words\n",
        "f2 = open(input_dir_preprocessing+\"bad_words.csv\", 'r', encoding='utf8')\n",
        "\n",
        "bad_words_set = [] #list of lowercase words\n",
        "\n",
        "for x in f2:\n",
        "    y = x.rstrip()\n",
        "    y = y.lower()\n",
        "    if y != '':\n",
        "        bad_words_set.append(y)"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63ozW-a2-kNG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "bdf6907f-5999-449f-84da-d35cfdac9d9a"
      },
      "source": [
        "dataset_raw"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>hs</th>\n",
              "      <th>stereotype</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11976</td>\n",
              "      <td>Andate pure là, tanto quei fessi degli italiani.... Capito perché ci invadono? Il clandestino confessa</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12142</td>\n",
              "      <td>Che fine spero che faccia il killer nigeriano di Pamela. La furia cieca della Meloni: le sue parole più dure</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12088</td>\n",
              "      <td>Così i profughi ci svuotano i negozi a Pordenone</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12030</td>\n",
              "      <td>Così umiliano gli italiani e coccolano i clandestini. La follia del governo: \"Per loro gratis...\", e ci invadono</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11775</td>\n",
              "      <td>Danno soldi ai clandestini, ma ai disabili invece.... Bracconeri, il figlio autistico e il durissimo sfogo a \"Libero\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>10085</td>\n",
              "      <td>Sea Watch, il pm fa sbarcare i migranti. Salvini: denuncio chi ha aperto i porti</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>10044</td>\n",
              "      <td>Il pm fa sbarcare i migranti Il capo leghista smentito in tv</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>10602</td>\n",
              "      <td>Pisa, il poster di Salvini con i migranti fatto dagli studenti, Ceccardi: «Va rimosso»</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>10193</td>\n",
              "      <td>Sea Watch e lo sbarco del migrante con una sola gamba: ''Che possa realizzare i suoi sogni''</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>10282</td>\n",
              "      <td>Decreto Sicurezza Bis, multe più salate per chi soccorre i migranti</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... stereotype\n",
              "0    11976  ...          0\n",
              "1    12142  ...          1\n",
              "2    12088  ...          1\n",
              "3    12030  ...          1\n",
              "4    11775  ...          1\n",
              "..     ...  ...        ...\n",
              "495  10085  ...          0\n",
              "496  10044  ...          0\n",
              "497  10602  ...          0\n",
              "498  10193  ...          0\n",
              "499  10282  ...          0\n",
              "\n",
              "[500 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPBGTK1PiGEx"
      },
      "source": [
        "# Preprocessing phase\n",
        "Following the \"Preprocessing\" section of https://books.openedition.org/aaccademia/4832?lang=it#tocfrom1n4 we will perform:\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTSJjfRgjZ8s"
      },
      "source": [
        "### Extraction of the first feature: length of the comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hDGiCnLhlHB"
      },
      "source": [
        "def comment_length(text):\n",
        "    return len(text)"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF0sQvOrjqDN"
      },
      "source": [
        "dataset_raw['text_length'] = dataset_raw['text'].apply(comment_length)"
      ],
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTIhhE0Jjzqq"
      },
      "source": [
        "Extraction of the second feature: percentage of words written in CAPS-LOCK inside the comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8G1vCSnj4ML"
      },
      "source": [
        "def caps_lock_words(text):\n",
        "    words = text.split()\n",
        "    count_caps_lock = 0\n",
        "    number_of_words = len(words)\n",
        "    \n",
        "    for word in words:\n",
        "        if word.isupper() == True:\n",
        "            count_caps_lock = count_caps_lock + 1\n",
        "            \n",
        "    return ((count_caps_lock*100)//number_of_words)"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeH2idQEj66P"
      },
      "source": [
        "dataset_raw['#C-L words'] = dataset_raw['text'].apply(caps_lock_words)"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26m0-vh5JEOB"
      },
      "source": [
        " ### Removing Tags "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZTSDBK4JNIl"
      },
      "source": [
        "def clean_tag(text):\n",
        "    return re.sub(\n",
        "        r'@user', ' ', text)"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22IGkgdXJPRT"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_tag)"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpIlGt9OI5je"
      },
      "source": [
        "### Replace the characters ‘&’, ‘@’ respectively in the letters ‘e’, ‘a’"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xZ02VkZJUX5"
      },
      "source": [
        "def replace_e_a(text):\n",
        "    text = re.sub(r'&', 'e', text)\n",
        "    return re.sub(r'@', 'a', text)"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vURkcOrHJgiQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(replace_e_a)"
      ],
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9T6so5NJ21c"
      },
      "source": [
        "### Conversion of disguised bad words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joAwdJKBJ9Vw"
      },
      "source": [
        "def clean_disguised_bad_words(text):\n",
        "    text = \" \" + text + \" \"\n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' coglioni ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+e ', ' coglione ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+o ', ' cazzo ', text) \n",
        "    text = re.sub(r' ca[.x*@%#$^]+ro ', ' cazzaro ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' cazzi ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+a ', ' merda ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+e ', ' merde ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+ulo ', ' culo ', text) \n",
        "    text = re.sub(r' p[.x*@%#$^]+a ', ' puttana ', text)\n",
        "    text = re.sub(r' p[.x*@%#$^]+e ', ' puttane ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+a ', ' troia ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+e ', ' troie ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+o ', ' stronzo ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+i ', ' stronzi ', text)\n",
        "    return text"
      ],
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjzGx0uFJ9y5"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_disguised_bad_words)"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzeCUAIEKX5x"
      },
      "source": [
        "### Hashtag splitting and saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy7M8Rw3VltO"
      },
      "source": [
        "def find_hashtags(text):\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    if result:\n",
        "        return result\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mtFua9FVlCk"
      },
      "source": [
        "dataset_raw['hashtags'] = dataset_raw['text'].apply(find_hashtags)"
      ],
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHHr66WzQ7mJ"
      },
      "source": [
        "def split_hashtags(text):\n",
        "    \n",
        "    text = ' ' + text + ' '\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    \n",
        "    for word in result:\n",
        "        new_word = \" \".join(splitter.split(word[1:].lower(), 'it_IT'))\n",
        "        if len(new_word)==0:\n",
        "            new_word =  word[1:]\n",
        "                  \n",
        "        text = text.replace(word, new_word)\n",
        "        \n",
        "    return text"
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaNR6YmYRFlQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(split_hashtags)"
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZdFjV6zWCJ3"
      },
      "source": [
        " ### Removing URL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMMZC6EaWB92"
      },
      "source": [
        "def clean_URL(text):\n",
        "    return re.sub(r'URL', ' ', text)"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GInlm-4NWByp"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_URL)"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vw02ux1d8xr"
      },
      "source": [
        "### Extraction of the fourth feature: number of ‘?’ or ‘!’ inside the comment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAilG6e9d_Kv"
      },
      "source": [
        "def esclamations_and_questions(text):\n",
        "    return text.count('!') + text.count('?')"
      ],
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8aPhymFd_W4"
      },
      "source": [
        "dataset_raw['#?!'] = dataset_raw['text'].apply(esclamations_and_questions)"
      ],
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ODCBvIweoBT"
      },
      "source": [
        "### Extraction of the fifth feature: number of ‘.’ or ‘,’ inside the comment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4cXR155eoBV"
      },
      "source": [
        "def esclamations_and_questions(text):\n",
        "    return text.count(',') + text.count('.')"
      ],
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o2NoyA-eoBW"
      },
      "source": [
        "dataset_raw['#.,'] = dataset_raw['text'].apply(esclamations_and_questions)"
      ],
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2R9v14KfNvS"
      },
      "source": [
        "### Punctuation removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOfbQApfQq4"
      },
      "source": [
        "def strip_punctuation(text):\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    return regex.sub(' ', text)"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJruDmPfQ4M"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(strip_punctuation)"
      ],
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jld5EAWV11"
      },
      "source": [
        "### Removal of nearby equal vowels, removal of nearby equal consonants if they are more than 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At0CUlq9akCI"
      },
      "source": [
        "correct_words_vowels = ['sciiti,',\n",
        " 'welcomerefugees',\n",
        " 'livetweet',\n",
        " 'desiree',\n",
        " 'canaan',\n",
        " 'tweet.',\n",
        " 'weekend',\n",
        " 'romafeyenoord',\n",
        " 'ebree,',\n",
        " 'greencard',\n",
        " 'creerà',\n",
        " 'cooperative.',\n",
        " 'moschee,',\n",
        " 'cooperanti',\n",
        " 'streetart',\n",
        " 'khalidmasood',\n",
        " 'tweet',\n",
        " 'woolfe',\n",
        " 'cooperazione',\n",
        " 'coop',\n",
        " 'seehofer',\n",
        " 'speech',\n",
        " 'coffee',\n",
        " 'scooter',\n",
        " 'street',\n",
        " 'veemenza',\n",
        " 'moschee.',\n",
        " 'maalox.',\n",
        " 'book',\n",
        " 'tweet',\n",
        " 'facebook:',\n",
        " 'sociial,',\n",
        " 'coop,',\n",
        " 'canaaniti.',\n",
        " 'europee,',\n",
        " 'cooperative',\n",
        " 'google',\n",
        " 'creeranno',\n",
        " 'mediterranee',\n",
        " 'cooperazione',\n",
        " 'cooperativa',\n",
        " '“boom”',\n",
        " 'refugees',\n",
        " 'moonlight',\n",
        " 'imaam',\n",
        " 'shooting',\n",
        " 'sciiti',\n",
        " 'sunniti',\n",
        " 'book',\n",
        " 'atee.',\n",
        " 'looking',\n",
        " 'week',\n",
        " 'ayaan',\n",
        " 'temporanee.',\n",
        " 'idee.',\n",
        " 'sibiliini',\n",
        " 'food',\n",
        " 'refugees',\n",
        " 'retweeted',\n",
        " 'boom',\n",
        " 'keep',\n",
        " 'vodoo',\n",
        " 'hooligans',\n",
        " 'ebree',\n",
        " 'refugees',\n",
        " 'speed',\n",
        " 'bloomberg',\n",
        " 'riina',\n",
        " 'hatespeech',\n",
        " 'google',\n",
        " 'masood',\n",
        " 'linee.',\n",
        " 'boom']"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFnrr_UqWWQ-"
      },
      "source": [
        "def delete_duplicate_vowels_and_redundant_consonant (text):\n",
        "    parole = text.split()\n",
        "    stringa = \"\"\n",
        "    for a in parole:\n",
        "        parola = a\n",
        "        a = [list(g) for k, g in groupby(a)]    \n",
        "        vocali = ['a','e','i','o','u','y']\n",
        "        \n",
        "        for idx,val in enumerate(a):\n",
        "            if idx == 0:\n",
        "                stringa += a[idx][0] \n",
        "            elif idx == len(a)-1:\n",
        "                stringa += a[idx][0]\n",
        "            elif a[idx][0] in vocali and (parola.lower() not in correct_words_vowels):\n",
        "                stringa += a[idx][0]\n",
        "            elif len(a[idx]) == 1:\n",
        "                stringa += a[idx][0]\n",
        "            elif len (a[idx]) >= 2:\n",
        "                stringa += a[idx][0]\n",
        "                stringa += a[idx][1]\n",
        "        stringa =  stringa + \" \"\n",
        "        \n",
        "    return(stringa)  "
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol8qT-rhWWM6"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(delete_duplicate_vowels_and_redundant_consonant)"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s06b9uii4eN-"
      },
      "source": [
        "### Translation of emoticons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3ctOpMF79JW"
      },
      "source": [
        "def translate_emoticon(text):\n",
        "    text_result = emoji.demojize(text, language='it')\n",
        "    text_result=re.sub(r':', ' ', text_result)\n",
        "    return text_result"
      ],
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbR6rg3V7-Wt"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(translate_emoticon)"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi5I3Y606DVQ"
      },
      "source": [
        "emoticons_text = {\n",
        "    '<kiss>': 'bacio',\n",
        "    '<happy>': 'felice',\n",
        "    '<laugh>': 'risata',\n",
        "    '<sad>': 'triste',\n",
        "    '<surprise>': 'sorpreso',\n",
        "    '<wink>': 'occhiolino',\n",
        "    '<tong>': 'faccia con lingua',\n",
        "    '<annoyed>': 'annoiato',\n",
        "    '<seallips>': 'labbra sigillate',\n",
        "    '<angel>': 'angelo',\n",
        "    '<devil>': 'diavolo',\n",
        "    '<highfive>' : 'batti il cinque',\n",
        "    '<heart>': 'cuore',\n",
        "    '<user>' : 'persona',\n",
        "}"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWgDe3ca6EC6"
      },
      "source": [
        "def clean_emoticon_text(text):\n",
        "    text_words = text.split()\n",
        "    new_words  = [emoticons_text.get(ele, ele) for ele in text_words]\n",
        "    return ' '.join(new_words)"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRaxAUC86DJQ"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_emoticon_text)"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM8zUkgjA_Xf"
      },
      "source": [
        "### Replacement of the abbreviations with the respective words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJEKyYL-BMhk"
      },
      "source": [
        "abbr_word = {'cmq':'comunque', 'gov':'governatori', 'fb':'facebook', 'tw':'twitter', 'juve':'juventus', 'ing':'ingegnere', \n",
        "             'sx':'sinistra', 'qdo':'quando', 'rep':'repubblica', 'grz':'grazie', 'ita':'italia', 'mln':'milioni', \n",
        "             'mld':'miliardi', 'pke':'perche', 'anke':'anche', 'cm':'come', 'dlla':'della', 'dlle':'delle', 'qst':'questa',\n",
        "             'ke':'che', 'nn':'non', 'sn':'sono', 'cn':'con', 'xk':'perche', 'xke':'perche', 'art':'articolo',\n",
        "             'tv':'televisore', '€':'euro', 'xché':'perché', 'xké':'perché', 'pkè':'perché'}"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNUJsA9xBMsL"
      },
      "source": [
        "def replace_abbreviation(text):\n",
        "    text_words = text.split()\n",
        "    new_text = \"\"\n",
        "    for token in text_words:\n",
        "      new_text  += abbr_word.get(token, token) +\" \"\n",
        "    \n",
        "    return new_text.strip()"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRRSxGnEBRSe"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(replace_abbreviation)"
      ],
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcoE_FIQBZDO"
      },
      "source": [
        "### Removal of the laughs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTfCPOK3B9pL"
      },
      "source": [
        "laughs = ['ah', 'eh', 'he' 'ih', 'hi'] #non elimina ahahahah, ma solo ah\n",
        "vowels = ['a', 'e', 'i', 'o', 'u']\n",
        "\n",
        "def clean_laughs(text):\n",
        "    #s = \"ahahahah ho fame io, eh eh\" -> \" ho fame io,\"\n",
        "    text_words = text.split()\n",
        "    new_words  = [word for word in text_words if word not in laughs]\n",
        "    \n",
        "    new_text = ' '.join(new_words)\n",
        "    \n",
        "    for i in new_words:\n",
        "        for k in vowels:\n",
        "            if ('h' in i) and (len(i) >= 4):\n",
        "                if (len(i) - 2) <= (i.count(k) + i.count('h')):\n",
        "                    new_text = new_text.replace(i, '')\n",
        "    \n",
        "    return new_text"
      ],
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEMhN6UCB90s"
      },
      "source": [
        "dataset_raw['text'] = dataset_raw['text'].apply(clean_laughs)"
      ],
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPBk_1WFFbVv"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7TesaZ_Fcyx"
      },
      "source": [
        "def tokenization(text):\n",
        "    tknzr=SocialTokenizer(lowercase=False)\n",
        "    return tknzr.tokenize(text)"
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BcgiN-cFf25"
      },
      "source": [
        "dataset_raw['tokens'] = dataset_raw['text'].apply(tokenization)"
      ],
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADwlSkQyWH96"
      },
      "source": [
        "### Extraction of the sixth feature: number of bad words in the comment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ3Ty8mKWqyG"
      },
      "source": [
        "def number_bad_words(tokens):\n",
        "    n_bad_words = 0\n",
        "\n",
        "    for word in tokens:\n",
        "        if word.lower() in bad_words_set:\n",
        "            n_bad_words = n_bad_words + 1\n",
        "        \n",
        "    return n_bad_words"
      ],
      "execution_count": 308,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOJBDkIxWqk-"
      },
      "source": [
        "dataset_raw['#bad_words'] = dataset_raw['tokens'].apply(number_bad_words)"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Vtd6HGWgDJ"
      },
      "source": [
        "### Extraction of the seventh feature: percentage of bad words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_6xdkb0WYds"
      },
      "source": [
        "def percentage_bad_words(tokens):\n",
        "    n_words = 0\n",
        "    n_bad_words = 0\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word != '<' and word != '>':\n",
        "            n_words = n_words + 1\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word.lower() in bad_words_set:\n",
        "            n_bad_words = n_bad_words + 1\n",
        "        \n",
        "    return ((n_bad_words*100)//n_words)"
      ],
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvQRET5VWaxZ"
      },
      "source": [
        "dataset_raw['%bad_words'] = dataset_raw['tokens'].apply(percentage_bad_words)"
      ],
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g_W0gmIpkFKB",
        "outputId": "be4ce496-33f2-4493-8447-6966363883f0"
      },
      "source": [
        "dataset_raw.head(30)"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>hs</th>\n",
              "      <th>stereotype</th>\n",
              "      <th>text_length</th>\n",
              "      <th>#C-L words</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>#?!</th>\n",
              "      <th>#.,</th>\n",
              "      <th>tokens</th>\n",
              "      <th>#bad_words</th>\n",
              "      <th>%bad_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11976</td>\n",
              "      <td>Andate pure là tanto quei fessi degli italiani Capito perché ci invadono Il clandestino confessa</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>[Andate, pure, là, tanto, quei, fessi, degli, italiani, Capito, perché, ci, invadono, Il, clandestino, confessa]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12142</td>\n",
              "      <td>Che fine spero che faccia il killer nigeriano di Pamela La furia cieca della Meloni le sue parole più dure</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>108</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Che, fine, spero, che, faccia, il, killer, nigeriano, di, Pamela, La, furia, cieca, della, Meloni, le, sue, parole, più, dure]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12088</td>\n",
              "      <td>Così i profughi ci svuotano i negozi a Pordenone</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Così, i, profughi, ci, svuotano, i, negozi, a, Pordenone]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12030</td>\n",
              "      <td>Così umiliano gli italiani e coccolano i clandestini La follia del governo Per loro gratis e ci invadono</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>112</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>[Così, umiliano, gli, italiani, e, coccolano, i, clandestini, La, follia, del, governo, Per, loro, gratis, e, ci, invadono]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11775</td>\n",
              "      <td>Danno soldi ai clandestini ma ai disabili invece Bracconeri il figlio autistico e il durissimo sfogo a Libero</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>117</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>[Danno, soldi, ai, clandestini, ma, ai, disabili, invece, Bracconeri, il, figlio, autistico, e, il, durissimo, sfogo, a, Libero]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>11630</td>\n",
              "      <td>Emergenza Minniti annulla il viaggio e torna in Italia Immigrati invasione alle porte cose mai viste prima</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>110</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Emergenza, Minniti, annulla, il, viaggio, e, torna, in, Italia, Immigrati, invasione, alle, porte, cose, mai, viste, prima]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>11695</td>\n",
              "      <td>Gli immigrati sono matti parola di scienziati Malattie mentali e risorse lo studio horror</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Gli, immigrati, sono, matti, parola, di, scienziati, Malattie, mentali, e, risorse, lo, studio, horror]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>11866</td>\n",
              "      <td>Ho chiesto ai profughi di lavorare ma preferiscono non fare nulla</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>65</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Ho, chiesto, ai, profughi, di, lavorare, ma, preferiscono, non, fare, nulla]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>12174</td>\n",
              "      <td>Immigrati bomba sociale Subito via 600mila irregolari</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Immigrati, bomba, sociale, Subito, via, 6, 0, 0, mila, irregolari]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>11663</td>\n",
              "      <td>Immigrati così la sinistra ci costringe a tenerli tutti Meloni la denuncia Il trucco giuridico per rovinare l Italia</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>122</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[Immigrati, così, la, sinistra, ci, costringe, a, tenerli, tutti, Meloni, la, denuncia, Il, trucco, giuridico, per, rovinare, l, Italia]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11677</td>\n",
              "      <td>Immigrati venite tutti qui L ultima perla del Papa con Scalfari tifa invasione</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>83</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[Immigrati, venite, tutti, qui, L, ultima, perla, del, Papa, con, Scalfari, tifa, invasione]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12092</td>\n",
              "      <td>Invasi dagli immigrati Ci resta solo una possibilità La drastica idea del generale Jean per salvare l Italia</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>110</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Invasi, dagli, immigrati, Ci, resta, solo, una, possibilità, La, drastica, idea, del, generale, Jean, per, salvare, l, Italia]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12089</td>\n",
              "      <td>Io al governo Come ridurrò clandestini e stupratori Giorgia Meloni lezione alla sinistra che ci ha ridotti a schifo</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Io, al, governo, Come, ridurrò, clandestini, e, stupratori, Giorgia, Meloni, lezione, alla, sinistra, che, ci, ha, ridotti, a, schifo]</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>11714</td>\n",
              "      <td>Io ostaggio degli immigrati ecco come mi hanno ridotta</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Io, ostaggio, degli, immigrati, ecco, come, mi, hanno, ridotta]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>11808</td>\n",
              "      <td>Italia serva calpestata e invasa adesso basta Immigrati Feltri sale sul panzer Atto di forza</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>99</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>[Italia, serva, calpestata, e, invasa, adesso, basta, Immigrati, Feltri, sale, sul, panzer, Atto, di, forza]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>11904</td>\n",
              "      <td>L immigrazione indiscriminata porta al islamizzazione</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[L, immigrazione, indiscriminata, porta, al, islamizzazione]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>12183</td>\n",
              "      <td>Macché poveri e disperati Questi sono falsi profughi</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Macché, poveri, e, disperati, Questi, sono, falsi, profughi]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>12108</td>\n",
              "      <td>Malaria E solo l inizio La tremenda profezia del leghista africano cosa succederà in Italia</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>98</td>\n",
              "      <td>7</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>[Malaria, E, solo, l, inizio, La, tremenda, profezia, del, leghista, africano, cosa, succederà, in, Italia]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>12023</td>\n",
              "      <td>Migranti o profughi comunque invasori</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Migranti, o, profughi, comunque, invasori]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>12114</td>\n",
              "      <td>Perché non voglio immigrati cosa sarebbe l Italia senza di loro Sallusti una durissima verità</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>[Perché, non, voglio, immigrati, cosa, sarebbe, l, Italia, senza, di, loro, Sallusti, una, durissima, verità]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>11853</td>\n",
              "      <td>Sapete cosa voglio fare agli immigrati violenti Don Gigi il prete giustiziere anche con l Islam</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Sapete, cosa, voglio, fare, agli, immigrati, violenti, Don, Gigi, il, prete, giustiziere, anche, con, l, Islam]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>11742</td>\n",
              "      <td>Sapete quanti soldi diamo agli immigrati e quanti ai poveri italiani Socci svela la vergogna</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[Sapete, quanti, soldi, diamo, agli, immigrati, e, quanti, ai, poveri, italiani, Socci, svela, la, vergogna]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>11780</td>\n",
              "      <td>Altro che profughi arrivano clandestini</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Altro, che, profughi, arrivano, clandestini]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>11897</td>\n",
              "      <td>Atti osceni davanti ai bambini il parco di Milano in balìa dei migranti</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Atti, osceni, davanti, ai, bambini, il, parco, di, Milano, in, balìa, dei, migranti]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>11634</td>\n",
              "      <td>Calderoli la sua soluzione drastica contro gli stupri dei migranti Dobbiamo castrarli tutti</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Calderoli, la, sua, soluzione, drastica, contro, gli, stupri, dei, migranti, Dobbiamo, castrarli, tutti]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>11618</td>\n",
              "      <td>Ritira la pensione fa elemosina a una nomade e va al bar dalle tasche gli spariscono 10 euro</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Ritira, la, pensione, fa, elemosina, a, una, nomade, e, va, al, bar, dalle, tasche, gli, spariscono, 10, euro]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>11889</td>\n",
              "      <td>Tentano di violentarla mentre va al lavoro caccia a due stranieri a Grosseto</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[Tentano, di, violentarla, mentre, va, al, lavoro, caccia, a, due, stranieri, a, Grosseto]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>11823</td>\n",
              "      <td>Lecco immigrato fra cacca e pipì tra i mezzi dei vigili poi le botte al agente di polizia</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Lecco, immigrato, fra, cacca, e, pipì, tra, i, mezzi, dei, vigili, poi, le, botte, al, agente, di, polizia]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>11727</td>\n",
              "      <td>Il cimitero della Parrocchietta nel degrado circondato da campi rom e discariche</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>81</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[Il, cimitero, della, Parrocchietta, nel, degrado, circondato, da, campi, rom, e, discariche]</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>11903</td>\n",
              "      <td>“Uomini dell’Isis dietro i flussi dei migranti dalla Libia”</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[“, Uomini, dell, ’, Isis, dietro, i, flussi, dei, migranti, dalla, Libia, ”]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... %bad_words\n",
              "0   11976  ...          0\n",
              "1   12142  ...          0\n",
              "2   12088  ...          0\n",
              "3   12030  ...          0\n",
              "4   11775  ...          0\n",
              "5   11630  ...          0\n",
              "6   11695  ...          0\n",
              "7   11866  ...          0\n",
              "8   12174  ...          0\n",
              "9   11663  ...          0\n",
              "10  11677  ...          0\n",
              "11  12092  ...          0\n",
              "12  12089  ...          5\n",
              "13  11714  ...          0\n",
              "14  11808  ...          0\n",
              "15  11904  ...          0\n",
              "16  12183  ...          0\n",
              "17  12108  ...          0\n",
              "18  12023  ...          0\n",
              "19  12114  ...          0\n",
              "20  11853  ...          0\n",
              "21  11742  ...          0\n",
              "22  11780  ...          0\n",
              "23  11897  ...          0\n",
              "24  11634  ...          0\n",
              "25  11618  ...          0\n",
              "26  11889  ...          0\n",
              "27  11823  ...          0\n",
              "28  11727  ...          8\n",
              "29  11903  ...          0\n",
              "\n",
              "[30 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzmHMXMI5gYT"
      },
      "source": [
        "dataset_raw.to_csv('test_dataset_news.csv', index=False)"
      ],
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNzXv4vCHNr"
      },
      "source": [
        ""
      ],
      "execution_count": 313,
      "outputs": []
    }
  ]
}