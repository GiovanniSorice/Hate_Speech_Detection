{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP6dna1EvSYl/u769kkQ0q+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Hate_Speech_Detection/blob/main/notebooks/scraper_tweet_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEWF7JvqtF5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b711c41-27bb-4582-c30d-bac76e3f401a"
      },
      "source": [
        "!pip install tokenizer\n",
        "!pip install ekphrasis\n",
        "!pip install wordninja\n",
        "!pip install emoji\n",
        "!pip install spacy_udpipe\n",
        "!pip install language_tool_python\n",
        "!pip install compound-word-splitter\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install enchant\n",
        "!sudo apt-get install hunspell-it\n",
        "!pip install transformers"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizer in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.4)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.9)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: wordninja in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: spacy_udpipe in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ufal.udpipe>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy_udpipe) (1.2.0.3)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy_udpipe) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (54.1.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.4.3)\n",
            "Requirement already satisfied: language_tool_python in /usr/local/lib/python3.7/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Requirement already satisfied: compound-word-splitter in /usr/local/lib/python3.7/dist-packages (0.4)\n",
            "Requirement already satisfied: pyenchant in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "hunspell-it is already the newest version (1:6.0.3-3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 5.0MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=16fa9865a1ab081cecc7dea4ed6299faa68f69aaec0ea5f80bc233507199117b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U92s4XL1tTtG"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from tokenizer import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "import wordninja\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import ast\n",
        "\n",
        "import emoji\n",
        "import unicodedata\n",
        "\n",
        "import gzip\n",
        "\n",
        "import spacy_udpipe\n",
        "import language_tool_python\n",
        "\n",
        "import splitter\n",
        "import enchant\n",
        "from itertools import groupby\n",
        "import string\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "#KimCNN\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ALBERTo\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset,DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRp-i_rntxPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6ec643-fab5-4043-e852-42ac9d32654b"
      },
      "source": [
        "with_drive = True\n",
        "\n",
        "if with_drive:\n",
        "  from google.colab import drive\n",
        "  # This will prompt for authorization.\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYi7p-INLb5E"
      },
      "source": [
        "def preprocessing_KimCNN_BiLSTM(dataset_path, bad_words_path):\n",
        "  def comment_length(text):\n",
        "    return len(text)\n",
        "\n",
        "  def caps_lock_words(text):\n",
        "    words = text.split()\n",
        "    count_caps_lock = 0\n",
        "    number_of_words = len(words)\n",
        "    \n",
        "    for word in words:\n",
        "        if word.isupper() == True:\n",
        "            count_caps_lock = count_caps_lock + 1\n",
        "            \n",
        "    return ((count_caps_lock*100)//number_of_words)\n",
        "\n",
        "  def clean_tag(text):\n",
        "    return re.sub(\n",
        "        r'@user', ' ', text)\n",
        "    \n",
        "  def replace_e_a(text):\n",
        "    text = re.sub(r'&', 'e', text)\n",
        "    return re.sub(r'@', 'a', text)\n",
        "\n",
        "  def clean_disguised_bad_words(text):\n",
        "    text = \" \" + text + \" \"\n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' coglioni ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+e ', ' coglione ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+o ', ' cazzo ', text) \n",
        "    text = re.sub(r' ca[.x*@%#$^]+ro ', ' cazzaro ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' cazzi ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+a ', ' merda ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+e ', ' merde ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+ulo ', ' culo ', text) \n",
        "    text = re.sub(r' p[.x*@%#$^]+a ', ' puttana ', text)\n",
        "    text = re.sub(r' p[.x*@%#$^]+e ', ' puttane ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+a ', ' troia ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+e ', ' troie ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+o ', ' stronzo ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+i ', ' stronzi ', text)\n",
        "    return text\n",
        "\n",
        "  def find_hashtags(text):\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    if result:\n",
        "        return result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "  def split_hashtags(text):\n",
        "    \n",
        "    text = ' ' + text + ' '\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    \n",
        "    for word in result:\n",
        "        new_word = \" \".join(splitter.split(word[1:].lower(), 'it_IT'))\n",
        "        if len(new_word)==0:\n",
        "            new_word =  word[1:]\n",
        "                  \n",
        "        text = text.replace(word, new_word)\n",
        "        \n",
        "    return text\n",
        "\n",
        "  def clean_URL(text):\n",
        "    return re.sub(r'URL', ' ', text)\n",
        "\n",
        "  def esclamations_and_questions(text):\n",
        "    return text.count('!') + text.count('?')\n",
        "\n",
        "  def esclamations_and_questions(text):\n",
        "      return text.count(',') + text.count('.')\n",
        "\n",
        "  def strip_punctuation(text):\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    return regex.sub(' ', text)\n",
        "\n",
        "  correct_words_vowels = ['sciiti,',\n",
        "  'welcomerefugees',\n",
        "  'livetweet',\n",
        "  'desiree',\n",
        "  'canaan',\n",
        "  'tweet.',\n",
        "  'weekend',\n",
        "  'romafeyenoord',\n",
        "  'ebree,',\n",
        "  'greencard',\n",
        "  'creerà',\n",
        "  'cooperative.',\n",
        "  'moschee,',\n",
        "  'cooperanti',\n",
        "  'streetart',\n",
        "  'khalidmasood',\n",
        "  'tweet',\n",
        "  'woolfe',\n",
        "  'cooperazione',\n",
        "  'coop',\n",
        "  'seehofer',\n",
        "  'speech',\n",
        "  'coffee',\n",
        "  'scooter',\n",
        "  'street',\n",
        "  'veemenza',\n",
        "  'moschee.',\n",
        "  'maalox.',\n",
        "  'book',\n",
        "  'tweet',\n",
        "  'facebook:',\n",
        "  'sociial,',\n",
        "  'coop,',\n",
        "  'canaaniti.',\n",
        "  'europee,',\n",
        "  'cooperative',\n",
        "  'google',\n",
        "  'creeranno',\n",
        "  'mediterranee',\n",
        "  'cooperazione',\n",
        "  'cooperativa',\n",
        "  '“boom”',\n",
        "  'refugees',\n",
        "  'moonlight',\n",
        "  'imaam',\n",
        "  'shooting',\n",
        "  'sciiti',\n",
        "  'sunniti',\n",
        "  'book',\n",
        "  'atee.',\n",
        "  'looking',\n",
        "  'week',\n",
        "  'ayaan',\n",
        "  'temporanee.',\n",
        "  'idee.',\n",
        "  'sibiliini',\n",
        "  'food',\n",
        "  'refugees',\n",
        "  'retweeted',\n",
        "  'boom',\n",
        "  'keep',\n",
        "  'vodoo',\n",
        "  'hooligans',\n",
        "  'ebree',\n",
        "  'refugees',\n",
        "  'speed',\n",
        "  'bloomberg',\n",
        "  'riina',\n",
        "  'hatespeech',\n",
        "  'google',\n",
        "  'masood',\n",
        "  'linee.',\n",
        "  'boom']\n",
        "  def delete_duplicate_vowels_and_redundant_consonant (text):\n",
        "      parole = text.split()\n",
        "      stringa = \"\"\n",
        "      for a in parole:\n",
        "          parola = a\n",
        "          a = [list(g) for k, g in groupby(a)]    \n",
        "          vocali = ['a','e','i','o','u','y']\n",
        "          \n",
        "          for idx,val in enumerate(a):\n",
        "              if idx == 0:\n",
        "                  stringa += a[idx][0] \n",
        "              elif idx == len(a)-1:\n",
        "                  stringa += a[idx][0]\n",
        "              elif a[idx][0] in vocali and (parola.lower() not in correct_words_vowels):\n",
        "                  stringa += a[idx][0]\n",
        "              elif len(a[idx]) == 1:\n",
        "                  stringa += a[idx][0]\n",
        "              elif len (a[idx]) >= 2:\n",
        "                  stringa += a[idx][0]\n",
        "                  stringa += a[idx][1]\n",
        "          stringa =  stringa + \" \"\n",
        "          \n",
        "      return(stringa)\n",
        "\n",
        "  def translate_emoticon(text):\n",
        "    text_result = emoji.demojize(text, language='it')\n",
        "    text_result=re.sub(r':', ' ', text_result)\n",
        "    return text_result\n",
        "\n",
        "  emoticons_text = {\n",
        "    '<kiss>': 'bacio',\n",
        "    '<happy>': 'felice',\n",
        "    '<laugh>': 'risata',\n",
        "    '<sad>': 'triste',\n",
        "    '<surprise>': 'sorpreso',\n",
        "    '<wink>': 'occhiolino',\n",
        "    '<tong>': 'faccia con lingua',\n",
        "    '<annoyed>': 'annoiato',\n",
        "    '<seallips>': 'labbra sigillate',\n",
        "    '<angel>': 'angelo',\n",
        "    '<devil>': 'diavolo',\n",
        "    '<highfive>' : 'batti il cinque',\n",
        "    '<heart>': 'cuore',\n",
        "    '<user>' : 'persona',\n",
        "  }\n",
        "\n",
        "  def clean_emoticon_text(text):\n",
        "    text_words = text.split()\n",
        "    new_words  = [emoticons_text.get(ele, ele) for ele in text_words]\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "  abbr_word = {'cmq':'comunque', 'gov':'governatori', 'fb':'facebook', 'tw':'twitter', 'juve':'juventus', 'ing':'ingegnere', \n",
        "             'sx':'sinistra', 'qdo':'quando', 'rep':'repubblica', 'grz':'grazie', 'ita':'italia', 'mln':'milioni', \n",
        "             'mld':'miliardi', 'pke':'perche', 'anke':'anche', 'cm':'come', 'dlla':'della', 'dlle':'delle', 'qst':'questa',\n",
        "             'ke':'che', 'nn':'non', 'sn':'sono', 'cn':'con', 'xk':'perche', 'xke':'perche', 'art':'articolo',\n",
        "             'tv':'televisore', '€':'euro', 'xché':'perché', 'xké':'perché', 'pkè':'perché'}\n",
        "             \n",
        "  def replace_abbreviation(text):\n",
        "    text_words = text.split()\n",
        "    new_text = \"\"\n",
        "    for token in text_words:\n",
        "      new_text  += abbr_word.get(token, token) +\" \"\n",
        "    \n",
        "    return new_text.strip()\n",
        "\n",
        "  laughs = ['ah', 'eh', 'he' 'ih', 'hi'] #non elimina ahahahah, ma solo ah\n",
        "  vowels = ['a', 'e', 'i', 'o', 'u']\n",
        "\n",
        "  def clean_laughs(text):\n",
        "      #s = \"ahahahah ho fame io, eh eh\" -> \" ho fame io,\"\n",
        "      text_words = text.split()\n",
        "      new_words  = [word for word in text_words if word not in laughs]\n",
        "      \n",
        "      new_text = ' '.join(new_words)\n",
        "      \n",
        "      for i in new_words:\n",
        "          for k in vowels:\n",
        "              if ('h' in i) and (len(i) >= 4):\n",
        "                  if (len(i) - 2) <= (i.count(k) + i.count('h')):\n",
        "                      new_text = new_text.replace(i, '')\n",
        "      \n",
        "      return new_text\n",
        "\n",
        "  def tokenization(text):\n",
        "    tknzr=SocialTokenizer(lowercase=False)\n",
        "    return tknzr.tokenize(text)\n",
        "\n",
        "  def number_bad_words(tokens):\n",
        "      n_bad_words = 0\n",
        "\n",
        "      for word in tokens:\n",
        "          if word.lower() in bad_words_set:\n",
        "              n_bad_words = n_bad_words + 1\n",
        "          \n",
        "      return n_bad_words\n",
        "\n",
        "  def percentage_bad_words(tokens):\n",
        "    n_words = 0\n",
        "    n_bad_words = 0\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word != '<' and word != '>':\n",
        "            n_words = n_words + 1\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word.lower() in bad_words_set:\n",
        "            n_bad_words = n_bad_words + 1\n",
        "        \n",
        "    return ((n_bad_words*100)//n_words)\n",
        "\n",
        "\n",
        "  csv_file = open(dataset_path)\n",
        "\n",
        "  dataset_raw = pd.read_csv(csv_file,sep=',', index_col=0)\n",
        "\n",
        "  dataset_raw.rename(columns={\"Text\": \"text\"}, inplace=True) \n",
        "   #Bad Words\n",
        "  f2 = open(bad_words_path, 'r', encoding='utf8')\n",
        "\n",
        "  bad_words_set = [] #list of lowercase words\n",
        "\n",
        "  for x in f2:\n",
        "      y = x.rstrip()\n",
        "      y = y.lower()\n",
        "      if y != '':\n",
        "          bad_words_set.append(y)\n",
        "\n",
        "  dataset_raw['text_length'] = dataset_raw['text'].apply(comment_length)\n",
        "  dataset_raw['#C-L words'] = dataset_raw['text'].apply(caps_lock_words)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_tag)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(replace_e_a)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_disguised_bad_words)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_disguised_bad_words)\n",
        "  dataset_raw['hashtags'] = dataset_raw['text'].apply(find_hashtags)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(split_hashtags)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_URL)\n",
        "  dataset_raw['#?!'] = dataset_raw['text'].apply(esclamations_and_questions)\n",
        "  dataset_raw['#.,'] = dataset_raw['text'].apply(esclamations_and_questions)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(strip_punctuation)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(delete_duplicate_vowels_and_redundant_consonant)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(translate_emoticon)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_emoticon_text)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(replace_abbreviation)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_laughs)\n",
        "  dataset_raw['tokens'] = dataset_raw['text'].apply(tokenization)\n",
        "  dataset_raw['#bad_words'] = dataset_raw['tokens'].apply(number_bad_words)\n",
        "  dataset_raw['%bad_words'] = dataset_raw['tokens'].apply(percentage_bad_words)\n",
        "\n",
        "  return dataset_raw"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTksciFmHn6W"
      },
      "source": [
        "parole_non_ric = set()\n",
        "def sentence_to_emb2(sentence, w2v, truncate = None, padding = False):\n",
        "  global parole_non_ric\n",
        "  pad_token = [0]*128\n",
        "  s_emb = [ w2v[word.lower()] for word in sentence if word.lower() in w2v.vocab]\n",
        "  parole_non_ric.update(set([ word.lower() for word in sentence if word.lower() not in w2v.vocab]))\n",
        "  if truncate is not None:\n",
        "    s_emb = s_emb[:truncate] #truncate\n",
        "  if padding:\n",
        "    s_emb += [pad_token] * (truncate - len(s_emb))\n",
        "  return np.array(s_emb)\n",
        "\n",
        "def get_data_to_emb2(data, w2v, truncate = None, padding = False):\n",
        "  X = [sentence_to_emb2(sentence, w2v, truncate, padding) for sentence in data]\n",
        "  return np.array(X)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx8od0JsZZZW"
      },
      "source": [
        "def preprocessing_ALBERTo(dataset_path):\n",
        "  text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\"},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    dicts=[emoticons]\n",
        "    )\n",
        "\n",
        "  class AlBERTo_Preprocessing(object):\n",
        "      def __init__(self, do_lower_case=True, **kwargs):\n",
        "          self.do_lower_case = do_lower_case\n",
        "\n",
        "      def preprocess(self, text):\n",
        "          if self.do_lower_case:\n",
        "              text = text.lower()\n",
        "          text = str(\" \".join(text_processor.pre_process_doc(text)))\n",
        "          text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n",
        "          text = re.sub(r'\\s+', ' ', text)\n",
        "          text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
        "          text = re.sub(r'^\\s', '', text)\n",
        "          text = re.sub(r'\\s$', '', text)\n",
        "          return text\n",
        "\n",
        "  AlBERTo_Preprocess = AlBERTo_Preprocessing(do_lower_case=True)\n",
        "  def preprocess(text):\n",
        "      return AlBERTo_Preprocess.preprocess(text)\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained('m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0')\n",
        "\n",
        "  csv_file = open(dataset_path)\n",
        "\n",
        "  dataset_raw = pd.read_csv(csv_file,sep=',', index_col=0)\n",
        "\n",
        "  dataset_raw.rename(columns={\"Text\": \"text\"}, inplace=True) \n",
        "\n",
        "\n",
        "  dataset_raw['text_preprocessed'] = dataset_raw['text'].apply(preprocess)\n",
        "  encoding_dataset = tokenizer(list(dataset_raw[\"text_preprocessed\"].values), padding=True, truncation=True, max_length=64)\n",
        "  input_ids_dataset = torch.tensor(encoding_dataset['input_ids'])\n",
        "  attention_mask_dataset = torch.tensor(encoding_dataset['attention_mask'])\n",
        "\n",
        "  test_tweets_data = TensorDataset(input_ids_dataset, attention_mask_dataset)\n",
        "  test_tweets_sampler = SequentialSampler(test_tweets_data)\n",
        "  test_tweets_dataloader = DataLoader(test_tweets_data, sampler=test_tweets_sampler, batch_size=len(test_tweets_data))\n",
        "\n",
        "  return test_tweets_dataloader"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiKvRHeXXFGU"
      },
      "source": [
        "def predict_with_ensemble_tf(models, test_input):\n",
        "  # make predictions\n",
        "  results = []\n",
        "  y_predict = [np.squeeze(np.where(model.predict(test_input) > 0.5, 1,0).reshape(1,-1)) for model in models]\n",
        "  # sum across ensemble members\n",
        "  y_predict = np.array(y_predict)\n",
        "\n",
        "  for i in range(y_predict.shape[1]):\n",
        "    counts = np.bincount(y_predict[:,i])\n",
        "    results.append(np.argmax(counts))\n",
        "  # argmax across classes\n",
        "  return results"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwPZ8YbMd9f9"
      },
      "source": [
        "def evaluate_for_kfold_torch(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "    f1_value = []\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        # Get the predictions\n",
        "        preds+=(torch.argmax(logits, dim=1).flatten().tolist())\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uHTm25HbsBF"
      },
      "source": [
        "def predict_with_ensemble_torch(models, test_dataloader):\n",
        "  # make predictions\n",
        "  results = []\n",
        "  y_predict = []\n",
        "  for model in models:\n",
        "    model.to(device)\n",
        "    y_predict.append(evaluate_for_kfold_torch(model, test_dataloader))\n",
        "    model.to(\"cpu\")\n",
        "\n",
        "  #y_predict = [np.squeeze(evaluate_for_kfold(model, test_dataloader)) for model in models]\n",
        "  # sum across ensemble members\n",
        "  y_predict = np.array(y_predict)\n",
        "\n",
        "  for i in range(y_predict.shape[1]):\n",
        "    counts = np.bincount(y_predict[:,i])\n",
        "    results.append(np.argmax(counts))\n",
        "  # argmax across classes\n",
        "  return results"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuBlhDXGtY2H"
      },
      "source": [
        "def evaluate_with_KimCNN(dataset_path, bad_words_path,models_path_list):\n",
        "  print(\"Evaluate_with_KimCNN\")\n",
        "  # Preprocessing\n",
        "  dataset = preprocessing_KimCNN_BiLSTM(dataset_path, bad_words_path)\n",
        "  # Split dataset in embeddings and other feature\n",
        "  X_dataset = get_data_to_emb2(dataset[\"tokens\"], w2v, max_length , True)\n",
        "  dataset_other = dataset.drop(['text','tokens', 'hashtags'], axis=1)\n",
        "\n",
        "  inputs   = {\"text\": X_dataset, \"other\": dataset_other.values}\n",
        "\n",
        "  # Load models\n",
        "  models = []\n",
        "  for path in models_path_list:\n",
        "    models.append(tf.keras.models.load_model(path))\n",
        "\n",
        "  return predict_with_ensemble_tf(models, inputs)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axVq0C6tLU-3"
      },
      "source": [
        "def evaluate_with_BiLSTM(dataset_path, bad_words_path, models_path_list):\n",
        "  print(\"evaluate_with_BiLSTM\")\n",
        "  # Preprocessing\n",
        "  dataset = preprocessing_KimCNN_BiLSTM(dataset_path, bad_words_path)\n",
        "  # Split dataset in embeddings and other feature\n",
        "  X_dataset = get_data_to_emb2(dataset[\"tokens\"], w2v, max_length , True)\n",
        "  dataset_other = dataset.drop(['text','tokens', 'hashtags'], axis=1)\n",
        "\n",
        "  inputs   = {\"text\": X_dataset, \"other\": dataset_other.values}\n",
        "\n",
        "  # Load models\n",
        "  models = []\n",
        "  for path in models_path_list:\n",
        "    models.append(tf.keras.models.load_model(path))\n",
        "\n",
        "  return predict_with_ensemble_tf(models, inputs)\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS7yYirzLYoq"
      },
      "source": [
        "def evaluate_with_ALBERTo(dataset_path, models_path_list):\n",
        "  print(\"evaluate_with_ALBERTo\")\n",
        "  dataset = preprocessing_ALBERTo(dataset_path)\n",
        "\n",
        "  # Load models\n",
        "  models = []\n",
        "  for path in models_path_list:\n",
        "    models.append(model = torch.load(path))\n",
        "\n",
        "  return predict_with_ensemble_torch(models, inputs)\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15bO8iggH0GS"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "w2v_path = \"/content/drive/My Drive/HLT/w2v/twitter128.bin\"\n",
        "w2v = KeyedVectors.load_word2vec_format(datapath(w2v_path), binary=True)\n",
        "max_length = 64"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQsTUJvRSVRR"
      },
      "source": [
        "dataset_path = '/content/drive/My Drive/HLT/scraper/tweets_perchetendenza.csv'\n",
        "bad_words_path = '/content/drive/My Drive/HLT/preprocessing/bad_words.csv'\n",
        "base_model_path_KimCNN = '/content/drive/MyDrive/HLT/clean_dataset_training/model_output/kimCNN/512_0.0_nadam_0.0002/' \n",
        "base_model_path_BiLSTM = '/content/drive/MyDrive/HLT/clean_dataset_training/model_output/biLSTM/64_0.2_nadam_0.0/' \n",
        "base_model_path_ALBERTo = '/content/drive/MyDrive/HLT/alberto/dr_0.5_dr_0.7/' \n",
        "model_path_list_KimCNN = []\n",
        "model_path_list_BiLSTM = []\n",
        "model_path_list_ALBERTo = []\n",
        "for i in range(5):\n",
        "  model_path_list_KimCNN.append(base_model_path_KimCNN + \"model_{0}.h5\".format(i))\n",
        "for i in range(5):\n",
        "  model_path_list_BiLSTM.append(base_model_path_BiLSTM + \"model_{0}.h5\".format(i))\n",
        "for i in range(4):\n",
        "  model_path_list_ALBERTo.append(base_model_path_ALBERTo + \"model_{0}\".format(i))"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zn7k3oKSnPL",
        "outputId": "c04505ac-fe1a-4ec5-b51b-23e3688effb6"
      },
      "source": [
        "evaluate_with_KimCNN(dataset_path, bad_words_path, model_path_list_KimCNN)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate_with_KimCNN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb829d04a70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb829359830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb8293595f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb828802710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb8266483b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4na5Ou5WRH7G",
        "outputId": "64d9b9c5-e228-4c0d-d13c-1bfcae8ff0d1"
      },
      "source": [
        "evaluate_with_BiLSTM(dataset_path, bad_words_path, model_path_list_BiLSTM)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluate_with_BiLSTM\n",
            "/content/drive/MyDrive/HLT/clean_dataset_training/model_output/biLSTM/64_0.2_nadam_0.0/model_0.h5\n",
            "/content/drive/MyDrive/HLT/clean_dataset_training/model_output/biLSTM/64_0.2_nadam_0.0/model_1.h5\n",
            "/content/drive/MyDrive/HLT/clean_dataset_training/model_output/biLSTM/64_0.2_nadam_0.0/model_2.h5\n",
            "/content/drive/MyDrive/HLT/clean_dataset_training/model_output/biLSTM/64_0.2_nadam_0.0/model_3.h5\n",
            "/content/drive/MyDrive/HLT/clean_dataset_training/model_output/biLSTM/64_0.2_nadam_0.0/model_4.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb823a914d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb82367c200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb823266200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb822ed0e60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb822b79c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "x-V1y44FV9q9",
        "outputId": "c88b2495-e69b-4c8c-8f41-0f720d45d7ef"
      },
      "source": [
        "evaluate_with_ALBERTo(dataset_path, model_path_list_ALBERTo)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluate_with_ALBERTo\n",
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "Reading english - 1grams ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-fe4a6b97867b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_with_ALBERTo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path_list_ALBERTo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-110-369d48957587>\u001b[0m in \u001b[0;36mevaluate_with_ALBERTo\u001b[0;34m(dataset_path, models_path_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_path_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpredict_with_ensemble_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'BertClassifier' on <module '__main__'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx4NbQcpe2DM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}