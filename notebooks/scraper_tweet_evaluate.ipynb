{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " scraper_tweet_evaluate.ipynb ",
      "provenance": [],
      "authorship_tag": "ABX9TyPcLF1LMvq5skSI7W1OXCj4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Hate_Speech_Detection/blob/main/notebooks/scraper_tweet_evaluate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEWF7JvqtF5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b98653-9ed9-4f8a-a9af-6d811e288844"
      },
      "source": [
        "!pip install tokenizer\n",
        "!pip install ekphrasis\n",
        "!pip install wordninja\n",
        "!pip install emoji\n",
        "!pip install spacy_udpipe\n",
        "!pip install language_tool_python\n",
        "!pip install compound-word-splitter\n",
        "!pip install pyenchant\n",
        "!sudo apt-get install enchant\n",
        "!sudo apt-get install hunspell-it\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/e9/3c617c16df4a60a52e53e77e66c58819a5023de29387a8f8f018d2c105c7/tokenizer-2.5.0-py2.py3-none-any.whl (107kB)\n",
            "\r\u001b[K     |███                             | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 81kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 102kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 7.5MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizer\n",
            "Successfully installed tokenizer-2.5.0\n",
            "Collecting ekphrasis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/37c59d65e78c3a2aaf662df58faca7250eb6b36c559b912a39a7ca204cfb/ekphrasis-0.5.1.tar.gz (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting ujson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Building wheels for collected packages: ekphrasis, ftfy\n",
            "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-cp37-none-any.whl size=82844 sha256=852dba033207c05312b64595d8ddbd8cf776d97162da2d1f6aad88e376aa539d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/c5/9b/c9b60f535a2cf9fdbc92d84c4801a010c35a9cd348011ed2a1\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=a386f5bf710df7c22feeb6e9603e5a350edba86f3427429e2bd3fe1d0b7c9624\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built ekphrasis ftfy\n",
            "Installing collected packages: colorama, ujson, ftfy, ekphrasis\n",
            "Successfully installed colorama-0.4.4 ekphrasis-0.5.1 ftfy-5.9 ujson-4.0.2\n",
            "Collecting wordninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 8.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: wordninja\n",
            "  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordninja: filename=wordninja-2.0.0-cp37-none-any.whl size=541554 sha256=e51913dc07064fdb315ecb828b57d10cf693f265d0ccf474aee776cf1dc012f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n",
            "Successfully built wordninja\n",
            "Installing collected packages: wordninja\n",
            "Successfully installed wordninja-2.0.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n",
            "Collecting spacy_udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy_udpipe) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (54.1.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (7.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.4.3)\n",
            "Building wheels for collected packages: ufal.udpipe\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626577 sha256=9d9be1284136a021564563c151f62489b6c0e62270e1b73a421447a1485c482a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "Successfully built ufal.udpipe\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe\n",
            "Successfully installed spacy-udpipe-0.3.2 ufal.udpipe-1.2.0.3\n",
            "Collecting language_tool_python\n",
            "  Downloading https://files.pythonhosted.org/packages/37/26/48b22ad565fd372edec3577218fb817e0e6626bf4e658033197470ad92b3/language_tool_python-2.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.5.3\n",
            "Collecting compound-word-splitter\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/0c/62c1ff670016291bd7c24ebed9476ad3cc165eda571ba8442628c636cac1/compound-word-splitter-0.4.tar.gz\n",
            "Building wheels for collected packages: compound-word-splitter\n",
            "  Building wheel for compound-word-splitter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compound-word-splitter: filename=compound_word_splitter-0.4-cp37-none-any.whl size=2565 sha256=9b92c1efb8b05aedf3473344a17635c2e9a0fc46d89e48f9f6f0b1310156ca17\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/de/38/8898744bbad2093c1d909ad5b95f221e85877c3e96131f09d1\n",
            "Successfully built compound-word-splitter\n",
            "Installing collected packages: compound-word-splitter\n",
            "Successfully installed compound-word-splitter-0.4\n",
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/8c/bd224a5db562ac008edbfaf015f5d5c98ea13e745247cd4ab5fc5b683085/pyenchant-3.2.0-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.1 [309 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.1 [87.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,310 kB in 1s (1,095 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 160980 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  hunspell libreoffice-writer\n",
            "The following NEW packages will be installed:\n",
            "  hunspell-it\n",
            "0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 340 kB of archives.\n",
            "After this operation, 1,752 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-it all 1:6.0.3-3 [340 kB]\n",
            "Fetched 340 kB in 1s (350 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package hunspell-it.\n",
            "(Reading database ... 161394 files and directories currently installed.)\n",
            "Preparing to unpack .../hunspell-it_1%3a6.0.3-3_all.deb ...\n",
            "Unpacking hunspell-it (1:6.0.3-3) ...\n",
            "Setting up hunspell-it (1:6.0.3-3) ...\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 7.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=e2deb4caac338e1bc27a786b3afb6c048a60bdbcd35d755cace2399eb2b66ca9\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U92s4XL1tTtG"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from tokenizer import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "import wordninja\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "import ast\n",
        "\n",
        "import emoji\n",
        "import unicodedata\n",
        "\n",
        "import gzip\n",
        "\n",
        "import spacy_udpipe\n",
        "import language_tool_python\n",
        "\n",
        "import splitter\n",
        "import enchant\n",
        "from itertools import groupby\n",
        "import string\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "#KimCNN\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ALBERTo\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset,DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRp-i_rntxPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb4e6d5-90b5-4a57-eeaf-742201c2412d"
      },
      "source": [
        "with_drive = True\n",
        "\n",
        "if with_drive:\n",
        "  from google.colab import drive\n",
        "  # This will prompt for authorization.\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYi7p-INLb5E"
      },
      "source": [
        "def preprocessing_KimCNN_BiLSTM(dataset_path, bad_words_path):\n",
        "  def comment_length(text):\n",
        "    return len(text)\n",
        "\n",
        "  def caps_lock_words(text):\n",
        "    words = text.split()\n",
        "    count_caps_lock = 0\n",
        "    number_of_words = len(words)\n",
        "    \n",
        "    for word in words:\n",
        "        if word.isupper() == True:\n",
        "            count_caps_lock = count_caps_lock + 1\n",
        "            \n",
        "    return ((count_caps_lock*100)//number_of_words)\n",
        "\n",
        "  def clean_tag(text):\n",
        "    return re.sub(\n",
        "        r'@user', ' ', text)\n",
        "    \n",
        "  def replace_e_a(text):\n",
        "    text = re.sub(r'&', 'e', text)\n",
        "    return re.sub(r'@', 'a', text)\n",
        "\n",
        "  def clean_disguised_bad_words(text):\n",
        "    text = \" \" + text + \" \"\n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' coglioni ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+e ', ' coglione ', text)\n",
        "    text = re.sub(r' c[.x*@%#$^]+o ', ' cazzo ', text) \n",
        "    text = re.sub(r' ca[.x*@%#$^]+ro ', ' cazzaro ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+i ', ' cazzi ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+a ', ' merda ', text) \n",
        "    text = re.sub(r' m[.x*@%#$^]+e ', ' merde ', text) \n",
        "    text = re.sub(r' c[.x*@%#$^]+ulo ', ' culo ', text) \n",
        "    text = re.sub(r' p[.x*@%#$^]+a ', ' puttana ', text)\n",
        "    text = re.sub(r' p[.x*@%#$^]+e ', ' puttane ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+a ', ' troia ', text)\n",
        "    text = re.sub(r' t[.x*@%#$^]+e ', ' troie ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+o ', ' stronzo ', text)\n",
        "    text = re.sub(r' s[.x*@%#$^]+i ', ' stronzi ', text)\n",
        "    return text\n",
        "\n",
        "  def find_hashtags(text):\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    if result:\n",
        "        return result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "  def split_hashtags(text):\n",
        "    \n",
        "    text = ' ' + text + ' '\n",
        "    result = re.findall(r'#\\S+', text)\n",
        "    \n",
        "    for word in result:\n",
        "        new_word = \" \".join(splitter.split(word[1:].lower(), 'it_IT'))\n",
        "        if len(new_word)==0:\n",
        "            new_word =  word[1:]\n",
        "                  \n",
        "        text = text.replace(word, new_word)\n",
        "        \n",
        "    return text\n",
        "\n",
        "  def clean_URL(text):\n",
        "    return re.sub(r'URL', ' ', text)\n",
        "\n",
        "  def esclamations_and_questions(text):\n",
        "    return text.count('!') + text.count('?')\n",
        "\n",
        "  def esclamations_and_questions(text):\n",
        "      return text.count(',') + text.count('.')\n",
        "\n",
        "  def strip_punctuation(text):\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    return regex.sub(' ', text)\n",
        "\n",
        "  correct_words_vowels = ['sciiti,',\n",
        "  'welcomerefugees',\n",
        "  'livetweet',\n",
        "  'desiree',\n",
        "  'canaan',\n",
        "  'tweet.',\n",
        "  'weekend',\n",
        "  'romafeyenoord',\n",
        "  'ebree,',\n",
        "  'greencard',\n",
        "  'creerà',\n",
        "  'cooperative.',\n",
        "  'moschee,',\n",
        "  'cooperanti',\n",
        "  'streetart',\n",
        "  'khalidmasood',\n",
        "  'tweet',\n",
        "  'woolfe',\n",
        "  'cooperazione',\n",
        "  'coop',\n",
        "  'seehofer',\n",
        "  'speech',\n",
        "  'coffee',\n",
        "  'scooter',\n",
        "  'street',\n",
        "  'veemenza',\n",
        "  'moschee.',\n",
        "  'maalox.',\n",
        "  'book',\n",
        "  'tweet',\n",
        "  'facebook:',\n",
        "  'sociial,',\n",
        "  'coop,',\n",
        "  'canaaniti.',\n",
        "  'europee,',\n",
        "  'cooperative',\n",
        "  'google',\n",
        "  'creeranno',\n",
        "  'mediterranee',\n",
        "  'cooperazione',\n",
        "  'cooperativa',\n",
        "  '“boom”',\n",
        "  'refugees',\n",
        "  'moonlight',\n",
        "  'imaam',\n",
        "  'shooting',\n",
        "  'sciiti',\n",
        "  'sunniti',\n",
        "  'book',\n",
        "  'atee.',\n",
        "  'looking',\n",
        "  'week',\n",
        "  'ayaan',\n",
        "  'temporanee.',\n",
        "  'idee.',\n",
        "  'sibiliini',\n",
        "  'food',\n",
        "  'refugees',\n",
        "  'retweeted',\n",
        "  'boom',\n",
        "  'keep',\n",
        "  'vodoo',\n",
        "  'hooligans',\n",
        "  'ebree',\n",
        "  'refugees',\n",
        "  'speed',\n",
        "  'bloomberg',\n",
        "  'riina',\n",
        "  'hatespeech',\n",
        "  'google',\n",
        "  'masood',\n",
        "  'linee.',\n",
        "  'boom']\n",
        "  def delete_duplicate_vowels_and_redundant_consonant (text):\n",
        "      parole = text.split()\n",
        "      stringa = \"\"\n",
        "      for a in parole:\n",
        "          parola = a\n",
        "          a = [list(g) for k, g in groupby(a)]    \n",
        "          vocali = ['a','e','i','o','u','y']\n",
        "          \n",
        "          for idx,val in enumerate(a):\n",
        "              if idx == 0:\n",
        "                  stringa += a[idx][0] \n",
        "              elif idx == len(a)-1:\n",
        "                  stringa += a[idx][0]\n",
        "              elif a[idx][0] in vocali and (parola.lower() not in correct_words_vowels):\n",
        "                  stringa += a[idx][0]\n",
        "              elif len(a[idx]) == 1:\n",
        "                  stringa += a[idx][0]\n",
        "              elif len (a[idx]) >= 2:\n",
        "                  stringa += a[idx][0]\n",
        "                  stringa += a[idx][1]\n",
        "          stringa =  stringa + \" \"\n",
        "          \n",
        "      return(stringa)\n",
        "\n",
        "  def translate_emoticon(text):\n",
        "    text_result = emoji.demojize(text, language='it')\n",
        "    text_result=re.sub(r':', ' ', text_result)\n",
        "    return text_result\n",
        "\n",
        "  emoticons_text = {\n",
        "    '<kiss>': 'bacio',\n",
        "    '<happy>': 'felice',\n",
        "    '<laugh>': 'risata',\n",
        "    '<sad>': 'triste',\n",
        "    '<surprise>': 'sorpreso',\n",
        "    '<wink>': 'occhiolino',\n",
        "    '<tong>': 'faccia con lingua',\n",
        "    '<annoyed>': 'annoiato',\n",
        "    '<seallips>': 'labbra sigillate',\n",
        "    '<angel>': 'angelo',\n",
        "    '<devil>': 'diavolo',\n",
        "    '<highfive>' : 'batti il cinque',\n",
        "    '<heart>': 'cuore',\n",
        "    '<user>' : 'persona',\n",
        "  }\n",
        "\n",
        "  def clean_emoticon_text(text):\n",
        "    text_words = text.split()\n",
        "    new_words  = [emoticons_text.get(ele, ele) for ele in text_words]\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "  abbr_word = {'cmq':'comunque', 'gov':'governatori', 'fb':'facebook', 'tw':'twitter', 'juve':'juventus', 'ing':'ingegnere', \n",
        "             'sx':'sinistra', 'qdo':'quando', 'rep':'repubblica', 'grz':'grazie', 'ita':'italia', 'mln':'milioni', \n",
        "             'mld':'miliardi', 'pke':'perche', 'anke':'anche', 'cm':'come', 'dlla':'della', 'dlle':'delle', 'qst':'questa',\n",
        "             'ke':'che', 'nn':'non', 'sn':'sono', 'cn':'con', 'xk':'perche', 'xke':'perche', 'art':'articolo',\n",
        "             'tv':'televisore', '€':'euro', 'xché':'perché', 'xké':'perché', 'pkè':'perché'}\n",
        "             \n",
        "  def replace_abbreviation(text):\n",
        "    text_words = text.split()\n",
        "    new_text = \"\"\n",
        "    for token in text_words:\n",
        "      new_text  += abbr_word.get(token, token) +\" \"\n",
        "    \n",
        "    return new_text.strip()\n",
        "\n",
        "  laughs = ['ah', 'eh', 'he' 'ih', 'hi'] #non elimina ahahahah, ma solo ah\n",
        "  vowels = ['a', 'e', 'i', 'o', 'u']\n",
        "\n",
        "  def clean_laughs(text):\n",
        "      #s = \"ahahahah ho fame io, eh eh\" -> \" ho fame io,\"\n",
        "      text_words = text.split()\n",
        "      new_words  = [word for word in text_words if word not in laughs]\n",
        "      \n",
        "      new_text = ' '.join(new_words)\n",
        "      \n",
        "      for i in new_words:\n",
        "          for k in vowels:\n",
        "              if ('h' in i) and (len(i) >= 4):\n",
        "                  if (len(i) - 2) <= (i.count(k) + i.count('h')):\n",
        "                      new_text = new_text.replace(i, '')\n",
        "      \n",
        "      return new_text\n",
        "\n",
        "  def tokenization(text):\n",
        "    tknzr=SocialTokenizer(lowercase=False)\n",
        "    return tknzr.tokenize(text)\n",
        "\n",
        "  def number_bad_words(tokens):\n",
        "      n_bad_words = 0\n",
        "\n",
        "      for word in tokens:\n",
        "          if word.lower() in bad_words_set:\n",
        "              n_bad_words = n_bad_words + 1\n",
        "          \n",
        "      return n_bad_words\n",
        "\n",
        "  def percentage_bad_words(tokens):\n",
        "    n_words = 0\n",
        "    n_bad_words = 0\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word != '<' and word != '>':\n",
        "            n_words = n_words + 1\n",
        "    \n",
        "    for word in tokens:\n",
        "        if word.lower() in bad_words_set:\n",
        "            n_bad_words = n_bad_words + 1\n",
        "        \n",
        "    return ((n_bad_words*100)//n_words)\n",
        "\n",
        "\n",
        "  csv_file = open(dataset_path)\n",
        "\n",
        "  dataset_raw = pd.read_csv(csv_file,sep=',', index_col=0)\n",
        "\n",
        "  dataset_raw.rename(columns={\"Text\": \"text\"}, inplace=True) \n",
        "   #Bad Words\n",
        "  f2 = open(bad_words_path, 'r', encoding='utf8')\n",
        "\n",
        "  bad_words_set = [] #list of lowercase words\n",
        "\n",
        "  for x in f2:\n",
        "      y = x.rstrip()\n",
        "      y = y.lower()\n",
        "      if y != '':\n",
        "          bad_words_set.append(y)\n",
        "\n",
        "  dataset_raw['text_length'] = dataset_raw['text'].apply(comment_length)\n",
        "  dataset_raw['#C-L words'] = dataset_raw['text'].apply(caps_lock_words)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_tag)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(replace_e_a)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_disguised_bad_words)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_disguised_bad_words)\n",
        "  dataset_raw['hashtags'] = dataset_raw['text'].apply(find_hashtags)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(split_hashtags)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_URL)\n",
        "  dataset_raw['#?!'] = dataset_raw['text'].apply(esclamations_and_questions)\n",
        "  dataset_raw['#.,'] = dataset_raw['text'].apply(esclamations_and_questions)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(strip_punctuation)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(delete_duplicate_vowels_and_redundant_consonant)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(translate_emoticon)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_emoticon_text)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(replace_abbreviation)\n",
        "  dataset_raw['text'] = dataset_raw['text'].apply(clean_laughs)\n",
        "  dataset_raw['tokens'] = dataset_raw['text'].apply(tokenization)\n",
        "  dataset_raw['#bad_words'] = dataset_raw['tokens'].apply(number_bad_words)\n",
        "  dataset_raw['%bad_words'] = dataset_raw['tokens'].apply(percentage_bad_words)\n",
        "\n",
        "  return dataset_raw"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTksciFmHn6W"
      },
      "source": [
        "parole_non_ric = set()\n",
        "def sentence_to_emb2(sentence, w2v, truncate = None, padding = False):\n",
        "  global parole_non_ric\n",
        "  pad_token = [0]*128\n",
        "  s_emb = [ w2v[word.lower()] for word in sentence if word.lower() in w2v.vocab]\n",
        "  parole_non_ric.update(set([ word.lower() for word in sentence if word.lower() not in w2v.vocab]))\n",
        "  if truncate is not None:\n",
        "    s_emb = s_emb[:truncate] #truncate\n",
        "  if padding:\n",
        "    s_emb += [pad_token] * (truncate - len(s_emb))\n",
        "  return np.array(s_emb)\n",
        "\n",
        "def get_data_to_emb2(data, w2v, truncate = None, padding = False):\n",
        "  X = [sentence_to_emb2(sentence, w2v, truncate, padding) for sentence in data]\n",
        "  return np.array(X)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AISidccy9X8o",
        "outputId": "a734a7e5-fc23-43e6-bf39-2614737991ae"
      },
      "source": [
        "text_processor = TextPreProcessor(\n",
        "  # terms that will be normalized\n",
        "  normalize=['url', 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
        "  # terms that will be annotated\n",
        "  annotate={\"hashtag\"},\n",
        "  fix_html=True,  # fix HTML tokens\n",
        "\n",
        "  unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "\n",
        "  # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "  # the tokenizer, should take as input a string and return a list of tokens\n",
        "  tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "  dicts=[emoticons]\n",
        "  )\n",
        "\n",
        "class AlBERTo_Preprocessing(object):\n",
        "    def __init__(self, do_lower_case=True, **kwargs):\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        if self.do_lower_case:\n",
        "            text = text.lower()\n",
        "        text = str(\" \".join(text_processor.pre_process_doc(text)))\n",
        "        text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
        "        text = re.sub(r'^\\s', '', text)\n",
        "        text = re.sub(r'\\s$', '', text)\n",
        "        return text\n",
        "\n",
        "AlBERTo_Preprocess = AlBERTo_Preprocessing(do_lower_case=True)\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "Reading english - 1grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx8od0JsZZZW"
      },
      "source": [
        "def preprocessing_ALBERTo(dataset_path):\n",
        "\n",
        "  def preprocess(text):\n",
        "      return AlBERTo_Preprocess.preprocess(text)\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained('m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0')\n",
        "\n",
        "  csv_file = open(dataset_path)\n",
        "\n",
        "  dataset_raw = pd.read_csv(csv_file,sep=',', index_col=0)\n",
        "\n",
        "  dataset_raw.rename(columns={\"Text\": \"text\"}, inplace=True) \n",
        "\n",
        "\n",
        "  dataset_raw['text_preprocessed'] = dataset_raw['text'].apply(preprocess)\n",
        "  encoding_dataset = tokenizer(list(dataset_raw[\"text_preprocessed\"].values), padding=True, truncation=True, max_length=64)\n",
        "  input_ids_dataset = torch.tensor(encoding_dataset['input_ids'])\n",
        "  attention_mask_dataset = torch.tensor(encoding_dataset['attention_mask'])\n",
        "\n",
        "  test_tweets_data = TensorDataset(input_ids_dataset, attention_mask_dataset)\n",
        "  test_tweets_sampler = SequentialSampler(test_tweets_data)\n",
        "  test_tweets_dataloader = DataLoader(test_tweets_data, sampler=test_tweets_sampler, batch_size=len(test_tweets_data))\n",
        "\n",
        "  return test_tweets_dataloader\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiKvRHeXXFGU"
      },
      "source": [
        "def predict_with_ensemble_tf(models, test_input):\n",
        "  # make predictions\n",
        "  results = []\n",
        "  y_predict = [np.squeeze(np.where(model.predict(test_input) > 0.5, 1,0).reshape(1,-1)) for model in models]\n",
        "  # sum across ensemble members\n",
        "  y_predict = np.array(y_predict)\n",
        "\n",
        "  for i in range(y_predict.shape[1]):\n",
        "    counts = np.bincount(y_predict[:,i])\n",
        "    results.append(np.argmax(counts))\n",
        "  # argmax across classes\n",
        "  return results"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwPZ8YbMd9f9"
      },
      "source": [
        "def evaluate_for_kfold_torch(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "    f1_value = []\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        # Get the predictions\n",
        "        preds+=(torch.argmax(logits, dim=1).flatten().tolist())\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uHTm25HbsBF"
      },
      "source": [
        "def predict_with_ensemble_torch(models, test_dataloader):\n",
        "  # make predictions\n",
        "  results = []\n",
        "  y_predict = []\n",
        "  for model in models:\n",
        "    model.to(device)\n",
        "    y_predict.append(evaluate_for_kfold_torch(model, test_dataloader))\n",
        "    model.to(\"cpu\")\n",
        "\n",
        "  #y_predict = [np.squeeze(evaluate_for_kfold(model, test_dataloader)) for model in models]\n",
        "  # sum across ensemble members\n",
        "  y_predict = np.array(y_predict)\n",
        "\n",
        "  for i in range(y_predict.shape[1]):\n",
        "    counts = np.bincount(y_predict[:,i])\n",
        "    results.append(np.argmax(counts))\n",
        "  # argmax across classes\n",
        "  return results"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xroNcnnw7j3u"
      },
      "source": [
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, Hidden_1, Hidden_2, D_out = 768, 256, 64, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, Hidden_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.7),\n",
        "            nn.Linear(Hidden_1, Hidden_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.7),\n",
        "            nn.Linear(Hidden_2, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def freeze(self, freeze_bert=False):\n",
        "        # Freeze or unfreeze the BERT model\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = not freeze_bert"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuBlhDXGtY2H"
      },
      "source": [
        "def evaluate_with_KimCNN(dataset_path, bad_words_path,models_path_list):\n",
        "  print(\"Evaluate_with_KimCNN\")\n",
        "  # Preprocessing\n",
        "  dataset = preprocessing_KimCNN_BiLSTM(dataset_path, bad_words_path)\n",
        "  # Split dataset in embeddings and other feature\n",
        "  X_dataset = get_data_to_emb2(dataset[\"tokens\"], w2v, max_length , True)\n",
        "  dataset_other = dataset.drop(['text','tokens', 'hashtags'], axis=1)\n",
        "\n",
        "  inputs   = {\"text\": X_dataset, \"other\": dataset_other.values}\n",
        "\n",
        "  # Load models\n",
        "  models = []\n",
        "  for path in models_path_list:\n",
        "    models.append(tf.keras.models.load_model(path))\n",
        "\n",
        "  return predict_with_ensemble_tf(models, inputs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axVq0C6tLU-3"
      },
      "source": [
        "def evaluate_with_BiLSTM(dataset_path, bad_words_path, models_path_list):\n",
        "  print(\"evaluate_with_BiLSTM\")\n",
        "  # Preprocessing\n",
        "  dataset = preprocessing_KimCNN_BiLSTM(dataset_path, bad_words_path)\n",
        "  # Split dataset in embeddings and other feature\n",
        "  X_dataset = get_data_to_emb2(dataset[\"tokens\"], w2v, max_length , True)\n",
        "  dataset_other = dataset.drop(['text','tokens', 'hashtags'], axis=1)\n",
        "\n",
        "  inputs   = {\"text\": X_dataset, \"other\": dataset_other.values}\n",
        "\n",
        "  # Load models\n",
        "  models = []\n",
        "  for path in models_path_list:\n",
        "    models.append(tf.keras.models.load_model(path))\n",
        "\n",
        "  return predict_with_ensemble_tf(models, inputs)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS7yYirzLYoq",
        "outputId": "fbb4c7e7-7263-48f3-fbda-32c24396eef7"
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "def evaluate_with_ALBERTo(dataset_path, models_path_list):\n",
        "  print(\"evaluate_with_ALBERTo\")\n",
        "  dataset = preprocessing_ALBERTo(dataset_path)\n",
        "\n",
        "  # Load models\n",
        "  models = []\n",
        "  for path in models_path_list:\n",
        "    models.append(torch.load(path))\n",
        "\n",
        "  result = predict_with_ensemble_torch(models, dataset)\n",
        "  for model in models:\n",
        "    del model \n",
        "  \n",
        "  torch._C._cuda_emptyCache()\n",
        "\n",
        "  return result\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15bO8iggH0GS"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "w2v_path = \"/content/drive/My Drive/HLT/w2v/twitter128.bin\"\n",
        "w2v = KeyedVectors.load_word2vec_format(datapath(w2v_path), binary=True)\n",
        "max_length = 64"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQsTUJvRSVRR"
      },
      "source": [
        "dataset_path = '/content/drive/MyDrive/HLT/scraper/{0}.csv'.format(\"tweets_CasaPoundPadova\")\n",
        "output_path = '/content/drive/MyDrive/HLT/scraper/{0}_with_labels.csv'.format(\"tweets_CasaPoundPadova\")\n",
        "bad_words_path = '/content/drive/MyDrive/HLT/preprocessing/bad_words.csv'\n",
        "base_model_path_KimCNN = '/content/drive/MyDrive/HLT/clean_dataset_training/model_output/kimCNN/512_0.0_nadam_0.0002/' \n",
        "base_model_path_BiLSTM = '/content/drive/MyDrive/HLT/clean_dataset_training/model_output/biLSTM/64_0.2_nadam_0.0/' \n",
        "base_model_path_ALBERTo = '/content/drive/MyDrive/HLT/alberto/dr_0.5_dr_0.7/' \n",
        "model_path_list_KimCNN = []\n",
        "model_path_list_BiLSTM = []\n",
        "model_path_list_ALBERTo = []\n",
        "for i in range(5):\n",
        "  model_path_list_KimCNN.append(base_model_path_KimCNN + \"model_{0}.h5\".format(i))\n",
        "for i in range(5):\n",
        "  model_path_list_BiLSTM.append(base_model_path_BiLSTM + \"model_{0}.h5\".format(i))\n",
        "for i in range(4):\n",
        "  model_path_list_ALBERTo.append(base_model_path_ALBERTo + \"model_{0}\".format(i))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zn7k3oKSnPL",
        "outputId": "e1caf933-fa57-4dfb-e027-1efcac641441"
      },
      "source": [
        "KimCNN_eval = evaluate_with_KimCNN(dataset_path, bad_words_path, model_path_list_KimCNN)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate_with_KimCNN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8671b73710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f86713544d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "6 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8671354200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4na5Ou5WRH7G",
        "outputId": "4b8f4bd1-c770-49bb-9e66-312632f9a706"
      },
      "source": [
        "BiLSTM_eval = evaluate_with_BiLSTM(dataset_path, bad_words_path, model_path_list_BiLSTM)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluate_with_BiLSTM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8670af0710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f869be560e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f869f216680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f86d7d0c830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f86cf49f560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-V1y44FV9q9",
        "outputId": "58d296ba-fdb4-4ed8-b48c-466ce52f3566"
      },
      "source": [
        "ALBERTo_eval = evaluate_with_ALBERTo(dataset_path, model_path_list_ALBERTo)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "evaluate_with_ALBERTo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HPD9vlf-EE_"
      },
      "source": [
        "data = pd.read_csv(dataset_path,sep=',', index_col=0)\n",
        "data.rename(columns={\"Text\": \"text\"}, inplace=True) "
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziUawn-MoMTb",
        "outputId": "d776dbd3-1070-4193-ae64-b97f0cf0eb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "data['KimCNN_label'] = KimCNN_eval\n",
        "data['BiLSTM_eval'] = BiLSTM_eval\n",
        "data['ALBERTo_eval'] = ALBERTo_eval\n",
        "data.head()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>KimCNN_label</th>\n",
              "      <th>BiLSTM_eval</th>\n",
              "      <th>ALBERTo_eval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Vediamo \\n@ItaliaViva\\n fa una proposta per sospendere i mutui, ma passa i 3/4 del video a spiegare come la #BCE debba dare più soldi alle #banche. Non vi viene il sospetto che, visto che molti mutui saltano x la situazione economica, Non vogliano semplicemente aiutare banche amiche?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Vediamo \\n@ItaliaViva\\n fa una proposta per sospendere i mutui, ma passa i 3/4 del video a spiegare come la #BCE debba dare più soldi alle #banche. Non vi viene il sospetto che, visto che molti mutui saltano x la situazione economica, Non vogliano semplicemente aiutare banche amiche?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#Veneto Senza misure shock l'economia dell'intera nazione rischia di collassare. Servono subito 50 miliardi, in deficit e senza riguardo ai vincoli europei, per far ripartire l'Italia. #Lombardia #EmiliaRomagna #bced  \\n@CasaPoundItalia\\n @GiuseppeConteIT</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Le #Sardine ai tempi del #COVID2019, quando i neuroni cozzano fra di loro.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In questo momento di difficoltà per l'Europa c'è chi difende i nostri #confini. Nei giorni scorsi a #Trieste abbiamo portato la solidarietà a tutti coloro che in #Grecia stanno ci difendendo! #GreeceUnderAttack #Turkish #COVID19  \\nhttps://urly.it/34qks</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                           text  ...  ALBERTo_eval\n",
              "0  Vediamo \\n@ItaliaViva\\n fa una proposta per sospendere i mutui, ma passa i 3/4 del video a spiegare come la #BCE debba dare più soldi alle #banche. Non vi viene il sospetto che, visto che molti mutui saltano x la situazione economica, Non vogliano semplicemente aiutare banche amiche?  ...             0\n",
              "1  Vediamo \\n@ItaliaViva\\n fa una proposta per sospendere i mutui, ma passa i 3/4 del video a spiegare come la #BCE debba dare più soldi alle #banche. Non vi viene il sospetto che, visto che molti mutui saltano x la situazione economica, Non vogliano semplicemente aiutare banche amiche?  ...             0\n",
              "2                               #Veneto Senza misure shock l'economia dell'intera nazione rischia di collassare. Servono subito 50 miliardi, in deficit e senza riguardo ai vincoli europei, per far ripartire l'Italia. #Lombardia #EmiliaRomagna #bced  \\n@CasaPoundItalia\\n @GiuseppeConteIT  ...             0\n",
              "3                                                                                                                                                                                                                    Le #Sardine ai tempi del #COVID2019, quando i neuroni cozzano fra di loro.  ...             0\n",
              "4                                 In questo momento di difficoltà per l'Europa c'è chi difende i nostri #confini. Nei giorni scorsi a #Trieste abbiamo portato la solidarietà a tutti coloro che in #Grecia stanno ci difendendo! #GreeceUnderAttack #Turkish #COVID19  \\nhttps://urly.it/34qks  ...             0\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9zDNcmpoNbs"
      },
      "source": [
        "data.to_csv(output_path)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTTEFH93oUJL"
      },
      "source": [
        ""
      ],
      "execution_count": 84,
      "outputs": []
    }
  ]
}