{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlBerto with pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Hate_Speech_Detection/blob/main/AlBerto_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCSPLoZGkjUY"
      },
      "source": [
        "# AlBERTo Hate Speech Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O71JSpSt4Vi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6630d2c-81fd-4d2b-b5e8-c81ee4c1fc22"
      },
      "source": [
        "!pip install ekphrasis\n",
        "!pip install transformers\n",
        "import datetime\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#for code working\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset,DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "from torch import tensor\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#Prepare and import BERT modules\n",
        "import subprocess\n",
        "subprocess.call([\"git\", \"clone\", \"https://github.com/google-research/bert\",\"bert_repo\"])\n",
        "\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "\n",
        "import modeling\n",
        "import tokenization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.9)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwrQH01T_nIS",
        "outputId": "4ce930eb-5a47-4dd3-afc0-6ba16bbcb301"
      },
      "source": [
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpDEutt7zkXx",
        "outputId": "f6501cf2-6cb3-4841-af82-5b1e3451b340"
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WjePYBOyZZz"
      },
      "source": [
        "## Load training dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ewn7ge0AB8n"
      },
      "source": [
        "# directory name \n",
        "input_test_dir = \"/content/drive/My Drive/HLT/dataset_test_evalita/\"\n",
        "model_path = '/content/drive/MyDrive/HLT/alberto/'\n",
        "input_dir_not_clean = '/content/drive/My Drive/HLT/dataset_training/' \n",
        "# Spec\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e79wrBv04JDz"
      },
      "source": [
        "raw_tsv_file = open(input_dir_not_clean+\"haspeede2_dev_taskAB.tsv\")\n",
        "raw_dataset = pd.read_csv(raw_tsv_file,sep='\\t')\n",
        "raw_dataset.rename(columns={\"text \": \"text\"}, inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoN4a6_LndlF"
      },
      "source": [
        "# Inizialize parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZF5y4p17tbY"
      },
      "source": [
        "OUTPUT_DIR = model_path + 'output'\n",
        "#SET THE PARAMETERS\n",
        "MAX_SEQ_LENGTH = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "label_list = [0, 1]\n",
        "\n",
        "#SET THE PARAMETERS FOR TRAINING \n",
        "BATCH_SIZE = 32\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7konutm-F___"
      },
      "source": [
        "# Preprocessing sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1gZmei05Iu8",
        "outputId": "fad71d33-7bdc-4153-be17-7312f8c31f3f"
      },
      "source": [
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\"},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "class AlBERTo_Preprocessing(object):\n",
        "    def __init__(self, do_lower_case=True, **kwargs):\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        if self.do_lower_case:\n",
        "            text = text.lower()\n",
        "        text = str(\" \".join(text_processor.pre_process_doc(text)))\n",
        "        text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
        "        text = re.sub(r'^\\s', '', text)\n",
        "        text = re.sub(r'\\s$', '', text)\n",
        "        return text"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading english - 1grams ...\n",
            "Reading english - 2grams ...\n",
            "Reading english - 1grams ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCdhoarh6BPZ"
      },
      "source": [
        "AlBERTo_Preprocess = AlBERTo_Preprocessing(do_lower_case=True)\n",
        "def preprocess(text):\n",
        "    return AlBERTo_Preprocess.preprocess(text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EqwNrcW5B9V"
      },
      "source": [
        "raw_dataset['text_preprocessed'] = raw_dataset['text'].apply(preprocess)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD-nMYwCFbcC"
      },
      "source": [
        "X_train_kfold_values = list(raw_dataset['text_preprocessed'].values)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv2rozBxsPRC"
      },
      "source": [
        "# Tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElBYWmPx5sV7"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prUSRI10FWje"
      },
      "source": [
        "encoding_dataset_kfold = tokenizer(X_train_kfold_values, padding=True, truncation=True, max_length=MAX_SEQ_LENGTH)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgtlx-IgFWjf"
      },
      "source": [
        "input_ids_dataset_kfold = torch.tensor(encoding_dataset_kfold['input_ids'])\n",
        "attention_mask_dataset_kfold  = torch.tensor(encoding_dataset_kfold['attention_mask'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY8g-uu7FWjh"
      },
      "source": [
        "# Convert other data types to torch.Tensor\n",
        "dataset_kfold_labels = torch.tensor(raw_dataset['hs'].values)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_mZY4iN04jo"
      },
      "source": [
        "# Define the model and support functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcvtzkPQr0J8"
      },
      "source": [
        "## BertClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfcLC7ENt1ad"
      },
      "source": [
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, Hidden_1, Hidden_2, D_out = 768, 256, 64, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, Hidden_1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.7),\n",
        "            nn.Linear(Hidden_1, Hidden_2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.7),\n",
        "            nn.Linear(Hidden_2, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def freeze(self, freeze_bert=False):\n",
        "        # Freeze or unfreeze the BERT model\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = not freeze_bert\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J7Cd_zl2qsh"
      },
      "source": [
        "def initialize_model(len_data, epochs=4, num_warmup_steps = 0):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=LEARNING_RATE,  \n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len_data * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuy8t0ikr9JH"
      },
      "source": [
        "## Train and evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E1zSFWWly2c"
      },
      "source": [
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_copy = torch.tensor(preds)\n",
        "    preds_flat = np.argmax(preds_copy.cpu(), axis=1).flatten()\n",
        "    labels_flat = labels.cpu().flatten()\n",
        "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "def set_seed(seed_value=128, random_seed = False):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    if not random_seed:\n",
        "      random.seed(seed_value)\n",
        "      np.random.seed(seed_value)\n",
        "      torch.manual_seed(seed_value)\n",
        "      torch.cuda.manual_seed_all(seed_value)\n",
        "    else:\n",
        "      random.seed()\n",
        "      np.random.seed()\n",
        "      torch.manual_seed(random.getrandbits(32))\n",
        "      torch.cuda.manual_seed_all(random.getrandbits(32))\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, optimizer, scheduler, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'F1 Train':^9} | {'Val Loss':^10} | {'Val Acc':^9} | {'F1 Val':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*95)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts, f1_value_train_batch, f1_value_train_tot  = 0, 0, 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            #b_labels = b_labels *1.0\n",
        "            #b_labels = b_labels.unsqueeze(1)\n",
        "\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            f1_value_train_batch+= f1_score_func(logits, b_labels) \n",
        "            f1_value_train_tot+= f1_score_func(logits, b_labels) \n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {f1_value_train_batch / batch_counts:^9.2f} | {'-':^10} | {'-':^9} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts, f1_value_train_batch = 0, 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        avg_f1_value = f1_value_train_tot / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*95)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy, f1_value_validation = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {avg_f1_value:^9.2f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {f1_value_validation:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*95)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    b_input_ids = torch.tensor([], dtype=torch.long)\n",
        "    b_attn_mask = torch.tensor([], dtype=torch.long)\n",
        "    b_labels = torch.tensor([],dtype=torch.long)\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids_curr, b_attn_mask_curr, b_labels_curr = tuple(t for t in batch)\n",
        "\n",
        "        # Concat input and labels\n",
        "        b_input_ids = torch.cat((b_input_ids, b_input_ids_curr), 0)\n",
        "        b_attn_mask = torch.cat((b_attn_mask, b_attn_mask_curr), 0)\n",
        "        b_labels = torch.cat((b_labels, b_labels_curr), 0)\n",
        "\n",
        "    b_input_ids = b_input_ids.to(device)\n",
        "    b_attn_mask = b_attn_mask.to(device)\n",
        "\n",
        "    # Compute logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "    del b_input_ids\n",
        "    del b_attn_mask\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Compute loss\n",
        "    b_labels = b_labels.to(device)\n",
        "    loss = loss_fn(logits, b_labels)\n",
        "    val_loss = loss.item()\n",
        "\n",
        "    # Get the predictions\n",
        "    preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "    # Calculate the accuracy rate\n",
        "    val_accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "    f1_value = f1_score_func(logits, b_labels) * 100\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    return val_loss, val_accuracy, f1_value\n",
        "\n",
        "def evaluate_for_kfold(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "    f1_value = []\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds+=(torch.argmax(logits, dim=1).flatten().tolist())\n",
        "    \n",
        "    return preds"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1MvFTAMnieD"
      },
      "source": [
        "## k-fold cross-validation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0VWT0w24rmd"
      },
      "source": [
        "def train_test_model_with_kfold(hparams):\n",
        "  number_of_splits = 4\n",
        "  cv_kfold = StratifiedKFold(n_splits=number_of_splits, shuffle=True, random_state=100)\n",
        "  models = []\n",
        "  epoch_full_model = 3\n",
        "  epoch_dense_part = 7\n",
        "  for train_index, validation_index in cv_kfold.split(input_ids_dataset_kfold, dataset_kfold_labels):\n",
        "    set_seed(random_seed=True)\n",
        "    num_train_steps = int(len(input_ids_dataset_kfold) / BATCH_SIZE * (epoch_full_model+epoch_dense_part))+1\n",
        "    num_warmup_steps = int((epoch_full_model+epoch_dense_part) * WARMUP_PROPORTION)\n",
        "    bert_classifier, optimizer, scheduler = initialize_model(epochs=(epoch_full_model+epoch_dense_part), num_warmup_steps= num_warmup_steps, len_data = len(input_ids_dataset_kfold))\n",
        "\n",
        "    # Create the DataLoader for our training set\n",
        "    train_data_kfold = TensorDataset(input_ids_dataset_kfold[train_index], attention_mask_dataset_kfold[train_index], dataset_kfold_labels[train_index])\n",
        "    train_sampler_kfold = RandomSampler(train_data_kfold)\n",
        "    train_dataloader_kfold = DataLoader(train_data_kfold, sampler=train_sampler_kfold, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Create the DataLoader for our validation set\n",
        "    val_data_kfold = TensorDataset(input_ids_dataset_kfold[validation_index], attention_mask_dataset_kfold[validation_index], dataset_kfold_labels[validation_index])\n",
        "    val_sampler_kfold = SequentialSampler(val_data_kfold)\n",
        "    val_dataloader_kfold = DataLoader(val_data_kfold, sampler=val_sampler_kfold, batch_size=BATCH_SIZE)\n",
        "\n",
        "    train(bert_classifier, train_dataloader_kfold, optimizer, scheduler,val_dataloader_kfold, epochs=epoch_full_model, evaluation=True,)\n",
        "\n",
        "    bert_classifier.freeze(True)\n",
        "\n",
        "    train(bert_classifier, train_dataloader_kfold, optimizer, scheduler, val_dataloader_kfold, epochs=epoch_dense_part, evaluation=True)\n",
        "    bert_classifier.to(\"cpu\")\n",
        "    models.append(bert_classifier)\n",
        "\n",
        "  return models"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2-fOfMDA0Wo"
      },
      "source": [
        "def predict_with_ensemble(models, test_dataloader):\n",
        "  # make predictions\n",
        "  results = []\n",
        "  y_predict = []\n",
        "  for model in models:\n",
        "    model.to(device)\n",
        "    y_predict.append(evaluate_for_kfold(model, test_dataloader))\n",
        "    model.to(\"cpu\")\n",
        "\n",
        "  #y_predict = [np.squeeze(evaluate_for_kfold(model, test_dataloader)) for model in models]\n",
        "  # sum across ensemble members\n",
        "  y_predict = np.array(y_predict)\n",
        "\n",
        "  for i in range(y_predict.shape[1]):\n",
        "    counts = np.bincount(y_predict[:,i])\n",
        "    results.append(np.argmax(counts))\n",
        "  # argmax across classes\n",
        "  return results"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT7zhiSZA2LL"
      },
      "source": [
        "def run_with_kfold(hparams = None):\n",
        "\n",
        "    models = train_test_model_with_kfold(hparams)\n",
        "    y_test_pred_tweets = predict_with_ensemble(models, test_tweets_dataloader)\n",
        "    y_test_pred_news = predict_with_ensemble(models, test_news_dataloader)\n",
        "\n",
        "    print(\"f1_score test tweets: {}\".format(f1_score(test_tweets_labels, y_test_pred_tweets,average=\"macro\")))\n",
        "    print(\"f1_score test news: {}\".format(f1_score(test_news_labels, y_test_pred_news,average=\"macro\")))\n",
        "    return models"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXUp4kNCsgud"
      },
      "source": [
        "# Load and tokenization test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhQsXA0LtGGZ"
      },
      "source": [
        "csv_test_tweets_file = open(input_test_dir+\"haspeede2_reference_taskAB-tweets.tsv\")\n",
        "\n",
        "testset_tweets = pd.read_csv(csv_test_tweets_file,sep='\\t', header=None)\n",
        "testset_tweets.rename(columns={0: \"id\"}, inplace=True)\n",
        "testset_tweets.rename(columns={1: \"text\"}, inplace=True)\n",
        "testset_tweets.rename(columns={2: \"hs\"}, inplace=True)\n",
        "testset_tweets.rename(columns={3: \"stereotype\"}, inplace=True)\n",
        "\n",
        "csv_test_news_file = open(input_test_dir+\"haspeede2_reference_taskAB-news.tsv\")\n",
        "\n",
        "testset_news = pd.read_csv(csv_test_news_file,sep='\\t', header=None)\n",
        "testset_news.rename(columns={0: \"id\"}, inplace=True)\n",
        "testset_news.rename(columns={1: \"text\"}, inplace=True)\n",
        "testset_news.rename(columns={2: \"hs\"}, inplace=True)\n",
        "testset_news.rename(columns={3: \"stereotype\"}, inplace=True)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2Jb6ZB6MXt6"
      },
      "source": [
        "testset_tweets['text_preprocessed'] = testset_tweets['text'].apply(preprocess)\n",
        "testset_news['text_preprocessed'] = testset_news['text'].apply(preprocess)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLmdxQonwlK2"
      },
      "source": [
        "encoding_test_tweets = tokenizer(list(testset_tweets[\"text_preprocessed\"].values), padding=True, truncation=True, max_length=64)\n",
        "encoding_test_news = tokenizer(list(testset_news[\"text_preprocessed\"].values), padding=True, truncation=True, max_length=64)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOm1CnxwwlK4"
      },
      "source": [
        "input_ids_test_tweets = torch.tensor(encoding_test_tweets['input_ids'])\n",
        "attention_mask_test_tweets = torch.tensor(encoding_test_tweets['attention_mask'])\n",
        "input_ids_test_news = torch.tensor(encoding_test_news['input_ids'])\n",
        "attention_mask_test_news = torch.tensor(encoding_test_news['attention_mask'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eakqd_sfwlK6"
      },
      "source": [
        "# Convert other data types to torch.Tensor\n",
        "test_tweets_labels = torch.tensor(list(testset_tweets[\"hs\"].values))\n",
        "test_news_labels = torch.tensor(list(testset_news[\"hs\"].values))\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "test_tweets_data = TensorDataset(input_ids_test_tweets, attention_mask_test_tweets, test_tweets_labels)\n",
        "test_tweets_sampler = SequentialSampler(test_tweets_data)\n",
        "test_tweets_dataloader = DataLoader(test_tweets_data, sampler=test_tweets_sampler, batch_size=len(test_tweets_data))\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "test_news_data = TensorDataset(input_ids_test_news, attention_mask_test_news, test_news_labels)\n",
        "test_news_sampler = SequentialSampler(test_news_data)\n",
        "test_news_dataloader = DataLoader(test_news_data, sampler=test_news_sampler, batch_size=len(test_tweets_data))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSwXo1hdsyK0"
      },
      "source": [
        "# Run k-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raUS8aAEDhYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280e05a2-20af-4af8-d51f-a816fb7b7c2e"
      },
      "source": [
        "models = run_with_kfold()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |   20    |   0.702577   |   0.50    |     -      |     -     |     -     |   4.64   \n",
            "   1    |   40    |   0.674774   |   0.57    |     -      |     -     |     -     |   4.40   \n",
            "   1    |   60    |   0.635106   |   0.62    |     -      |     -     |     -     |   4.42   \n",
            "   1    |   80    |   0.634515   |   0.62    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   100   |   0.609636   |   0.63    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   120   |   0.581992   |   0.67    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   140   |   0.523572   |   0.73    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   160   |   0.590075   |   0.71    |     -      |     -     |     -     |   4.29   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |    -    |   0.619550   |   0.63    |  0.493153  |   75.67   |   75.59   |   38.43  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |   20    |   0.507047   |   0.74    |     -      |     -     |     -     |   4.69   \n",
            "   2    |   40    |   0.525437   |   0.77    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   60    |   0.501781   |   0.76    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   80    |   0.495295   |   0.78    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   100   |   0.482005   |   0.81    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   120   |   0.465739   |   0.80    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   140   |   0.469742   |   0.77    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   160   |   0.506710   |   0.77    |     -      |     -     |     -     |   4.29   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |    -    |   0.494299   |   0.77    |  0.453079  |   78.54   |   78.38   |   38.50  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |   20    |   0.386805   |   0.85    |     -      |     -     |     -     |   4.69   \n",
            "   3    |   40    |   0.415254   |   0.84    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   60    |   0.417073   |   0.82    |     -      |     -     |     -     |   4.40   \n",
            "   3    |   80    |   0.375264   |   0.85    |     -      |     -     |     -     |   4.42   \n",
            "   3    |   100   |   0.401170   |   0.85    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   120   |   0.460208   |   0.83    |     -      |     -     |     -     |   4.42   \n",
            "   3    |   140   |   0.375413   |   0.85    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   160   |   0.383116   |   0.85    |     -      |     -     |     -     |   4.28   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |    -    |   0.401695   |   0.84    |  0.483473  |   79.01   |   79.15   |   38.48  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |   20    |   0.267969   |   0.92    |     -      |     -     |     -     |   2.15   \n",
            "   1    |   40    |   0.285829   |   0.90    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   60    |   0.266549   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   1    |   80    |   0.263586   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   100   |   0.334487   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   1    |   120   |   0.287059   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   1    |   140   |   0.261919   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   160   |   0.304844   |   0.90    |     -      |     -     |     -     |   1.97   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |    -    |   0.283931   |   0.91    |  0.570294  |   78.83   |   78.92   |   19.24  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |   20    |   0.290332   |   0.91    |     -      |     -     |     -     |   2.17   \n",
            "   2    |   40    |   0.264545   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   60    |   0.289811   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   2    |   80    |   0.277798   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   2    |   100   |   0.235436   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   120   |   0.316517   |   0.90    |     -      |     -     |     -     |   2.02   \n",
            "   2    |   140   |   0.305272   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   160   |   0.207069   |   0.94    |     -      |     -     |     -     |   1.95   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |    -    |   0.273453   |   0.92    |  0.596172  |   78.77   |   78.86   |   19.17  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |   20    |   0.295436   |   0.91    |     -      |     -     |     -     |   2.14   \n",
            "   3    |   40    |   0.283904   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   60    |   0.243973   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   3    |   80    |   0.254684   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   100   |   0.241860   |   0.93    |     -      |     -     |     -     |   2.00   \n",
            "   3    |   120   |   0.326847   |   0.90    |     -      |     -     |     -     |   2.00   \n",
            "   3    |   140   |   0.207289   |   0.93    |     -      |     -     |     -     |   1.99   \n",
            "   3    |   160   |   0.256431   |   0.91    |     -      |     -     |     -     |   1.95   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |    -    |   0.263999   |   0.91    |  0.614002  |   78.83   |   78.90   |   19.09  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   4    |   20    |   0.246220   |   0.92    |     -      |     -     |     -     |   2.14   \n",
            "   4    |   40    |   0.266503   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   4    |   60    |   0.318855   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   80    |   0.253594   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   100   |   0.254091   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   120   |   0.275554   |   0.93    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   140   |   0.278376   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   160   |   0.224150   |   0.93    |     -      |     -     |     -     |   1.95   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   4    |    -    |   0.264553   |   0.92    |  0.616215  |   78.71   |   78.79   |   19.10  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   5    |   20    |   0.258449   |   0.91    |     -      |     -     |     -     |   2.15   \n",
            "   5    |   40    |   0.275861   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   60    |   0.258612   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   80    |   0.258204   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   5    |   100   |   0.294301   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   120   |   0.253081   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   5    |   140   |   0.286426   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   160   |   0.232450   |   0.93    |     -      |     -     |     -     |   1.96   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   5    |    -    |   0.264634   |   0.92    |  0.615732  |   78.71   |   78.78   |   19.12  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   6    |   20    |   0.289250   |   0.91    |     -      |     -     |     -     |   2.15   \n",
            "   6    |   40    |   0.237971   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   60    |   0.303100   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   80    |   0.232141   |   0.93    |     -      |     -     |     -     |   2.00   \n",
            "   6    |   100   |   0.289536   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   6    |   120   |   0.256984   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   6    |   140   |   0.233939   |   0.93    |     -      |     -     |     -     |   2.01   \n",
            "   6    |   160   |   0.306956   |   0.90    |     -      |     -     |     -     |   1.97   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   6    |    -    |   0.268862   |   0.92    |  0.621531  |   78.77   |   78.84   |   19.15  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   7    |   20    |   0.289524   |   0.93    |     -      |     -     |     -     |   2.17   \n",
            "   7    |   40    |   0.318270   |   0.89    |     -      |     -     |     -     |   2.05   \n",
            "   7    |   60    |   0.242977   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   7    |   80    |   0.231906   |   0.93    |     -      |     -     |     -     |   2.04   \n",
            "   7    |   100   |   0.323371   |   0.91    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   120   |   0.220903   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   7    |   140   |   0.205947   |   0.93    |     -      |     -     |     -     |   2.01   \n",
            "   7    |   160   |   0.225052   |   0.93    |     -      |     -     |     -     |   1.98   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   7    |    -    |   0.257444   |   0.92    |  0.620168  |   78.71   |   78.78   |   19.26  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |   20    |   0.706921   |   0.52    |     -      |     -     |     -     |   4.65   \n",
            "   1    |   40    |   0.673818   |   0.58    |     -      |     -     |     -     |   4.42   \n",
            "   1    |   60    |   0.631098   |   0.65    |     -      |     -     |     -     |   4.42   \n",
            "   1    |   80    |   0.611169   |   0.67    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   100   |   0.584890   |   0.71    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   120   |   0.607994   |   0.67    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   140   |   0.550482   |   0.71    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   160   |   0.543002   |   0.74    |     -      |     -     |     -     |   4.28   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |    -    |   0.614251   |   0.66    |  0.501070  |   77.00   |   77.14   |   38.42  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |   20    |   0.494032   |   0.78    |     -      |     -     |     -     |   4.67   \n",
            "   2    |   40    |   0.538630   |   0.78    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   60    |   0.506837   |   0.78    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   80    |   0.486926   |   0.80    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   100   |   0.508642   |   0.80    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   120   |   0.476448   |   0.80    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   140   |   0.481368   |   0.79    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   160   |   0.453643   |   0.79    |     -      |     -     |     -     |   4.29   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |    -    |   0.493320   |   0.79    |  0.468207  |   77.76   |   77.79   |   38.45  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |   20    |   0.386381   |   0.87    |     -      |     -     |     -     |   4.67   \n",
            "   3    |   40    |   0.414662   |   0.87    |     -      |     -     |     -     |   4.43   \n",
            "   3    |   60    |   0.429913   |   0.86    |     -      |     -     |     -     |   4.43   \n",
            "   3    |   80    |   0.418572   |   0.86    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   100   |   0.421284   |   0.86    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   120   |   0.424035   |   0.84    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   140   |   0.409711   |   0.83    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   160   |   0.388803   |   0.86    |     -      |     -     |     -     |   4.29   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |    -    |   0.411513   |   0.86    |  0.506701  |   77.65   |   77.82   |   38.49  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |   20    |   0.331783   |   0.88    |     -      |     -     |     -     |   2.15   \n",
            "   1    |   40    |   0.403337   |   0.84    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   60    |   0.390340   |   0.88    |     -      |     -     |     -     |   2.03   \n",
            "   1    |   80    |   0.339766   |   0.87    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   100   |   0.325538   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   1    |   120   |   0.331486   |   0.89    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   140   |   0.339052   |   0.89    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   160   |   0.328932   |   0.91    |     -      |     -     |     -     |   1.98   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |    -    |   0.348674   |   0.88    |  0.551093  |   77.18   |   76.81   |   19.24  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |   20    |   0.343955   |   0.88    |     -      |     -     |     -     |   2.16   \n",
            "   2    |   40    |   0.323038   |   0.89    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   60    |   0.344915   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   2    |   80    |   0.279791   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   2    |   100   |   0.294147   |   0.90    |     -      |     -     |     -     |   2.02   \n",
            "   2    |   120   |   0.295912   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   140   |   0.344150   |   0.89    |     -      |     -     |     -     |   2.02   \n",
            "   2    |   160   |   0.303348   |   0.89    |     -      |     -     |     -     |   1.97   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |    -    |   0.316330   |   0.90    |  0.553924  |   78.29   |   78.11   |   19.20  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |   20    |   0.255694   |   0.91    |     -      |     -     |     -     |   2.15   \n",
            "   3    |   40    |   0.298128   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   3    |   60    |   0.287346   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   3    |   80    |   0.271042   |   0.91    |     -      |     -     |     -     |   2.03   \n",
            "   3    |   100   |   0.277942   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   3    |   120   |   0.304647   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   140   |   0.356773   |   0.88    |     -      |     -     |     -     |   2.03   \n",
            "   3    |   160   |   0.263659   |   0.92    |     -      |     -     |     -     |   1.98   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |    -    |   0.289194   |   0.91    |  0.580283  |   78.00   |   77.91   |   19.23  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   4    |   20    |   0.295921   |   0.90    |     -      |     -     |     -     |   2.16   \n",
            "   4    |   40    |   0.223525   |   0.93    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   60    |   0.330072   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   4    |   80    |   0.303048   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   4    |   100   |   0.291725   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   4    |   120   |   0.293501   |   0.91    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   140   |   0.268201   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   160   |   0.327831   |   0.90    |     -      |     -     |     -     |   1.99   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   4    |    -    |   0.291754   |   0.91    |  0.594771  |   78.06   |   78.05   |   19.22  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   5    |   20    |   0.241052   |   0.93    |     -      |     -     |     -     |   2.13   \n",
            "   5    |   40    |   0.269744   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   5    |   60    |   0.331915   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   80    |   0.317003   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   5    |   100   |   0.308698   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   120   |   0.234359   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   5    |   140   |   0.343008   |   0.89    |     -      |     -     |     -     |   2.03   \n",
            "   5    |   160   |   0.294507   |   0.91    |     -      |     -     |     -     |   1.97   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   5    |    -    |   0.292216   |   0.91    |  0.597270  |   77.53   |   77.57   |   19.15  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   6    |   20    |   0.319696   |   0.90    |     -      |     -     |     -     |   2.16   \n",
            "   6    |   40    |   0.313760   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   6    |   60    |   0.285000   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   6    |   80    |   0.243418   |   0.94    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   100   |   0.367018   |   0.88    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   120   |   0.245009   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   6    |   140   |   0.296522   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   160   |   0.300206   |   0.92    |     -      |     -     |     -     |   1.98   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   6    |    -    |   0.296474   |   0.91    |  0.601352  |   77.41   |   77.47   |   19.19  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   7    |   20    |   0.300081   |   0.91    |     -      |     -     |     -     |   2.17   \n",
            "   7    |   40    |   0.285303   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   7    |   60    |   0.302548   |   0.91    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   80    |   0.216719   |   0.93    |     -      |     -     |     -     |   2.01   \n",
            "   7    |   100   |   0.308743   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   7    |   120   |   0.309794   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   140   |   0.291826   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   7    |   160   |   0.297959   |   0.92    |     -      |     -     |     -     |   1.98   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   7    |    -    |   0.289190   |   0.91    |  0.599407  |   77.12   |   77.21   |   19.24  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |   20    |   0.685905   |   0.55    |     -      |     -     |     -     |   4.71   \n",
            "   1    |   40    |   0.667551   |   0.56    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   60    |   0.631552   |   0.62    |     -      |     -     |     -     |   4.42   \n",
            "   1    |   80    |   0.645913   |   0.63    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   100   |   0.592066   |   0.67    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   120   |   0.569844   |   0.71    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   140   |   0.554100   |   0.72    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   160   |   0.606434   |   0.71    |     -      |     -     |     -     |   4.29   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |    -    |   0.619585   |   0.65    |  0.493032  |   77.24   |   77.25   |   38.48  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |   20    |   0.510100   |   0.77    |     -      |     -     |     -     |   4.66   \n",
            "   2    |   40    |   0.507412   |   0.77    |     -      |     -     |     -     |   4.43   \n",
            "   2    |   60    |   0.518290   |   0.75    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   80    |   0.536093   |   0.75    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   100   |   0.466674   |   0.78    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   120   |   0.493731   |   0.78    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   140   |   0.497733   |   0.77    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   160   |   0.463454   |   0.78    |     -      |     -     |     -     |   4.28   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |    -    |   0.499254   |   0.77    |  0.434982  |   79.70   |   79.75   |   38.47  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |   20    |   0.436610   |   0.82    |     -      |     -     |     -     |   4.67   \n",
            "   3    |   40    |   0.444181   |   0.85    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   60    |   0.456557   |   0.82    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   80    |   0.366437   |   0.87    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   100   |   0.410077   |   0.85    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   120   |   0.442265   |   0.81    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   140   |   0.405139   |   0.82    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   160   |   0.431407   |   0.84    |     -      |     -     |     -     |   4.29   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |    -    |   0.424162   |   0.83    |  0.543614  |   77.47   |   77.65   |   38.44  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |   20    |   0.309428   |   0.90    |     -      |     -     |     -     |   2.14   \n",
            "   1    |   40    |   0.313667   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   1    |   60    |   0.349047   |   0.88    |     -      |     -     |     -     |   2.03   \n",
            "   1    |   80    |   0.342849   |   0.88    |     -      |     -     |     -     |   2.01   \n",
            "   1    |   100   |   0.306987   |   0.89    |     -      |     -     |     -     |   2.01   \n",
            "   1    |   120   |   0.319985   |   0.89    |     -      |     -     |     -     |   2.01   \n",
            "   1    |   140   |   0.261525   |   0.91    |     -      |     -     |     -     |   2.00   \n",
            "   1    |   160   |   0.273709   |   0.91    |     -      |     -     |     -     |   1.96   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |    -    |   0.309648   |   0.90    |  0.536932  |   79.05   |   78.95   |   19.15  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |   20    |   0.269338   |   0.91    |     -      |     -     |     -     |   2.14   \n",
            "   2    |   40    |   0.308357   |   0.89    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   60    |   0.263206   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   80    |   0.304278   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   100   |   0.354416   |   0.89    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   120   |   0.318089   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   140   |   0.315352   |   0.89    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   160   |   0.359616   |   0.88    |     -      |     -     |     -     |   1.96   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |    -    |   0.311319   |   0.90    |  0.552827  |   78.82   |   78.74   |   19.10  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |   20    |   0.296618   |   0.90    |     -      |     -     |     -     |   2.15   \n",
            "   3    |   40    |   0.302458   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   60    |   0.358092   |   0.88    |     -      |     -     |     -     |   2.05   \n",
            "   3    |   80    |   0.310243   |   0.90    |     -      |     -     |     -     |   2.02   \n",
            "   3    |   100   |   0.259156   |   0.90    |     -      |     -     |     -     |   2.00   \n",
            "   3    |   120   |   0.275642   |   0.90    |     -      |     -     |     -     |   2.00   \n",
            "   3    |   140   |   0.255115   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   160   |   0.262598   |   0.92    |     -      |     -     |     -     |   1.95   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |    -    |   0.290031   |   0.90    |  0.560808  |   79.05   |   79.02   |   19.16  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   4    |   20    |   0.276752   |   0.92    |     -      |     -     |     -     |   2.14   \n",
            "   4    |   40    |   0.320944   |   0.90    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   60    |   0.288240   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   80    |   0.293138   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   100   |   0.314287   |   0.91    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   120   |   0.299758   |   0.90    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   140   |   0.290126   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   160   |   0.341377   |   0.89    |     -      |     -     |     -     |   1.98   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   4    |    -    |   0.302914   |   0.91    |  0.556339  |   78.94   |   78.91   |   19.11  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   5    |   20    |   0.305666   |   0.92    |     -      |     -     |     -     |   2.15   \n",
            "   5    |   40    |   0.263572   |   0.91    |     -      |     -     |     -     |   2.00   \n",
            "   5    |   60    |   0.357743   |   0.89    |     -      |     -     |     -     |   2.02   \n",
            "   5    |   80    |   0.311614   |   0.90    |     -      |     -     |     -     |   2.02   \n",
            "   5    |   100   |   0.212678   |   0.93    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   120   |   0.306915   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   5    |   140   |   0.299813   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   160   |   0.315441   |   0.89    |     -      |     -     |     -     |   1.98   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   5    |    -    |   0.296736   |   0.91    |  0.556632  |   78.88   |   78.86   |   19.16  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   6    |   20    |   0.305337   |   0.90    |     -      |     -     |     -     |   2.16   \n",
            "   6    |   40    |   0.345624   |   0.90    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   60    |   0.294275   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   6    |   80    |   0.266949   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   6    |   100   |   0.270550   |   0.91    |     -      |     -     |     -     |   2.00   \n",
            "   6    |   120   |   0.315321   |   0.89    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   140   |   0.297894   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   160   |   0.272870   |   0.90    |     -      |     -     |     -     |   1.97   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   6    |    -    |   0.296160   |   0.91    |  0.553437  |   78.88   |   78.85   |   19.18  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   7    |   20    |   0.284730   |   0.90    |     -      |     -     |     -     |   2.15   \n",
            "   7    |   40    |   0.277482   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   7    |   60    |   0.298362   |   0.91    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   80    |   0.288048   |   0.91    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   100   |   0.262625   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   7    |   120   |   0.315103   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   7    |   140   |   0.313740   |   0.91    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   160   |   0.271973   |   0.91    |     -      |     -     |     -     |   1.97   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   7    |    -    |   0.288981   |   0.91    |  0.551734  |   78.99   |   78.99   |   19.22  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |   20    |   0.676098   |   0.57    |     -      |     -     |     -     |   4.64   \n",
            "   1    |   40    |   0.673435   |   0.52    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   60    |   0.633307   |   0.64    |     -      |     -     |     -     |   4.48   \n",
            "   1    |   80    |   0.600731   |   0.63    |     -      |     -     |     -     |   4.42   \n",
            "   1    |   100   |   0.597141   |   0.65    |     -      |     -     |     -     |   4.42   \n",
            "   1    |   120   |   0.564277   |   0.70    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   140   |   0.586802   |   0.69    |     -      |     -     |     -     |   4.41   \n",
            "   1    |   160   |   0.533325   |   0.77    |     -      |     -     |     -     |   4.30   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |    -    |   0.608562   |   0.65    |  0.466625  |   78.88   |   78.81   |   38.49  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |   20    |   0.495672   |   0.77    |     -      |     -     |     -     |   4.67   \n",
            "   2    |   40    |   0.492986   |   0.79    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   60    |   0.509899   |   0.76    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   80    |   0.436036   |   0.80    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   100   |   0.513014   |   0.76    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   120   |   0.456852   |   0.78    |     -      |     -     |     -     |   4.41   \n",
            "   2    |   140   |   0.454177   |   0.79    |     -      |     -     |     -     |   4.42   \n",
            "   2    |   160   |   0.428704   |   0.81    |     -      |     -     |     -     |   4.29   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |    -    |   0.473556   |   0.78    |  0.434312  |   80.22   |   80.15   |   38.45  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |   20    |   0.393299   |   0.84    |     -      |     -     |     -     |   4.67   \n",
            "   3    |   40    |   0.356865   |   0.86    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   60    |   0.363212   |   0.86    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   80    |   0.377029   |   0.86    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   100   |   0.378478   |   0.85    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   120   |   0.369911   |   0.85    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   140   |   0.372161   |   0.85    |     -      |     -     |     -     |   4.41   \n",
            "   3    |   160   |   0.356211   |   0.87    |     -      |     -     |     -     |   4.28   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |    -    |   0.371035   |   0.85    |  0.465808  |   78.94   |   79.10   |   38.43  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |   20    |   0.288147   |   0.91    |     -      |     -     |     -     |   2.14   \n",
            "   1    |   40    |   0.313786   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   1    |   60    |   0.305362   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   1    |   80    |   0.249455   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   100   |   0.297535   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   1    |   120   |   0.296591   |   0.90    |     -      |     -     |     -     |   2.01   \n",
            "   1    |   140   |   0.273381   |   0.91    |     -      |     -     |     -     |   2.04   \n",
            "   1    |   160   |   0.276217   |   0.90    |     -      |     -     |     -     |   1.99   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   1    |    -    |   0.287563   |   0.90    |  0.558610  |   77.47   |   77.68   |   19.20  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |   20    |   0.305004   |   0.90    |     -      |     -     |     -     |   2.15   \n",
            "   2    |   40    |   0.268570   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   60    |   0.267455   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   80    |   0.258120   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   2    |   100   |   0.208492   |   0.93    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   120   |   0.302913   |   0.90    |     -      |     -     |     -     |   2.02   \n",
            "   2    |   140   |   0.281374   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   2    |   160   |   0.261882   |   0.92    |     -      |     -     |     -     |   1.96   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   2    |    -    |   0.269449   |   0.91    |  0.571327  |   78.17   |   78.37   |   19.14  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |   20    |   0.218113   |   0.93    |     -      |     -     |     -     |   2.13   \n",
            "   3    |   40    |   0.250395   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   3    |   60    |   0.230564   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   3    |   80    |   0.239986   |   0.93    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   100   |   0.302839   |   0.89    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   120   |   0.261222   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   140   |   0.249839   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   3    |   160   |   0.245679   |   0.93    |     -      |     -     |     -     |   1.96   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   3    |    -    |   0.249633   |   0.92    |  0.596108  |   78.64   |   78.82   |   19.10  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   4    |   20    |   0.219798   |   0.92    |     -      |     -     |     -     |   2.12   \n",
            "   4    |   40    |   0.295916   |   0.93    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   60    |   0.290317   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   80    |   0.225027   |   0.92    |     -      |     -     |     -     |   2.00   \n",
            "   4    |   100   |   0.237205   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   4    |   120   |   0.242550   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   4    |   140   |   0.293745   |   0.89    |     -      |     -     |     -     |   2.02   \n",
            "   4    |   160   |   0.252286   |   0.92    |     -      |     -     |     -     |   1.96   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   4    |    -    |   0.256874   |   0.92    |  0.604115  |   79.23   |   79.38   |   19.11  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   5    |   20    |   0.183765   |   0.95    |     -      |     -     |     -     |   2.13   \n",
            "   5    |   40    |   0.283846   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   5    |   60    |   0.269522   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   80    |   0.257640   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   100   |   0.251805   |   0.93    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   120   |   0.213911   |   0.91    |     -      |     -     |     -     |   2.01   \n",
            "   5    |   140   |   0.281548   |   0.89    |     -      |     -     |     -     |   2.02   \n",
            "   5    |   160   |   0.284914   |   0.90    |     -      |     -     |     -     |   1.97   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   5    |    -    |   0.252937   |   0.92    |  0.609523  |   79.17   |   79.32   |   19.14  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   6    |   20    |   0.231442   |   0.93    |     -      |     -     |     -     |   2.15   \n",
            "   6    |   40    |   0.267971   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   60    |   0.214699   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   6    |   80    |   0.245036   |   0.92    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   100   |   0.257600   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   120   |   0.274335   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   140   |   0.251139   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   6    |   160   |   0.257840   |   0.92    |     -      |     -     |     -     |   1.97   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   6    |    -    |   0.249892   |   0.92    |  0.604625  |   79.29   |   79.44   |   19.18  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  | F1 Train  |  Val Loss  |  Val Acc  |  F1 Val   |  Elapsed \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   7    |   20    |   0.272851   |   0.92    |     -      |     -     |     -     |   2.15   \n",
            "   7    |   40    |   0.269983   |   0.91    |     -      |     -     |     -     |   2.02   \n",
            "   7    |   60    |   0.220489   |   0.92    |     -      |     -     |     -     |   2.01   \n",
            "   7    |   80    |   0.296718   |   0.90    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   100   |   0.259449   |   0.93    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   120   |   0.224573   |   0.92    |     -      |     -     |     -     |   2.04   \n",
            "   7    |   140   |   0.208138   |   0.92    |     -      |     -     |     -     |   2.03   \n",
            "   7    |   160   |   0.194096   |   0.93    |     -      |     -     |     -     |   1.96   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "   7    |    -    |   0.243471   |   0.92    |  0.612254  |   79.17   |   79.32   |   19.21  \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n",
            "f1_score test tweets: 0.751172041661438\n",
            "f1_score test news: 0.6692288306451613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjfL8xXLtBUt"
      },
      "source": [
        "## Results for each model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHl7MQkUyNIC",
        "outputId": "b200ae3a-d9fd-46ee-cc98-da5c15f9c639"
      },
      "source": [
        "for i in range(len(models)):\n",
        "  models[i].to(device)\n",
        "  val_loss_tweets, val_accuracy_tweets, f1_value_tweets = evaluate(models[i], test_tweets_dataloader)\n",
        "  val_loss_news, val_accuracy_news, f1_value_news = evaluate(models[i], test_news_dataloader)\n",
        "  models[i].to('cpu')\n",
        "  print(\"Bert model {0} tweets testset result => Loss: {1} Accuracy: {2} F1 score: {3}\".format(i, val_loss_tweets, val_accuracy_tweets, f1_value_tweets))\n",
        "  print(\"Bert model {0} news testset result => Loss: {1} Accuracy: {2} F1 score: {3}\".format(i, val_loss_news, val_accuracy_news, f1_value_news))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bert model 0 tweets testset result => Loss: 0.6804441809654236 Accuracy: 73.00079176563737 F1 score: 72.83390410493762\n",
            "Bert model 0 news testset result => Loss: 0.9204741716384888 Accuracy: 74.6 F1 score: 72.19597245732034\n",
            "Bert model 1 tweets testset result => Loss: 0.6492424607276917 Accuracy: 73.39667458432304 F1 score: 73.28976793944528\n",
            "Bert model 1 news testset result => Loss: 0.6158900260925293 Accuracy: 78.2 F1 score: 76.82230172756489\n",
            "Bert model 2 tweets testset result => Loss: 0.6604871153831482 Accuracy: 73.15914489311164 F1 score: 73.08383965226407\n",
            "Bert model 2 news testset result => Loss: 0.746150016784668 Accuracy: 74.6 F1 score: 71.92074671363999\n",
            "Bert model 3 tweets testset result => Loss: 0.6813445687294006 Accuracy: 72.60490894695171 F1 score: 72.32035492491438\n",
            "Bert model 3 news testset result => Loss: 1.0090466737747192 Accuracy: 75.2 F1 score: 72.63002585649645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4KN4advuQJc"
      },
      "source": [
        "## Save models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x50kh9Rf9fNO"
      },
      "source": [
        "#for i in range(len(models)):\n",
        "  #torch.save(models[i], \"{0}dr_0.7_dr_0.7/model_{1}\".format(model_path,i))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBN1XqwyOTZx"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}